{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1HVHz3DEayu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`README.md`**\n",
        "\n",
        "# A Quantitative Framework for Modeling Political Distance and Trade\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2509.17303-b31b1b.svg)](https://arxiv.org/abs/2509.17303)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/trade_political_distance_wto)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-International%20Political%20Economy-blue)](https://github.com/chirindaopensource/trade_political_distance_wto)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Structural%20Gravity%20%7C%20PPML%20%7C%20Particle%20Filter-orange)](https://github.com/chirindaopensource/trade_political_distance_wto)\n",
        "[![Data Source](https://img.shields.io/badge/Data-GDELT%20%7C%20UNGA%20Votes%20%7C%20IMF%20DOTS-lightgrey)](https://github.com/chirindaopensource/trade_political_distance_wto)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%23025596?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/statsmodels-1A568C.svg?style=flat)](https://www.statsmodels.org/stable/index.html)\n",
        "[![PyFixest](https://img.shields.io/badge/pyfixest-0B559E.svg?style=flat)](https://github.com/s3alfisc/pyfixest)\n",
        "[![Pydantic](https://img.shields.io/badge/pydantic-E92063.svg?style=flat)](https://pydantic-docs.helpmanual.io/)\n",
        "[![PyYAML](https://img.shields.io/badge/PyYAML-4B5F6E.svg?style=flat)](https://pyyaml.org/)\n",
        "[![Joblib](https://img.shields.io/badge/joblib-2F72A4.svg?style=flat)](https://joblib.readthedocs.io/en/latest/)\n",
        "[![Modelsummary](https://img.shields.io/badge/modelsummary-D4352C.svg?style=flat)](https://modelsummary.com/)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Geopolitical%20Risk-brightgreen)](https://github.com/chirindaopensource/trade_political_distance_wto)\n",
        "[![Framework](https://img.shields.io/badge/Framework-Bayesian%20Filtering-blueviolet)](https://github.com/chirindaopensource/trade_political_distance_wto)\n",
        "[![Model](https://img.shields.io/badge/Model-Non--Linear%20Panel-red)](https://github.com/chirindaopensource/trade_political_distance_wto)\n",
        "[![Validation](https://img.shields.io/badge/Validation-Temporal%20Analysis-yellow)](https://github.com/chirindaopensource/trade_political_distance_wto)\n",
        "[![Robustness](https://img.shields.io/badge/Robustness-Sensitivity%20Analysis-lightgrey)](https://github.com/chirindaopensource/trade_political_distance_wto)\n",
        "\n",
        "--\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/trade_political_distance_wto`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Trade, Political Distance and the World Trade Organization\"** by:\n",
        "\n",
        "*   Samuel Hardwick\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for quantifying the economic penalty of political friction on international trade and assessing the mitigating role of the World Trade Organization. It delivers a modular, auditable, and extensible pipeline that replicates the paper's entire workflow: from rigorous data validation and the sophisticated filtering of high-frequency event data, through the estimation of complex non-linear panel models with high-dimensional fixed effects, to the final synthesis and presentation of results.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: execute_full_research_pipeline](#key-callable-execute_full_research_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Trade, Political Distance and the World Trade Organization.\" The core of this repository is the iPython Notebook `trade_political_distance_wto_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final generation of tables, figures, and a synthesis report.\n",
        "\n",
        "The paper addresses a key question in international political economy: Do multilateral institutions like the WTO shield commerce from geopolitical tensions? This codebase operationalizes the paper's advanced approach, allowing users to:\n",
        "-   Rigorously validate and assess the quality of a complex, multi-source panel dataset.\n",
        "-   Extract a smoothed signal of political relations from noisy, high-frequency news event data using a state-space model (Particle Filter).\n",
        "-   Estimate a state-of-the-art structural gravity model using Poisson Pseudo-Maximum Likelihood (PPML) with high-dimensional fixed effects.\n",
        "-   Analyze the extensive margin of trade using a Logit model with advanced, bias-corrected estimation and inference (Split-Panel Jackknife with Bootstrap).\n",
        "-   Quantify the economic and statistical significance of the WTO's mitigating effect on political distance.\n",
        "-   Systematically test the stability of the findings across a wide array of robustness checks.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in modern econometrics, computational statistics, and international political economy.\n",
        "\n",
        "**1. Structural Gravity Model:**\n",
        "The workhorse model is the structural gravity equation of trade, which has strong theoretical microfoundations. The model explains bilateral trade flows as a function of economic size and various trade frictions or promoters. In this paper, the key friction is political distance. The model is specified in its multiplicative form and estimated with PPML to handle heteroskedasticity and zero trade flows correctly. The inclusion of high-dimensional fixed effects (exporter-year, importer-year, and country-pair) is critical to control for all time-varying country-specific factors (Multilateral Resistance Terms) and all time-invariant dyadic factors, isolating the effect of interest.\n",
        "$$ X_{ijt} = \\exp\\left(\\left[\\beta_0 PD_{ijt} + \\sum_z \\beta_z (PD_{ijt} \\times z_{ijt})\\right] \\mathbf{1}_{(i \\neq j)} + \\delta_{it} + \\delta_{jt} + \\delta_{ij} + \\text{Border}_{ijt}\\right) \\times \\varepsilon_{ijt} $$\n",
        "\n",
        "**2. State-Space Model and Particle Filter:**\n",
        "To measure short-term political relations, the paper treats the \"true\" but unobserved political stance between two countries as a latent state ($s_{ijt}$) that evolves over time. The noisy, high-frequency GDELT event data ($y_{ijt}$) is treated as a measurement of this state.\n",
        "-   **State Equation (Random Walk):** $s_{ijt} = s_{ij,t-1} + v_t$, where $v_t \\sim \\mathcal{N}(0, Q_{ij})$\n",
        "-   **Observation Equation:** $y_{ijt} = s_{ijt} + \\eta_t$, where $\\eta_t \\sim \\mathcal{N}(0, R_{ijt})$\n",
        "Because the distributions may not be strictly Gaussian, this system is solved using a **Particle Filter**, a Sequential Monte Carlo method that approximates the posterior distribution of the state at each time step. This is a sophisticated technique to filter signal from noise.\n",
        "\n",
        "**3. Inference in Non-Linear Panel Models:**\n",
        "-   **PPML:** Standard errors are clustered at the country-pair level to account for serial correlation in the error term for a given dyad.\n",
        "-   **Logit:** Estimating non-linear models (like Logit) with many fixed effects suffers from the incidental parameters problem, which biases the coefficients. The paper uses the **Split-Panel Jackknife** (Hinz, Stammann, and Wanner, 2021) to correct this bias. Standard errors are then computed via a **cluster bootstrap of the entire jackknife procedure**, a computationally intensive but highly robust inference method.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`trade_political_distance_wto_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Phase Architecture:** The entire pipeline is broken down into 21 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All methodological and computational parameters are managed in an external `config.yaml` file, allowing for easy customization without code changes.\n",
        "-   **Advanced GDELT Data Processing:** A complete Particle Filter implementation to generate a smoothed, high-quality measure of political relations from raw event data.\n",
        "-   **State-of-the-Art Estimation:** High-performance estimation of PPML and Logit models with multiple high-dimensional fixed effects using the `pyfixest` library.\n",
        "-   **Rigorous Inference:**\n",
        "    -   Complete implementation of the Split-Panel Jackknife for bias correction.\n",
        "    -   A parallelized cluster bootstrap of the jackknife procedure for valid standard errors.\n",
        "    -   A full implementation of the multivariate Delta Method for calculating the standard errors of non-linear marginal effects.\n",
        "-   **Comprehensive Analysis Suite:** Functions to quantify institutional effects, test for temporal stability, and perform formal hypothesis tests (Wald, F-tests).\n",
        "-   **Parallelized Robustness Framework:** A powerful, extensible orchestrator for running dozens of robustness checks in parallel using `joblib`.\n",
        "-   **Automated Reporting:** Programmatic generation of publication-quality tables (`modelsummary`), figures (`seaborn`), and a final JSON synthesis report.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation & Preprocessing (Tasks 1-6):** Ingests and rigorously validates all raw data and the `config.yaml` file, performs a deep data quality audit, runs the Particle Filter to create the GDELT measure, and constructs all other analytical variables.\n",
        "2.  **Model & Sample Preparation (Tasks 7-9):** Adds fixed effects identifiers and interaction terms, creates the specific analytical samples (including domestic trade), and prepares generators for bootstrap and jackknife inference.\n",
        "3.  **Estimation (Tasks 10-11):** Estimates the baseline PPML and Logit models using the advanced methods described above.\n",
        "4.  **Analysis & Interpretation (Tasks 12-15):** Runs model diagnostics, calculates economically meaningful marginal effects, quantifies the key institutional effects, and analyzes their stability over time.\n",
        "5.  **Robustness & Reporting (Tasks 16-21):** Orchestrates the entire suite of robustness checks and generates all final tables, figures, and a synthesis report.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `trade_political_distance_wto_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: execute_full_research_pipeline\n",
        "\n",
        "The central function in this project is `execute_full_research_pipeline`. It orchestrates the entire analytical workflow, providing a single entry point for running the baseline study and all associated robustness checks.\n",
        "\n",
        "```python\n",
        "def execute_full_research_pipeline(\n",
        "    master_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str,\n",
        "    run_intensive_logit: bool = True,\n",
        "    run_robustness_checks: bool = True\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end research pipeline for the study.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `statsmodels`, `pyyaml`, `tqdm`, `joblib`, `matplotlib`, `seaborn`, `pydantic`, `pyfixest`, `modelsummary`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/trade_political_distance_wto.git\n",
        "    cd trade_political_distance_wto\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires a single `pandas.DataFrame` and a `config.yaml` file. A mock data generation function is provided in the main notebook to create a valid example for testing. The `master_df` must conform to the 26-column schema defined in the `define_full_schema()` function, which includes dyadic trade data, country-level controls for both exporter and importer, and UNGA ideal points.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `trade_political_distance_wto_draft.ipynb` notebook provides a complete, step-by-step guide. The core workflow is:\n",
        "\n",
        "1.  **Prepare Inputs:** Load your master `pandas.DataFrame`. Ensure the `config.yaml` file is present in the same directory.\n",
        "2.  **Execute Pipeline:** Call the grand orchestrator function.\n",
        "\n",
        "    ```python\n",
        "    # This single call runs the entire project.\n",
        "    execute_full_research_pipeline(\n",
        "        master_df=my_master_data,\n",
        "        config=my_config_dict,\n",
        "        output_dir=\"./research_outputs\",\n",
        "        run_intensive_logit=False,       # Set to True for the full analysis\n",
        "        run_robustness_checks=False      # Set to True for the full analysis\n",
        "    )\n",
        "    ```\n",
        "3.  **Inspect Outputs:** Check the specified output directory (`./research_outputs`) for all generated tables, figures, and the final JSON report.\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The pipeline does not return an object. Its side effect is the creation of a structured output directory:\n",
        "\n",
        "```\n",
        "research_outputs/\n",
        "│\n",
        "├── table_1_main_results.csv              # Main PPML and Logit regression results\n",
        "├── table_2_robustness_summary.csv        # Summary of key coefficients from robustness checks\n",
        "├── figure_1_ppml_marginal_effects.png    # Plot of main marginal effects\n",
        "├── figure_2_temporal_heterogeneity.png   # Plot showing evolution of effects over time\n",
        "├── figure_3_robustness_stability.png     # Coefficient stability plot\n",
        "└── final_synthesis_report.json           # Programmatically generated summary of all findings\n",
        "```\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "trade_political_distance_wto/\n",
        "│\n",
        "├── trade_political_distance_wto_draft.ipynb   # Main implementation notebook\n",
        "├── config.yaml                                # Master configuration file\n",
        "├── requirements.txt                           # Python package dependencies\n",
        "├── LICENSE                                    # MIT license file\n",
        "└── README.md                                  # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can easily modify all methodological parameters, such as particle filter settings, estimation choices, and robustness check definitions, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Alternative Estimators:** Adding support for other non-linear estimators like Negative Binomial for the intensive margin.\n",
        "-   **Dynamic Panel Models:** Incorporating lagged dependent variables to model persistence in trade flows.\n",
        "-   **Machine Learning Benchmarks:** Comparing the structural model's performance against predictive models like Gradient Boosting or Random Forests for forecasting trade flows under geopolitical stress.\n",
        "-   **Interactive Visualization:** Building a dashboard (e.g., with Dash or Streamlit) to allow users to interactively explore the results and run custom scenarios.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{hardwick2025trade,\n",
        "  title={{Trade, Political Distance and the World Trade Organization}},\n",
        "  author={Hardwick, Samuel},\n",
        "  journal={arXiv preprint arXiv:2509.17303},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Quantitative Framework for Modeling Political Distance and Trade.\n",
        "GitHub repository: https://github.com/chirindaopensource/trade_political_distance_wto\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Samuel Hardwick** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, SciPy, Statsmodels, Pydantic, Joblib, Matplotlib, Seaborn, PyFixest, and Modelsummary**, whose work makes complex computational analysis accessible and robust.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `trade_political_distance_wto_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "_cfNALfjM4nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Trade, Political Distance and the World Trade Organization*\"\n",
        "\n",
        "Authors: Samuel Hardwick\n",
        "\n",
        "E-Journal Submission Date: 22 September 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2509.17303\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Trade agreements are often understood as shielding commerce from fluctuations in political relations. This paper provides evidence that World Trade Organization membership reduces the penalty of political distance on trade at the extensive margin. Using a structural gravity framework covering 1948 to 2023 and two measures of political distance, based on high-frequency events data and UN General Assembly votes, GATT/WTO status is consistently associated with a wider range of products traded between politically distant partners. The association is strongest in the early WTO years (1995 to 2008). Events-based estimates also suggest attenuation at the intensive margin, while UN vote-based estimates do not. Across all specifications, GATT/WTO membership increases aggregate trade volumes. The results indicate that a function of the multilateral trading system has been to foster new trade links across political divides, while raising trade volumes among both close and distant partners."
      ],
      "metadata": {
        "id": "L-B_F5MoEluM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Summary: Hardwick (2025)**\n",
        "\n",
        "This is a well-executed study that tackles a classic and newly urgent question: Does the multilateral trading system, embodied by the GATT/WTO, insulate international commerce from political friction? The author provides a nuanced, data-driven answer, suggesting that its primary function has been to foster the *creation* of new trade links across political divides, particularly during a \"golden era\" of multilateralism.\n",
        "\n",
        "Here is a breakdown of the paper's methodology, findings, and contributions.\n",
        "\n",
        "--\n",
        "\n",
        "#### **The Core Research Question and Motivation**\n",
        "\n",
        "The paper investigates whether membership in the GATT/WTO moderates the negative relationship between \"political distance\" (i.e., geopolitical disagreement or antagonism) and bilateral trade.\n",
        "\n",
        "*   **Motivation:** The study is framed against the backdrop of rising geopolitical tensions and fears of \"geoeconomic fragmentation,\" where trade patterns might revert to align with political blocs (e.g., US-centric vs. China-centric). Understanding the historical role of the WTO in mitigating such tendencies is therefore highly relevant for policy.\n",
        "*   **Hypothesis:** The implicit hypothesis is that the rules, norms, and dispute settlement mechanisms of the GATT/WTO provide a degree of policy certainty and reduce transaction costs, thereby lowering the \"penalty\" that political distance otherwise imposes on trade.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Data and Measurement of Political Distance (A Methodological Contribution)**\n",
        "\n",
        "A key strength of this paper is its use of two distinct, complementary measures of political distance over a long time horizon (1948–2023).\n",
        "\n",
        "1.  **UN General Assembly (UNGA) Ideal Point Distance:** This is a standard measure in political science. It uses countries' voting records in the UNGA to estimate their latent foreign policy positions on a one-dimensional scale. The absolute difference between two countries' \"ideal points\" serves as a proxy for their long-term ideological and geopolitical alignment. It is an annual, slow-moving indicator of structural alignment.\n",
        "\n",
        "2.  **GDELT Event-Based Index:** This is a high-frequency measure designed to capture short-run diplomatic shocks and interactions.\n",
        "    *   **Source:** It is constructed from the Global Database of Events, Language and Tone (GDELT), which machine-codes news reports for interactions between countries. Each event is assigned a \"Goldstein score\" from -10 (severe conflict) to +10 (major cooperation).\n",
        "    *   **Novelty (The CS/Econometrics Angle):** Raw event data is notoriously noisy and sparse for many country pairs. Hardwick makes a notable methodological contribution by applying a **particle filter** to this data. This is a sophisticated Bayesian state-space modeling technique. The intuition is as follows:\n",
        "        *   There exists a \"true,\" unobserved (latent) state of political relations between two countries.\n",
        "        *   The observed monthly GDELT score is a noisy signal of this true state.\n",
        "        *   The particle filter uses a simulation-based approach to estimate the evolution of the true latent state over time, giving more weight to periods with denser news coverage (and thus a stronger signal). This smooths the series and reduces measurement error, a significant improvement over using raw or simply averaged scores.\n",
        "\n",
        "--\n",
        "\n",
        "#### **The Econometric Framework (Adherence to Best Practices)**\n",
        "\n",
        "The author employs a state-of-the-art **structural gravity model**, which is the theoretical and empirical workhorse of modern international trade analysis.\n",
        "\n",
        "*   **Estimator:** The model is estimated using the **Poisson Pseudo-Maximum Likelihood (PPML)** estimator. This is now the standard for gravity models because it is robust to heteroskedasticity (a common issue in trade data) and naturally includes observations with zero trade, which are crucial for analyzing the formation of new trade links.\n",
        "*   **Fixed Effects:** The specification includes a comprehensive set of fixed effects to control for unobserved factors:\n",
        "    *   **Importer-Year (`d_jt`) and Exporter-Year (`d_it`):** These absorb all time-varying, country-specific factors, such as GDP, domestic policies, and, critically, \"multilateral resistance\" terms from gravity theory.\n",
        "    *   **Country-Pair (`d_ij`):** These absorb all time-invariant bilateral factors like geographic distance, common language, and colonial history.\n",
        "*   **Core Specification:** The central analysis relies on interacting the political distance (`PD`) variable with institutional variables, chiefly `GATTWTO` membership. The key coefficient of interest is on the `PD x GATTWTO` term. A positive coefficient would indicate that the negative effect of political distance on trade is *weaker* for pairs where both are WTO members.\n",
        "*   **Decomposition of Trade Margins:** The analysis goes beyond aggregate trade flows by decomposing the effect into:\n",
        "    *   **Intensive Margin:** The average value of trade per product or sector that is already being traded.\n",
        "    *   **Extensive Margin:** The number of different products (at the HS-6 digit level) or sectors being traded. This margin captures the creation of *new* trade relationships.\n",
        "\n",
        "--\n",
        "\n",
        "#### **The Principal Findings (The \"What, How, and When\")**\n",
        "\n",
        "The results are presented clearly and build a compelling narrative.\n",
        "\n",
        "1.  **Baseline:** Without accounting for institutions, political distance is, on average, negatively correlated with trade. A greater political divide means less trade.\n",
        "\n",
        "2.  **The Attenuating Effect of GATT/WTO:** The central finding is that joint membership in the GATT/WTO significantly **attenuates** the negative impact of political distance. The interaction term `PD x GATTWTO` is consistently positive and significant in many specifications.\n",
        "\n",
        "3.  **The Primacy of the Extensive Margin:** This is the most important nuance. The attenuating effect of the WTO is strongest and most robust at the **extensive margin**. This means the WTO's primary role in overcoming political friction has been to help countries establish *new* trade links (i.e., start trading in new products/sectors) rather than simply increasing the volume of existing trade. For politically distant partners, the WTO appears to lower the entry barriers for commerce.\n",
        "\n",
        "4.  **Temporal Heterogeneity:** The effect was not constant over time. The analysis by sub-period reveals that the WTO's ability to shield trade from politics was **strongest in the early WTO years (1995–2008)**. In the post-2008 period, the evidence suggests that politics and trade have become more closely linked again, even for WTO members, a finding consistent with the broader narrative of slowing globalization and rising geopolitical competition.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Robustness and Validation**\n",
        "\n",
        "The author performs a commendable series of robustness checks, which strengthens confidence in the core results. The main finding—that WTO membership attenuates the political distance penalty at the extensive margin—holds up when:\n",
        "*   Excluding China from the sample.\n",
        "*   Restricting the sample to only democratic country pairs.\n",
        "*   Removing pairs involved in active military conflict.\n",
        "*   Using alternative specifications of the GDELT index and different time frequencies (quarterly data).\n",
        "\n",
        "--\n",
        "\n",
        "#### **Conclusion and Contribution**\n",
        "\n",
        "In sum, this paper provides strong, robust evidence that a key function of the multilateral trading system has been to foster the creation of new trade ties across political divides. It did not necessarily make politically distant partners trade more intensely than close partners, but it created a framework where commerce between them could begin and be sustained.\n",
        "\n",
        "The finding that this effect was concentrated in the 1995-2008 period is particularly sobering. It suggests that the benefits of the system are not automatic and may depend on a broader context of active multilateral cooperation, which has waned in recent years. The paper thus provides a valuable historical and quantitative perspective on the very mechanisms of global economic integration that are currently under strain. It is a solid piece of empirical work that skillfully blends methods from economics, political science, and data science."
      ],
      "metadata": {
        "id": "a4MSCLsbYSQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "EyHH6i9QawPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  A Quantitative Framework for Modeling the Impact of Political Distance on\n",
        "#  International Trade and the Mitigating Role of the World Trade Organization\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Trade, Political Distance and the World\n",
        "#  Trade Organization\" by Samuel Hardwick (2025). It delivers a robust,\n",
        "#  end-to-end system for quantifying the economic penalty of political friction\n",
        "#  on trade flows and assessing the value of international institutions like the\n",
        "#  WTO in mitigating this geopolitical risk.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Structural Gravity Model of Trade with high-dimensional fixed effects.\n",
        "#  • Poisson Pseudo-Maximum Likelihood (PPML) for intensive margin estimation.\n",
        "#  • Logit models with Split-Panel Jackknife for extensive margin analysis.\n",
        "#  • Two distinct measures of Political Distance:\n",
        "#    1. UN General Assembly ideal point distances for long-term alignment.\n",
        "#    2. High-frequency GDELT event data for short-term diplomatic relations.\n",
        "#  • State-Space Modeling with a Particle Filter (Sequential Monte Carlo) to\n",
        "#    extract a smoothed signal of political relations from noisy event data.\n",
        "#  • Cluster-Robust Standard Errors at the dyad level for valid inference.\n",
        "#  • Bootstrap of the Jackknife procedure for robust Logit model inference.\n",
        "#  • Delta Method for calculating standard errors of non-linear marginal effects.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Modular, multi-phase pipeline with task-specific orchestrators.\n",
        "#  • Configuration-driven design for transparent and reproducible analysis.\n",
        "#  • Parallelized execution for computationally intensive estimation and\n",
        "#    robustness checks using `joblib`.\n",
        "#  • State-of-the-art econometric estimation via the `pyfixest` library.\n",
        "#  • Programmatic generation of publication-quality tables and figures using\n",
        "#    `modelsummary`, `matplotlib`, and `seaborn`.\n",
        "#  • Comprehensive data validation, diagnostics, and synthesis reporting.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Hardwick, S. (2025). Trade, Political Distance and the World Trade Organization.\n",
        "#  arXiv preprint arXiv:2509.17303.\n",
        "#  https://arxiv.org/abs/2509.17303\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Standard Library Imports\n",
        "# ==============================================================================\n",
        "import ast\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import traceback\n",
        "import warnings\n",
        "from copy import deepcopy\n",
        "from dataclasses import dataclass, field, FrozenInstanceError\n",
        "from pathlib import Path\n",
        "from typing import (Any, Callable, Dict, Final, Iterator, List, Tuple, Type)\n",
        "\n",
        "# ==============================================================================\n",
        "# Third-Party Library Imports\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import modelsummary\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from joblib import Parallel, delayed\n",
        "from pydantic import (BaseModel, ConfigDict, Field, ValidationError,\n",
        "                      field_validator)\n",
        "from pyfixest.estimation import Fixest, feglm, feols\n",
        "from pyfixest.model_matrix import model_matrix_fixest\n",
        "from scipy.special import logsumexp\n",
        "from scipy.stats import chi2, logistic, norm\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "UkYmXYNNa04A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "qqeQwsT0a3WA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Functional Decomposition of the End-to-End Research Pipeline**\n",
        "\n",
        "#### **Phase 1: Configuration and Data Validation**\n",
        "\n",
        "**Task 1: `run_configuration_and_validation`**\n",
        "*   **Inputs:** A raw Python dictionary (`config`) containing the nested configuration for the entire study.\n",
        "*   **Processes:**\n",
        "    1.  **Structural Validation:** Validates the entire nested structure, keys, data types, and exact values of the input dictionary against a rigorous Pydantic schema.\n",
        "    2.  **Functional Parsing:** Securely parses string-based parameters (e.g., the lambda function for observation variance) into executable Python objects using Abstract Syntax Trees (AST) for security.\n",
        "    3.  **Instantiation:** Consolidates all validated parameters into a single, immutable `ProjectConstants` dataclass object.\n",
        "*   **Outputs:** An immutable, validated `ProjectConstants` object.\n",
        "*   **Transformation:** Transforms a raw, potentially error-prone dictionary into a structured, validated, and type-safe parameter object.\n",
        "*   **Role in Research Pipeline:** This function serves as the **Gatekeeper and Initializer**. It ensures that the analysis is run with the exact parameters specified in the paper (e.g., `M=1000` particles), guaranteeing reproducibility. It implements the methodological constants cited in **Appendix A.1**.\n",
        "\n",
        "**Task 2: `run_dataframe_validation_and_assessment`**\n",
        "*   **Inputs:** A `pandas.DataFrame` (`master_df`) and a schema dictionary (`full_schema`).\n",
        "*   **Processes:**\n",
        "    1.  **Schema Validation:** Verifies the DataFrame's structure: exact column count, names, and dtypes.\n",
        "    2.  **Range Validation:** Checks that numerical variables fall within their theoretical bounds (e.g., `PolityScore` in [-10, 10]).\n",
        "    3.  **Consistency Validation:** Performs logical cross-checks on derived variables (e.g., `DomesticTrade` is consistent with `GDP` and `Exports`).\n",
        "*   **Outputs:** A boolean `True` if all checks pass; otherwise, it raises a `ValueError`.\n",
        "*   **Transformation:** This is a pure validation function; it asserts the state of the data without transforming it.\n",
        "*   **Role in Research Pipeline:** This is the **Data Integrity Checkpoint**. It ensures the raw data is clean, correctly formatted, and internally consistent before any computations, as required by the general empirical strategy in **Section 3**.\n",
        "\n",
        "**Task 3: `run_data_quality_assessment`**\n",
        "*   **Inputs:** The validated `pandas.DataFrame`.\n",
        "*   **Processes:**\n",
        "    1.  **Missingness Audit:** Calculates per-column missing value percentages and the correlation of missingness.\n",
        "    2.  **Temporal Assessment:** Calculates time series lengths per dyad, checks for temporal monotonicity, and flags significant data gaps.\n",
        "*   **Outputs:** A nested dictionary containing detailed diagnostic reports.\n",
        "*   **Transformation:** Transforms the DataFrame into a structured set of diagnostic statistics.\n",
        "*   **Role in Research Pipeline:** This is the **Exploratory Data Analysis (EDA)** stage, providing a comprehensive overview of the panel's structure and quality.\n",
        "\n",
        "#### **Phase 2: Data Preprocessing and Transformation**\n",
        "\n",
        "**Task 4: `run_gdelt_preprocessing_pipeline`**\n",
        "*   **Inputs:** The validated `pandas.DataFrame` and the `ProjectConstants` object.\n",
        "*   **Processes:**\n",
        "    1.  **Density Filtering:** Filters out dyadic pairs that do not meet the `baseline_min_obs_threshold` (270 months) of GDELT event data.\n",
        "    2.  **Data Structuring:** Splits the single DataFrame into a dictionary of smaller DataFrames, one for each dyad's time series.\n",
        "    3.  **Process Variance Estimation:** For each dyad, it fits an AR(1) model to the observed Goldstein scores and extracts the residual variance to estimate the process variance, $Q_{ij}$.\n",
        "*   **Outputs:** A tuple containing: (1) The dictionary of dyadic time series, and (2) A `pandas.Series` of the estimated process variances ($Q_{ij}$).\n",
        "*   **Transformation:** Transforms a single DataFrame into two structured outputs tailored for the particle filter.\n",
        "*   **Role in Research Pipeline:** Implements the **Data Preparation for the Particle Filter** as described in **Appendix A.1**.\n",
        "\n",
        "**Task 5: `run_particle_filter_pipeline`**\n",
        "*   **Inputs:** The dictionary of dyadic time series and the Series of process variances ($Q_{ij}$) from Task 4.\n",
        "*   **Processes:** Executes the full particle filter algorithm for each dyad, involving a sequential loop of:\n",
        "    *   **Propagation:** $s_{ijt}^{(m)} = s_{ij,t-1}^{(m)} + \\epsilon_t^{(m)}$, where $\\epsilon_t^{(m)} \\sim \\mathcal{N}(0, Q_{ij})$.\n",
        "    *   **Weighting:** $w_{ijt}^{(m)} \\propto p(y_{ijt} | s_{ijt}^{(m)})$, correctly handling missing observations.\n",
        "    *   **Resampling:** Performs systematic resampling based on the Effective Sample Size.\n",
        "*   **Outputs:** A `pandas.DataFrame` containing the final, smoothed political distance series (`PD_GDELT_Filtered`).\n",
        "*   **Transformation:** Transforms a dictionary of noisy time series into a single, clean DataFrame of estimated latent states.\n",
        "*   **Role in Research Pipeline:** This is the **methodological core of the GDELT index construction** (see **Appendix A.1**), creating a key explanatory variable.\n",
        "\n",
        "**Task 6: `run_variable_construction_pipeline`**\n",
        "*   **Inputs:** The main DataFrame, the filtered GDELT series, a DataFrame of UNGA ideal points, and a list of columns for IHS transformation.\n",
        "*   **Processes:**\n",
        "    1.  Merges the `PD_GDELT_Filtered` series into the main DataFrame.\n",
        "    2.  Computes the UNGA political distance: $PD_{ijt}^{\\text{UNGA}} = |\\text{IdealPoint}_{it} - \\text{IdealPoint}_{jt}|$.\n",
        "    3.  Applies the inverse hyperbolic sine transformation ($\\sinh^{-1}(x)$) to specified variables.\n",
        "    4.  Constructs the intensive (`X_ijt / S_ijt`) and extensive (`1(S_ijt > 0)`) trade margins.\n",
        "*   **Outputs:** A single, fully processed `pandas.DataFrame` with all variables required for estimation.\n",
        "*   **Transformation:** Enriches the DataFrame with the final set of constructed variables.\n",
        "*   **Role in Research Pipeline:** Finalizes the **creation of all analytical variables** described in **Section 2** and **Section 3**.\n",
        "\n",
        "#### **Phase 3: Model Preparation**\n",
        "\n",
        "**Task 7: `run_panel_data_preparation`**\n",
        "*   **Inputs:** The fully processed DataFrame and a specification for interaction terms.\n",
        "*   **Processes:**\n",
        "    1.  Creates the categorical identifier columns (`FE_ExporterYear`, etc.) for absorbing high-dimensional fixed effects.\n",
        "    2.  Programmatically creates all interaction terms (e.g., `PD x GATTWTO`).\n",
        "*   **Outputs:** A `pandas.DataFrame` ready for estimation.\n",
        "*   **Transformation:** Adds the final identifier and interaction columns required to specify the full econometric model.\n",
        "*   **Role in Research Pipeline:** Prepares the data for the specific structure of the **structural gravity model** from **Section 3**, specified by:\n",
        "    $X_{ijt} = \\exp\\left(\\left[\\beta_0 PD_{ijt} + \\sum_z \\beta_z (PD_{ijt} \\times z_{ijt})\\right] \\mathbf{1}_{(i \\neq j)} + \\delta_{it} + \\delta_{jt} + \\delta_{ij} + \\text{Border}_{ijt}\\right) \\times \\varepsilon_{ijt}$\n",
        "\n",
        "**Task 8: `run_sample_preparation_pipeline`**\n",
        "*   **Inputs:** The main DataFrame, a DataFrame of country-level data, and a configuration dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Create PPML Sample:** Augments the panel with domestic trade observations (`GDP - Exports`).\n",
        "    2.  **Create Logit Sample:** Filters out dyads where the binary trade outcome never changes.\n",
        "    3.  **Create Temporal Subsamples:** Slices the main sample into distinct time periods.\n",
        "*   **Outputs:** A dictionary containing the `ppml_sample`, `logit_sample`, and `temporal_subsamples` DataFrames.\n",
        "*   **Transformation:** Transforms the master DataFrame into multiple, specialized analytical samples.\n",
        "*   **Role in Research Pipeline:** Creates the **final analytical datasets** for every model estimated in the paper.\n",
        "\n",
        "**Task 9: `run_inference_preparation`**\n",
        "*   **Inputs:** An analytical DataFrame and parameters for bootstrap/jackknife.\n",
        "*   **Processes:**\n",
        "    1.  Validates the cluster structure (`DyadicPair`).\n",
        "    2.  Creates a generator for producing cluster-bootstrap resamples.\n",
        "    3.  Creates a generator for producing split-panel jackknife samples.\n",
        "*   **Outputs:** A dictionary containing the validated DataFrame and the two generators.\n",
        "*   **Transformation:** Creates iterable objects (generators) that produce transformed versions of the data on-the-fly.\n",
        "*   **Role in Research Pipeline:** Prepares the necessary components for the **advanced inference techniques** cited in the paper, such as \"pair-clustered standard errors\" and the \"split-panel jackknife method with bootstrap standard errors.\"\n",
        "\n",
        "#### **Phase 4: Primary Econometric Estimation**\n",
        "\n",
        "**Task 10: `run_ppml_estimation_pipeline`**\n",
        "*   **Inputs:** The PPML sample DataFrame and a model specification dictionary.\n",
        "*   **Processes:**\n",
        "    1.  Constructs a `patsy`-style formula string representing the full structural gravity model.\n",
        "    2.  Estimates the model using `pyfixest.feols` with `family='poisson'` and computes cluster-robust standard errors.\n",
        "*   **Outputs:** A `Fixest` results object containing all estimation outputs.\n",
        "*   **Transformation:** Transforms a dataset and specification into a fitted model object.\n",
        "*   **Role in Research Pipeline:** This is the **central estimation function** for the main results (e.g., Table 4).\n",
        "\n",
        "**Task 11: `run_logit_estimation_pipeline`**\n",
        "*   **Inputs:** The Logit sample, a model specification, and the `ProjectConstants` object.\n",
        "*   **Processes:**\n",
        "    1.  **Point Estimation:** Estimates coefficients using the split-panel jackknife procedure for bias correction: $\\hat{\\beta}_{BC} = \\hat{\\beta}_{Full} - \\frac{G-1}{G} \\sum_{g=1}^G (\\hat{\\beta}_{-g} - \\hat{\\beta}_{Full})$.\n",
        "    2.  **Inference:** Computes standard errors by bootstrapping the *entire* jackknife procedure in parallel.\n",
        "*   **Outputs:** A dictionary containing the final bias-corrected coefficients and their bootstrapped standard errors.\n",
        "*   **Transformation:** Transforms a dataset and specification into a final set of coefficient estimates and standard errors.\n",
        "*   **Role in Research Pipeline:** Implements the **estimation of the extensive margin models** (e.g., the \"Trade dummy\" column in Table 3).\n",
        "\n",
        "#### **Phase 5: Analysis and Interpretation**\n",
        "\n",
        "**Task 12: `run_diagnostics_pipeline`**\n",
        "*   **Inputs:** The fitted PPML and Logit model objects.\n",
        "*   **Processes:**\n",
        "    1.  **PPML Diagnostics:** Checks for convergence and tests for overdispersion.\n",
        "    2.  **Logit Diagnostics:** Checks for convergence (as a proxy for separation) and calculates McFadden's Pseudo R-squared.\n",
        "    3.  **Cross-Model Consistency:** Compares the sign and significance of key coefficients across models.\n",
        "*   **Outputs:** A nested dictionary containing all diagnostic reports.\n",
        "*   **Transformation:** Transforms fitted model objects into a structured set of diagnostic statistics.\n",
        "*   **Role in Research Pipeline:** This is the **Model Validation** stage, ensuring the estimated models are statistically sound.\n",
        "\n",
        "**Task 13: `compute_ppml_marginal_effects` & `compute_logit_marginal_effects`**\n",
        "*   **Role in Research Pipeline:** These are the **core calculation engines for effect sizes**. They translate raw coefficients into economically meaningful quantities (percentage changes or changes in probability). `compute_ppml_marginal_effects` uses the **Delta Method** for standard errors: $SE(g(\\beta)) \\approx \\sqrt{(\\nabla g)' V(\\beta) (\\nabla g)}$. `compute_logit_marginal_effects` uses the more robust **bootstrap distribution**. They produce the numbers for figures like Figure 3.\n",
        "\n",
        "**Task 14: `run_institutional_quantification_pipeline`**\n",
        "*   **Inputs:** A fitted model object and a specification of key interaction terms.\n",
        "*   **Processes:**\n",
        "    1.  Quantifies the WTO attenuation effect and its standard error using the full Delta Method.\n",
        "    2.  Performs a formal **Wald test** of the equality of WTO and RTA moderation effects.\n",
        "    3.  Performs an **F-test** of the joint significance of governance interaction terms.\n",
        "*   **Outputs:** A dictionary containing the results of these calculations and hypothesis tests.\n",
        "*   **Transformation:** Transforms model coefficients and VCV matrices into targeted quantities of interest and formal test results.\n",
        "*   **Role in Research Pipeline:** Directly answers the paper's central research questions by **quantifying and testing the significance of the institutional moderation effects**.\n",
        "\n",
        "**Task 15: `run_temporal_analysis_pipeline`**\n",
        "*   **Inputs:** The dictionary of temporal subsamples and the full DataFrame.\n",
        "*   **Processes:**\n",
        "    1.  Re-runs the estimation and marginal effect calculation on each temporal subsample.\n",
        "    2.  Runs a single, pooled regression with a triple-interaction term to formally test for a structural break.\n",
        "*   **Outputs:** A dictionary containing a DataFrame of period-specific effects and the results of the structural break test.\n",
        "*   **Transformation:** Transforms a static model into a dynamic analysis of the stability of findings over time.\n",
        "*   **Role in Research Pipeline:** Provides the quantitative evidence for the **temporal heterogeneity** of the results, as shown in Figure 4.\n",
        "\n",
        "#### **Phase 6: Robustness Analysis**\n",
        "\n",
        "**Task 16-18: `run_all_robustness_analyses`**\n",
        "*   **Inputs:** The baseline DataFrame, baseline model specification, and the `ProjectConstants` object.\n",
        "*   **Processes:** This meta-task orchestrates the entire suite of robustness checks. It programmatically generates configurations for dozens of alternative models (e.g., excluding China, using alternative variables), and then executes the estimation for each in parallel.\n",
        "*   **Outputs:** A dictionary where keys are the names of the robustness checks and values are the resulting fitted model objects.\n",
        "*   **Transformation:** Transforms a single baseline analysis into a comprehensive sensitivity analysis.\n",
        "*   **Role in Research Pipeline:** This is the **Sensitivity Analysis** phase, ensuring that the main findings are not artifacts of a specific sample or model specification.\n",
        "\n",
        "#### **Phase 7: Results Presentation**\n",
        "\n",
        "**Task 19: `run_results_presentation_pipeline`**\n",
        "*   **Inputs:** All fitted model objects from the main and robustness analyses.\n",
        "*   **Processes:**\n",
        "    1.  Uses the `modelsummary` library to combine multiple model results into publication-quality tables.\n",
        "    2.  Creates a custom summary table comparing key coefficients across all robustness checks.\n",
        "*   **Outputs:** A dictionary of `pandas.DataFrame` objects, each representing a formatted table.\n",
        "*   **Transformation:** Transforms complex model objects into human-readable presentation tables.\n",
        "*   **Role in Research Pipeline:** This is the **Results Reporting** stage, programmatically generating the final tables (like Table 4).\n",
        "\n",
        "**Task 20: `generate_and_save_all_figures`**\n",
        "*   **Role in Research Pipeline:** This is the **Visualization** stage. It takes the calculated effects and model results and transforms them into the key figures of the paper (like Figures 3 and 4), providing an intuitive visual summary of the findings.\n",
        "\n",
        "**Task 21: `generate_synthesis_report`**\n",
        "*   **Inputs:** All major results objects from the entire pipeline.\n",
        "*   **Processes:** Programmatically extracts the key quantitative findings, assesses their statistical and economic significance, and assembles them into a structured text-based report.\n",
        "*   **Outputs:** A nested dictionary containing a full, reproducible summary of the project's conclusions.\n",
        "*   **Transformation:** Transforms a collection of quantitative results into a coherent, interpretive narrative.\n",
        "*   **Role in Research Pipeline:** This is the **Final Synthesis and Conclusion** stage, automating the process of writing the abstract and conclusion.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "###**Usage Example**\n",
        "\n",
        "*   **Ideal Data Structure:** The example will be a Python script. It will demonstrate the instantiation of the required inputs: the `master_df` DataFrame and the `config` dictionary.\n",
        "*   **Accurate Implementation:**\n",
        "    1.  **Imports:** The script will begin by importing necessary libraries (`pandas`, `numpy`, `yaml`) and, crucially, the `execute_full_research_pipeline` function itself, along with any other required callables from the pipeline module.\n",
        "    2.  **Configuration Loading:** It will demonstrate the correct way to load the `config.yaml` file created in the previous step. This involves using the `PyYAML` library (`yaml.safe_load()`) to parse the YAML file into a Python dictionary. This is a critical step in a configuration-driven workflow.\n",
        "    3.  **Data Generation (Mock-up):** Since we do not have the actual raw data, the script will programmatically generate a small, but structurally correct, mock `master_df`. This mock DataFrame will:\n",
        "        *   Contain all 26 columns specified in the `define_full_schema` function with the correct dtypes.\n",
        "        *   Have a `pandas.DatetimeIndex`.\n",
        "        *   Contain data for a few dyads (e.g., USA-CHN, USA-CAN, DEU-FRA) over a few years.\n",
        "        *   The data will be generated using `numpy.random` but will be constrained to plausible ranges (e.g., Polity scores between -10 and 10, trade values non-negative). This makes the example fully self-contained and runnable.\n",
        "    4.  **Pipeline Execution:** The script will then make the call to `execute_full_research_pipeline`, passing the loaded `config`, the mock `master_df`, and a specified output directory path.\n",
        "    5.  **Clarity and Annotation:** Every step will be preceded by comments explaining what is being done and why, making the script a clear tutorial for a new user of the pipeline.\n",
        "*   **Anticipated Challenges & Resolutions:**\n",
        "    *   **Challenge:** The full pipeline is computationally expensive and would take a long time to run.\n",
        "    *   **Resolution:** The example will demonstrate how to use the `run_intensive_logit=False` and `run_robustness_checks=False` flags in the orchestrator. This allows a user to run a \"smoke test\" of the entire data preparation and baseline estimation pipeline quickly, before committing to the hours-long full run.\n",
        "*   **Module Selection:** `pandas`, `numpy`, `pyyaml`, `pathlib`.\n",
        "*   **Completeness & Best Practices:** This example demonstrates the complete, intended workflow. It shows how to separate configuration from code (loading the YAML file), how to structure the input data, and how to launch the entire analysis with a single function call. Providing a runnable example with mock data is a crucial part of delivering a professional-grade analytical tool.\n",
        "\n",
        "--\n",
        "\n",
        "#### Example: Using the End-to-End Pipeline\n",
        "\n",
        "This script serves as a complete, executable example of how to use the `execute_full_research_pipeline` function.\n",
        "\n",
        "```python\n",
        "# ==============================================================================\n",
        "# main_analysis.py\n",
        "#\n",
        "# Example script to execute the full end-to-end research pipeline for the\n",
        "# replication of Hardwick (2025), \"Trade, Political Distance and the World\n",
        "# Trade Organization\".\n",
        "#\n",
        "# This script demonstrates:\n",
        "#   1. Loading the master configuration from a YAML file.\n",
        "#   2. Creating a structurally correct (mock) input DataFrame.\n",
        "#   3. Calling the top-level orchestrator to run the entire analysis.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1. Import necessary libraries and the main pipeline function ---\n",
        "\n",
        "# Standard library imports\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "\n",
        "# Third-party imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import the main orchestrator function from the project's module.\n",
        "# It is assumed all other required functions are available within that module.\n",
        "# from geopolitical_trade_pipeline.main import execute_full_research_pipeline\n",
        "# from geopolitical_trade_pipeline.data_validation import define_full_schema\n",
        "\n",
        "# For this self-contained example, we assume the functions are defined in scope.\n",
        "\n",
        "def create_mock_master_df(\n",
        "    years: range = range(2000, 2010),\n",
        "    dyads: list = [('USA', 'CHN'), ('USA', 'CAN'), ('DEU', 'FRA')]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Creates a small, structurally correct mock DataFrame for demonstration.\"\"\"\n",
        "    \n",
        "    # Get the full schema to ensure all columns are created.\n",
        "    schema = define_full_schema()\n",
        "    \n",
        "    # Create the panel structure.\n",
        "    index = pd.to_datetime([f\"{y}-01-01\" for y in years])\n",
        "    df_list = []\n",
        "    for exp, imp in dyads:\n",
        "        df_dyad = pd.DataFrame(index=index)\n",
        "        df_dyad['ExporterISO'] = exp\n",
        "        df_dyad['ImporterISO'] = imp\n",
        "        df_list.append(df_dyad)\n",
        "    \n",
        "    df = pd.concat(df_list)\n",
        "    df.index.name = 'DateTime'\n",
        "    \n",
        "    # Populate with plausible random data.\n",
        "    rng = np.random.default_rng(2025)\n",
        "    n_obs = len(df)\n",
        "    \n",
        "    df['DyadicPair'] = df['ExporterISO'] + '_' + df['ImporterISO']\n",
        "    df['BilateralTradeValue_USD'] = rng.exponential(1e9, n_obs)\n",
        "    df['TradeProd_SectorCount'] = rng.integers(0, 10, n_obs)\n",
        "    df['BACI_TotalValue_USD'] = df['BilateralTradeValue_USD'] * rng.uniform(0.9, 1.1, n_obs)\n",
        "    df['GDELT_GoldsteinMean'] = rng.uniform(-5, 5, n_obs)\n",
        "    df['GDELT_GoldsteinStd'] = rng.uniform(0, 3, n_obs)\n",
        "    df['GDELT_EventCount'] = rng.integers(0, 50, n_obs)\n",
        "    df['GATTWTO_Both'] = 1\n",
        "    df['GATTWTO_One'] = 0\n",
        "    df['RTA'] = rng.choice([0, 1], n_obs)\n",
        "    df['GDP_USD_Exporter'] = rng.uniform(1e12, 2e13, n_obs)\n",
        "    df['GDP_USD_Importer'] = rng.uniform(1e12, 2e13, n_obs)\n",
        "    df['TotalExports_USD_Exporter'] = df['GDP_USD_Exporter'] * rng.uniform(0.1, 0.3, n_obs)\n",
        "    df['TotalExports_USD_Importer'] = df['GDP_USD_Importer'] * rng.uniform(0.1, 0.3, n_obs)\n",
        "    df['DomesticTrade_Exporter'] = df['GDP_USD_Exporter'] - df['TotalExports_USD_Exporter']\n",
        "    df['DomesticTrade_Importer'] = df['GDP_USD_Importer'] - df['TotalExports_USD_Importer']\n",
        "    df['PolityScore_Exporter'] = rng.integers(-10, 11, n_obs)\n",
        "    df['PolityScore_Importer'] = rng.integers(-10, 11, n_obs)\n",
        "    df['CorruptionIndex_Exporter'] = rng.uniform(0, 1, n_obs)\n",
        "    df['CorruptionIndex_Importer'] = rng.uniform(0, 1, n_obs)\n",
        "    df['DemocraticPair'] = ((df['PolityScore_Exporter'] > 6) & (df['PolityScore_Importer'] > 6)).astype(int)\n",
        "    df['PoliticalDistance'] = np.abs(df['PolityScore_Exporter'] - df['PolityScore_Importer'])\n",
        "    df['IdealPoint_Exporter'] = rng.normal(0, 1, n_obs)\n",
        "    df['IdealPoint_Importer'] = rng.normal(0, 1, n_obs)\n",
        "    \n",
        "    # Ensure dtypes match the schema.\n",
        "    for col, dtype in schema.items():\n",
        "        df[col] = df[col].astype(dtype)\n",
        "        \n",
        "    return df\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    # --- 2. Define Inputs for the Pipeline ---\n",
        "\n",
        "    # Define the path to the master configuration file.\n",
        "    # In a real project, this file would be managed and version controlled.\n",
        "    config_path = Path(\"config.yaml\") # Assumes the YAML file is in the same directory.\n",
        "\n",
        "    # Define the directory where all outputs (tables, figures, reports) will be saved.\n",
        "    output_directory = Path(\"./research_outputs\")\n",
        "\n",
        "    # --- 3. Load Configuration from YAML file ---\n",
        "\n",
        "    print(f\"Loading configuration from: {config_path}\")\n",
        "    # Use a try-except block for robust file handling.\n",
        "    try:\n",
        "        with open(config_path, 'r') as f:\n",
        "            # `yaml.safe_load` is the standard, secure way to parse YAML files.\n",
        "            master_config = yaml.safe_load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Configuration file not found at {config_path}. Exiting.\")\n",
        "        exit()\n",
        "    except yaml.YAMLError as e:\n",
        "        print(f\"ERROR: Failed to parse YAML configuration file. Details: {e}. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # --- 4. Load or Create the Master DataFrame ---\n",
        "\n",
        "    print(\"Loading/Creating master DataFrame...\")\n",
        "    # In a real project, this step would involve loading a large CSV or Parquet file.\n",
        "    # For this example, we generate a small, structurally correct mock DataFrame.\n",
        "    # This ensures the example is fully runnable without external data dependencies.\n",
        "    master_input_df = create_mock_master_df()\n",
        "    print(f\"DataFrame created with {len(master_input_df)} observations.\")\n",
        "    print(\"Data Schema:\\n\", master_input_df.info())\n",
        "\n",
        "    # --- 5. Execute the End-to-End Research Pipeline ---\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STARTING END-TO-END RESEARCH PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # This is the single call that runs the entire analysis.\n",
        "    # For this example run, we disable the most computationally intensive parts\n",
        "    # to allow for a quick \"smoke test\" of the pipeline's integrity.\n",
        "    # For a full replication, these would be set to True.\n",
        "    execute_full_research_pipeline(\n",
        "        master_df=master_input_df,\n",
        "        config=master_config,\n",
        "        output_dir=str(output_directory),\n",
        "        run_intensive_logit=False,       # Set to True for full replication\n",
        "        run_robustness_checks=False    # Set to True for full replication\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PIPELINE EXECUTION FINISHED\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFOXSzcVEQvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Parameter Validation and Configuration Setup\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1, Step 1: Configuration Dictionary Structure Validation\n",
        "# ==============================================================================\n",
        "\n",
        "class ParticleFilterParams(BaseModel):\n",
        "    \"\"\"\n",
        "    A Pydantic model for validating the 'particle_filter_params' sub-dictionary.\n",
        "\n",
        "    This model defines the precise schema for parameters related to the Particle\n",
        "    Filter algorithm used for smoothing the GDELT political distance index. It\n",
        "    enforces strict type and value constraints on each parameter to ensure\n",
        "    methodological consistency with the source paper.\n",
        "\n",
        "    Attributes:\n",
        "        M_particles (int): The number of particles to use in the filter. Must be\n",
        "                           exactly 1000 as per Appendix A.1.\n",
        "        observation_variance_function (str): The string representation of the\n",
        "                                             lambda function for observation\n",
        "                                             variance, R_ijt.\n",
        "        process_variance_estimation (str): A descriptive string specifying the\n",
        "                                           method for estimating process\n",
        "                                           variance, Q_ij.\n",
        "        process_variance_floor (float): The minimum allowed value for the\n",
        "                                        process variance to ensure numerical\n",
        "                                        stability. Must be 1e-6.\n",
        "        process_variance_default (float): The default process variance to use\n",
        "                                          when AR(1) estimation fails. Must be\n",
        "                                          0.001.\n",
        "        baseline_min_obs_threshold (int): The minimum number of non-zero monthly\n",
        "                                          observations required for a dyadic\n",
        "                                          pair to be included in the baseline\n",
        "                                          analysis (50% of sample). Must be 270.\n",
        "        robustness_min_obs_threshold (int): The stricter minimum observation\n",
        "                                            threshold for robustness checks\n",
        "                                            (60% of sample). Must be 324.\n",
        "    \"\"\"\n",
        "    # Configure the model to forbid any extra fields not defined in this schema.\n",
        "    # This ensures the configuration dictionary does not contain unexpected keys.\n",
        "    model_config = ConfigDict(extra='forbid')\n",
        "\n",
        "    # Define the number of particles, must be an integer equal to 1000.\n",
        "    # Source: Appendix A.1, page 27\n",
        "    M_particles: int = Field(\n",
        "        ...,\n",
        "        description=\"Number of particles for the Sequential Monte Carlo filter.\",\n",
        "        eq=1000\n",
        "    )\n",
        "\n",
        "    # Define the string for the observation variance function.\n",
        "    # Equation: R_ijt = 1 / (n_ijt + 1)\n",
        "    # Source: Appendix A.1, page 27\n",
        "    observation_variance_function: str = Field(\n",
        "        ...,\n",
        "        description=\"String representation of the observation variance function.\",\n",
        "        eq=\"lambda n_ijt: 1 / (n_ijt + 1)\"\n",
        "    )\n",
        "\n",
        "    # Define the description for the process variance estimation method.\n",
        "    # Source: Appendix A.1, page 27\n",
        "    process_variance_estimation: str = Field(\n",
        "        ...,\n",
        "        description=\"Method to estimate process variance Q_ij.\",\n",
        "        eq=\"Residual variance of AR(1) fit to y_ijt series\"\n",
        "    )\n",
        "\n",
        "    # Define the floor for the process variance, must be a float equal to 1e-6.\n",
        "    # Source: Appendix A.1, page 27\n",
        "    process_variance_floor: float = Field(\n",
        "        ...,\n",
        "        description=\"Minimum value for process variance to ensure stability.\",\n",
        "        eq=1e-6\n",
        "    )\n",
        "\n",
        "    # Define the default value for the process variance, must be a float equal to 0.001.\n",
        "    # Source: Appendix A.1, page 27\n",
        "    process_variance_default: float = Field(\n",
        "        ...,\n",
        "        description=\"Default process variance for failed AR(1) estimations.\",\n",
        "        eq=0.001\n",
        "    )\n",
        "\n",
        "    # Define the baseline threshold for minimum observations, must be an integer equal to 270.\n",
        "    # Source: Appendix A.1, page 27\n",
        "    baseline_min_obs_threshold: int = Field(\n",
        "        ...,\n",
        "        description=\"Minimum non-zero observations for baseline sample.\",\n",
        "        eq=270\n",
        "    )\n",
        "\n",
        "    # Define the robustness threshold for minimum observations, must be an integer equal to 324.\n",
        "    # Source: Appendix A.1, page 27\n",
        "    robustness_min_obs_threshold: int = Field(\n",
        "        ...,\n",
        "        description=\"Stricter minimum observations for robustness check sample.\",\n",
        "        eq=324\n",
        "    )\n",
        "\n",
        "\n",
        "class EstimationParams(BaseModel):\n",
        "    \"\"\"\n",
        "    A Pydantic model for validating the 'estimation_params' sub-dictionary.\n",
        "\n",
        "    This model specifies the schema for parameters governing the econometric\n",
        "    estimation phase, including the choice of estimators, fixed effects\n",
        "    structure, and methods for standard error calculation.\n",
        "\n",
        "    Attributes:\n",
        "        primary_estimator (str): The main estimator for the gravity model. Must\n",
        "                                 be \"PPML\".\n",
        "        extensive_margin_estimator (str): The estimator for the extensive margin\n",
        "                                          analysis. Must be \"Logit\".\n",
        "        fixed_effects_structure (List[str]): A list of strings defining the\n",
        "                                             high-dimensional fixed effects to\n",
        "                                             be absorbed in the models.\n",
        "        ppml_se_method (str): The method for calculating standard errors in the\n",
        "                              PPML models.\n",
        "        logit_se_method (str): The method for calculating standard errors in the\n",
        "                               Logit models.\n",
        "        bootstrap_replications (int): The number of replications for bootstrap-\n",
        "                                      based standard error calculations. Must\n",
        "                                      be 1000.\n",
        "    \"\"\"\n",
        "    # Configure the model to forbid any extra, undefined fields.\n",
        "    model_config = ConfigDict(extra='forbid')\n",
        "\n",
        "    # Define the primary estimator, must be the string \"PPML\".\n",
        "    # Source: Section 3, page 10\n",
        "    primary_estimator: str = Field(\n",
        "        ...,\n",
        "        description=\"Primary estimator for the structural gravity model.\",\n",
        "        eq=\"PPML\"\n",
        "    )\n",
        "\n",
        "    # Define the extensive margin estimator, must be the string \"Logit\".\n",
        "    # Source: Section 3, page 11\n",
        "    extensive_margin_estimator: str = Field(\n",
        "        ...,\n",
        "        description=\"Estimator for the extensive margin (binary/fractional) models.\",\n",
        "        eq=\"Logit\"\n",
        "    )\n",
        "\n",
        "    # Define the fixed effects structure, must be a list of exactly 4 strings.\n",
        "    # Source: Section 3, page 10\n",
        "    fixed_effects_structure: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"High-dimensional fixed effects structure for gravity models.\",\n",
        "        min_length=4,\n",
        "        max_length=4\n",
        "    )\n",
        "\n",
        "    # Define a custom validator to ensure the list contains the exact required elements.\n",
        "    @field_validator('fixed_effects_structure')\n",
        "    def check_exact_fe_structure(cls, v: List[str]) -> List[str]:\n",
        "        \"\"\"Validates the exact content of the fixed_effects_structure list.\"\"\"\n",
        "        # Define the expected list of fixed effects strings.\n",
        "        expected = [\n",
        "            \"ExporterISO + Year\",\n",
        "            \"ImporterISO + Year\",\n",
        "            \"ExporterISO + ImporterISO\",\n",
        "            \"Border + Year\"\n",
        "        ]\n",
        "        # Compare the input list with the expected list.\n",
        "        if v != expected:\n",
        "            # If they do not match, raise a ValueError with a precise error message.\n",
        "            raise ValueError(f\"fixed_effects_structure must be exactly {expected}\")\n",
        "        # If validation passes, return the original value.\n",
        "        return v\n",
        "\n",
        "    # Define the method for PPML standard errors.\n",
        "    # Source: Note to Table 2, page 14\n",
        "    ppml_se_method: str = Field(\n",
        "        ...,\n",
        "        description=\"Method for PPML standard error calculation.\",\n",
        "        eq=\"Clustered by ExporterISO-ImporterISO pair\"\n",
        "    )\n",
        "\n",
        "    # Define the method for Logit standard errors.\n",
        "    # Source: Section 3, page 12\n",
        "    logit_se_method: str = Field(\n",
        "        ...,\n",
        "        description=\"Method for Logit standard error calculation.\",\n",
        "        eq=\"Split-panel jackknife with bootstrap\"\n",
        "    )\n",
        "\n",
        "    # Define the number of bootstrap replications, must be an integer equal to 1000.\n",
        "    # Source: Note to Table 3, page 15\n",
        "    bootstrap_replications: int = Field(\n",
        "        ...,\n",
        "        description=\"Number of replications for bootstrap procedures.\",\n",
        "        eq=1000\n",
        "    )\n",
        "\n",
        "\n",
        "class RobustnessCheckParams(BaseModel):\n",
        "    \"\"\"\n",
        "    A Pydantic model for validating the 'robustness_check_params' sub-dictionary.\n",
        "\n",
        "    This model defines the schema for parameters that control the various\n",
        "    robustness checks performed in the study, such as alternative sample\n",
        "    definitions and variable specifications.\n",
        "\n",
        "    Attributes:\n",
        "        democratic_pair_definition (str): The logical condition defining a\n",
        "                                          \"democratic pair\" based on Polity\n",
        "                                          scores.\n",
        "        pd_trimming_rule (str): A description of the rule for trimming outliers\n",
        "                                from the political distance distribution.\n",
        "        alternative_governance_vars (List[str]): A list of alternative\n",
        "                                                 governance indicators to be\n",
        "                                                 used in robustness checks.\n",
        "        temporal_aggregation_robustness_frequency (str): The temporal frequency\n",
        "                                                         to use for robustness\n",
        "                                                         checks (e.g., \"Quarterly\").\n",
        "        first_month_gdelt_measure (str): A description of the rule for creating\n",
        "                                         the quarterly GDELT measure.\n",
        "    \"\"\"\n",
        "    # Configure the model to forbid any extra, undefined fields.\n",
        "    model_config = ConfigDict(extra='forbid')\n",
        "\n",
        "    # Define the rule for identifying democratic pairs.\n",
        "    # Source: Note to Table A4, page 31\n",
        "    democratic_pair_definition: str = Field(\n",
        "        ...,\n",
        "        description=\"Rule for defining a democratic dyad based on Polity scores.\",\n",
        "        eq=\"Polity_i > 6 and Polity_j > 6\"\n",
        "    )\n",
        "\n",
        "    # Define the rule for trimming the political distance distribution.\n",
        "    # Source: Table A4, Column 4 Header, page 31 & Footnote 7, page 16\n",
        "    pd_trimming_rule: str = Field(\n",
        "        ...,\n",
        "        description=\"Rule for trimming outliers from the political distance variable.\",\n",
        "        eq=\"Drop top and bottom 1% of the PD distribution\"\n",
        "    )\n",
        "\n",
        "    # Define the list of alternative governance variables.\n",
        "    # Source: Note to Table A4, page 31\n",
        "    alternative_governance_vars: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"Alternative governance indicators for robustness checks.\",\n",
        "        min_length=2,\n",
        "        max_length=2\n",
        "    )\n",
        "\n",
        "    # Define a custom validator to check the exact contents of the list.\n",
        "    @field_validator('alternative_governance_vars')\n",
        "    def check_exact_gov_vars(cls, v: List[str]) -> List[str]:\n",
        "        \"\"\"Validates the exact content of the alternative_governance_vars list.\"\"\"\n",
        "        # Define the expected list of governance variable names.\n",
        "        expected = [\n",
        "            \"WGI_Voice_and_Accountability\",\n",
        "            \"WGI_Rule_of_Law\"\n",
        "        ]\n",
        "        # Compare the sorted input list with the sorted expected list for order-insensitivity.\n",
        "        if sorted(v) != sorted(expected):\n",
        "            # If they do not match, raise a ValueError.\n",
        "            raise ValueError(f\"alternative_governance_vars must contain exactly {expected}\")\n",
        "        # If validation passes, return the original value.\n",
        "        return v\n",
        "\n",
        "    # Define the frequency for temporal aggregation robustness checks.\n",
        "    # Source: Table A10, Columns 3 & 4 Headers, page 36\n",
        "    temporal_aggregation_robustness_frequency: str = Field(\n",
        "        ...,\n",
        "        description=\"Temporal frequency for aggregation robustness checks.\",\n",
        "        eq=\"Quarterly\"\n",
        "    )\n",
        "\n",
        "    # Define the rule for the first-month GDELT measure.\n",
        "    # Source: Table A10, Column 4 Header, page 36\n",
        "    first_month_gdelt_measure: str = Field(\n",
        "        ...,\n",
        "        description=\"Rule for constructing the quarterly GDELT measure.\",\n",
        "        eq=\"Use GDELT index value from only the first month of each quarter\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Parameters(BaseModel):\n",
        "    \"\"\"\n",
        "    A Pydantic model that nests the parameter sub-dictionaries.\n",
        "\n",
        "    This model acts as an intermediate container, ensuring that the main\n",
        "    'parameters' key in the configuration dictionary contains the three\n",
        "    expected sub-dictionaries and nothing else.\n",
        "\n",
        "    Attributes:\n",
        "        particle_filter_params (ParticleFilterParams): The validated particle\n",
        "                                                      filter parameters object.\n",
        "        estimation_params (EstimationParams): The validated estimation\n",
        "                                              parameters object.\n",
        "        robustness_check_params (RobustnessCheckParams): The validated\n",
        "                                                         robustness check\n",
        "                                                         parameters object.\n",
        "    \"\"\"\n",
        "    # Configure the model to forbid any extra, undefined fields.\n",
        "    model_config = ConfigDict(extra='forbid')\n",
        "\n",
        "    # Nest the previously defined Pydantic models as fields.\n",
        "    # Pydantic will automatically delegate validation to these sub-models.\n",
        "    particle_filter_params: ParticleFilterParams\n",
        "    estimation_params: EstimationParams\n",
        "    robustness_check_params: RobustnessCheckParams\n",
        "\n",
        "\n",
        "class MasterConfig(BaseModel):\n",
        "    \"\"\"\n",
        "    The top-level Pydantic model for validating the entire configuration dictionary.\n",
        "\n",
        "    This model defines the root structure of the configuration, which must\n",
        "    contain a single key, 'parameters', whose value is an object that conforms\n",
        "    to the `Parameters` schema.\n",
        "\n",
        "    Attributes:\n",
        "        parameters (Parameters): The validated, nested object containing all\n",
        "                                 project parameters.\n",
        "    \"\"\"\n",
        "    # Configure the model to forbid any extra fields at the top level.\n",
        "    model_config = ConfigDict(extra='forbid')\n",
        "\n",
        "    # Define the single top-level key required in the configuration dictionary.\n",
        "    parameters: Parameters\n",
        "\n",
        "\n",
        "def validate_config_structure(config: Dict[str, Any]) -> MasterConfig:\n",
        "    \"\"\"\n",
        "    Validates the structure, types, and exact values of the master configuration\n",
        "    dictionary using a Pydantic schema.\n",
        "\n",
        "    This function ensures that the configuration dictionary adheres precisely to\n",
        "    the specifications required for the replication study, preventing runtime\n",
        "    errors due to malformed or incorrect configuration parameters.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The master configuration dictionary to be\n",
        "                                 validated.\n",
        "\n",
        "    Returns:\n",
        "        MasterConfig: A validated Pydantic model instance of the configuration,\n",
        "                      providing attribute-style access and type safety.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the configuration dictionary fails validation. The error\n",
        "                    message contains detailed information about the validation\n",
        "                    failures, including the exact location and nature of the\n",
        "                    discrepancy.\n",
        "    \"\"\"\n",
        "    # Check if the input is a dictionary.\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"Configuration must be a dictionary.\")\n",
        "\n",
        "    try:\n",
        "        # Attempt to parse and validate the input dictionary against the schema.\n",
        "        # Pydantic handles the recursive validation of all nested models.\n",
        "        validated_config = MasterConfig.model_validate(config)\n",
        "\n",
        "        # Return the validated Pydantic object upon success.\n",
        "        return validated_config\n",
        "\n",
        "    except ValidationError as e:\n",
        "        # If Pydantic's validation fails, catch the detailed error.\n",
        "        # Format the error into a clear, actionable message and raise a\n",
        "        # standard ValueError, which is more conventional for parameter validation.\n",
        "        error_details = e.errors()\n",
        "\n",
        "        # Create a formatted, readable error report from Pydantic's output.\n",
        "        report = \"\\n\".join(\n",
        "            [f\"  - Location: {'.'.join(map(str, err['loc']))}, Error: {err['msg']}\" for err in error_details]\n",
        "        )\n",
        "\n",
        "        # Raise a comprehensive ValueError.\n",
        "        raise ValueError(f\"Master configuration validation failed with the following errors:\\n{report}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1, Step 2: Lambda Function and Functional Parameter Parsing\n",
        "# ==============================================================================\n",
        "\n",
        "def _is_safe_ast_node(node: ast.AST) -> bool:\n",
        "    \"\"\"\n",
        "    Recursively checks if an Abstract Syntax Tree (AST) node and all its\n",
        "    children are within a predefined whitelist of safe nodes.\n",
        "\n",
        "    This is a security helper function to prevent arbitrary code execution when\n",
        "    parsing the lambda function string from the configuration.\n",
        "\n",
        "    Args:\n",
        "        node (ast.AST): The AST node to inspect.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the node and all its children are safe, False otherwise.\n",
        "    \"\"\"\n",
        "    # Define the whitelist of allowed AST node types. This list permits\n",
        "    # basic arithmetic operations, variable names, numbers, and the lambda form.\n",
        "    # It explicitly disallows imports, function calls (except lambda), etc.\n",
        "    SAFE_NODES = {\n",
        "        ast.Expression, ast.Lambda, ast.Name, ast.Load, ast.BinOp,\n",
        "        ast.UnaryOp, ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow,\n",
        "        ast.Num, ast.Constant, ast.arg, ast.arguments\n",
        "    }\n",
        "\n",
        "    # Check if the current node's type is in the whitelist.\n",
        "    if type(node) not in SAFE_NODES:\n",
        "        return False\n",
        "\n",
        "    # Recursively check all child nodes of the current node.\n",
        "    for child_node in ast.iter_child_nodes(node):\n",
        "        if not _is_safe_ast_node(child_node):\n",
        "            return False\n",
        "\n",
        "    # If the node and all its children are safe, return True.\n",
        "    return True\n",
        "\n",
        "\n",
        "def parse_functional_parameters(validated_config: MasterConfig) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Parses string-based functional parameters from the validated configuration\n",
        "    into executable Python objects and validates numerical bounds.\n",
        "\n",
        "    This function safely evaluates the lambda string for the observation\n",
        "    variance function after a rigorous security check using Abstract Syntax\n",
        "    Trees (AST). It also extracts and validates key numerical parameters.\n",
        "\n",
        "    Args:\n",
        "        validated_config (MasterConfig): The validated Pydantic model instance\n",
        "                                         of the configuration.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the parsed callable function\n",
        "                        and validated numerical parameters, ready for use in\n",
        "                        downstream computations.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the lambda string is syntactically incorrect, contains\n",
        "                    unsafe operations, or if numerical parameters are invalid.\n",
        "        TypeError: If the parsed lambda string does not result in a callable\n",
        "                   function.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the parsed functional parameters.\n",
        "    functional_params = {}\n",
        "\n",
        "    # --- Parse and Validate Observation Variance Function ---\n",
        "\n",
        "    # Extract the lambda string from the validated configuration.\n",
        "    lambda_str = validated_config.parameters.particle_filter_params.observation_variance_function\n",
        "\n",
        "    try:\n",
        "        # Safely parse the string into an Abstract Syntax Tree (AST).\n",
        "        # This will fail with a SyntaxError for malformed Python code.\n",
        "        tree = ast.parse(lambda_str, mode='eval')\n",
        "\n",
        "        # Perform a security check on the AST to ensure it contains only whitelisted operations.\n",
        "        if not _is_safe_ast_node(tree):\n",
        "            # If the AST contains disallowed nodes, raise a security error.\n",
        "            raise ValueError(f\"Lambda string '{lambda_str}' contains unsafe operations.\")\n",
        "\n",
        "        # If the AST is safe, compile and evaluate it in a restricted namespace.\n",
        "        # The 'globals' dictionary is empty, preventing access to any modules or functions.\n",
        "        # The '__builtins__' are also cleared for maximum security.\n",
        "        observation_func = eval(compile(tree, filename='<string>', mode='eval'), {\"__builtins__\": {}})\n",
        "\n",
        "        # Verify that the evaluated object is a callable function.\n",
        "        if not isinstance(observation_func, Callable):\n",
        "            raise TypeError(\"Parsed observation_variance_function is not a callable function.\")\n",
        "\n",
        "        # Perform a functional test on the parsed lambda function.\n",
        "        # Test with a sample input to ensure it behaves as expected.\n",
        "        test_input = 100\n",
        "        expected_output = 1 / (test_input + 1)\n",
        "        if not np.isclose(observation_func(test_input), expected_output):\n",
        "            raise ValueError(\"Parsed lambda function failed functional validation test.\")\n",
        "\n",
        "        # Store the validated, callable function.\n",
        "        functional_params['R_function'] = observation_func\n",
        "\n",
        "    except SyntaxError:\n",
        "        # Catch parsing errors for malformed lambda strings.\n",
        "        raise ValueError(f\"Lambda string '{lambda_str}' is not valid Python syntax.\")\n",
        "    except Exception as e:\n",
        "        # Re-raise other validation errors with context.\n",
        "        raise ValueError(f\"Failed to parse functional parameters: {e}\")\n",
        "\n",
        "    # --- Extract and Validate Numerical Parameters ---\n",
        "\n",
        "    # Extract the process variance floor.\n",
        "    q_floor = validated_config.parameters.particle_filter_params.process_variance_floor\n",
        "    # Validate that it is a non-negative float.\n",
        "    if not (isinstance(q_floor, float) and q_floor >= 0):\n",
        "        raise ValueError(\"process_variance_floor must be a non-negative float.\")\n",
        "    functional_params['Q_floor'] = q_floor\n",
        "\n",
        "    # Extract the process variance default value.\n",
        "    q_default = validated_config.parameters.particle_filter_params.process_variance_default\n",
        "    # Validate that it is a positive float.\n",
        "    if not (isinstance(q_default, float) and q_default > 0):\n",
        "        raise ValueError(\"process_variance_default must be a positive float.\")\n",
        "    functional_params['Q_default'] = q_default\n",
        "\n",
        "    # Return the dictionary of parsed and validated parameters.\n",
        "    return functional_params\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1, Step 3: Global Constants and Derived Parameters Initialization\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ProjectConstants:\n",
        "    \"\"\"\n",
        "    An immutable dataclass to store and provide access to all validated project\n",
        "    constants and derived parameters.\n",
        "\n",
        "    This class serves as the single source of truth for all configuration\n",
        "    parameters throughout the research pipeline, ensuring consistency and\n",
        "    preventing accidental modification of parameters during runtime. The `frozen=True`\n",
        "    argument makes instances of this class immutable.\n",
        "\n",
        "    Attributes:\n",
        "        M_PARTICLES (Final[int]): Number of particles for the Particle Filter.\n",
        "        R_FUNCTION (Final[Callable[[int], float]]): Parsed observation variance function.\n",
        "        Q_FLOOR (Final[float]): Floor for the process variance (Q_ij).\n",
        "        Q_DEFAULT (Final[float]): Default value for process variance if AR(1) fit fails.\n",
        "        BASELINE_MIN_OBS (Final[int]): Minimum non-zero observations for baseline GDELT index.\n",
        "        ROBUSTNESS_MIN_OBS (Final[int]): Stricter minimum observations for robustness check.\n",
        "        N_BOOTSTRAP (Final[int]): Number of bootstrap replications for Logit SEs.\n",
        "    \"\"\"\n",
        "    # Define class attributes with Final type hints for clarity and static analysis.\n",
        "    M_PARTICLES: Final[int]\n",
        "    R_FUNCTION: Final[Callable[[int], float]]\n",
        "    Q_FLOOR: Final[float]\n",
        "    Q_DEFAULT: Final[float]\n",
        "    BASELINE_MIN_OBS: Final[int]\n",
        "    ROBUSTNESS_MIN_OBS: Final[int]\n",
        "    N_BOOTSTRAP: Final[int]\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"\n",
        "        Performs post-initialization validation on the constants.\n",
        "\n",
        "        This method is automatically called by the dataclass constructor after\n",
        "        initialization. It is used here to enforce logical constraints between\n",
        "        parameters that cannot be checked by simple type or value validation.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If any cross-parameter validation checks fail.\n",
        "        \"\"\"\n",
        "        # Validate the logical relationship between Q_floor and Q_default.\n",
        "        # The default variance must be greater than or equal to the floor.\n",
        "        if self.Q_DEFAULT < self.Q_FLOOR:\n",
        "            raise ValueError(\n",
        "                f\"Q_default ({self.Q_DEFAULT}) must be greater than or equal to \"\n",
        "                f\"Q_floor ({self.Q_FLOOR}).\"\n",
        "            )\n",
        "\n",
        "        # Validate the logical relationship between observation thresholds.\n",
        "        # The robustness threshold should be stricter (greater) than the baseline.\n",
        "        if self.ROBUSTNESS_MIN_OBS <= self.BASELINE_MIN_OBS:\n",
        "            raise ValueError(\n",
        "                f\"Robustness minimum observations ({self.ROBUSTNESS_MIN_OBS}) \"\n",
        "                f\"must be strictly greater than the baseline minimum \"\n",
        "                f\"({self.BASELINE_MIN_OBS}).\"\n",
        "            )\n",
        "\n",
        "\n",
        "def initialize_project_constants(\n",
        "    validated_config: MasterConfig,\n",
        "    functional_params: Dict[str, Any]\n",
        ") -> ProjectConstants:\n",
        "    \"\"\"\n",
        "    Initializes and validates the immutable ProjectConstants object.\n",
        "\n",
        "    This function extracts parameters from the validated configuration and the\n",
        "    parsed functional parameters dictionary, instantiates the frozen dataclass,\n",
        "    and runs post-initialization validation checks.\n",
        "\n",
        "    Args:\n",
        "        validated_config (MasterConfig): The Pydantic-validated configuration object.\n",
        "        functional_params (Dict[str, Any]): The dictionary of parsed callables\n",
        "                                            and validated numerical parameters.\n",
        "\n",
        "    Returns:\n",
        "        ProjectConstants: An immutable, validated dataclass instance containing\n",
        "                          all necessary project constants.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If a required key is missing from the input dictionaries.\n",
        "        ValueError: If post-initialization validation within the dataclass fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Instantiate the frozen dataclass, mapping config values to attributes.\n",
        "        # This structure is explicit and robust to changes in dictionary order.\n",
        "        constants = ProjectConstants(\n",
        "            M_PARTICLES=validated_config.parameters.particle_filter_params.M_particles,\n",
        "            R_FUNCTION=functional_params['R_function'],\n",
        "            Q_FLOOR=functional_params['Q_floor'],\n",
        "            Q_DEFAULT=functional_params['Q_default'],\n",
        "            BASELINE_MIN_OBS=validated_config.parameters.particle_filter_params.baseline_min_obs_threshold,\n",
        "            ROBUSTNESS_MIN_OBS=validated_config.parameters.particle_filter_params.robustness_min_obs_threshold,\n",
        "            N_BOOTSTRAP=validated_config.parameters.estimation_params.bootstrap_replications\n",
        "        )\n",
        "\n",
        "        # Return the successfully created and validated constants object.\n",
        "        return constants\n",
        "\n",
        "    except (KeyError, AttributeError) as e:\n",
        "        # Catch potential errors if the input objects are malformed,\n",
        "        # which should not happen if they come from the previous validation steps.\n",
        "        raise KeyError(f\"Failed to initialize constants due to missing parameter: {e}\")\n",
        "    except (ValueError, TypeError) as e:\n",
        "        # Catch validation errors from the dataclass's __post_init__ method.\n",
        "        raise ValueError(f\"Post-initialization validation of constants failed: {e}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_configuration_and_validation(\n",
        "    config: Dict[str, Any]\n",
        ") -> ProjectConstants:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire configuration validation and setup process.\n",
        "\n",
        "    This master function serves as the single entry point for Task 1. It executes\n",
        "    the three core steps in sequence:\n",
        "    1. Validates the raw configuration dictionary's structure and values.\n",
        "    2. Parses functional parameters (e.g., lambda strings) into callable objects.\n",
        "    3. Initializes a final, immutable dataclass of project constants.\n",
        "\n",
        "    The function is designed to be transactional: it either completes\n",
        "    successfully and returns the constants object, or it fails and raises a\n",
        "    single, comprehensive error.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The raw master configuration dictionary for the\n",
        "                                 replication study.\n",
        "\n",
        "    Returns:\n",
        "        ProjectConstants: An immutable, fully validated dataclass instance\n",
        "                          containing all project parameters, ready for use in\n",
        "                          the subsequent phases of the analysis.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any stage of the validation or parsing fails.\n",
        "        TypeError: If the input configuration is not a dictionary or if a\n",
        "                   parsed function is not callable.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Validate the overall structure, types, and values of the config dict.\n",
        "        # This step ensures the configuration conforms to the required schema.\n",
        "        print(\"Step 1: Validating configuration structure...\")\n",
        "        validated_config_model = validate_config_structure(config)\n",
        "        print(\"Step 1: Success. Configuration structure is valid.\")\n",
        "\n",
        "        # Step 2: Parse string-based functions and validate functional parameters.\n",
        "        # This step safely converts strings to callables and checks numerical bounds.\n",
        "        print(\"Step 2: Parsing functional parameters...\")\n",
        "        parsed_functional_params = parse_functional_parameters(validated_config_model)\n",
        "        print(\"Step 2: Success. Functional parameters parsed and validated.\")\n",
        "\n",
        "        # Step 3: Initialize the final, immutable container for all project constants.\n",
        "        # This step creates the single source of truth for all downstream tasks.\n",
        "        print(\"Step 3: Initializing project constants object...\")\n",
        "        project_constants = initialize_project_constants(\n",
        "            validated_config=validated_config_model,\n",
        "            functional_params=parsed_functional_params\n",
        "        )\n",
        "        print(\"Step 3: Success. Immutable project constants object created.\")\n",
        "\n",
        "        # If all steps succeed, return the final constants object.\n",
        "        return project_constants\n",
        "\n",
        "    except (ValueError, TypeError, KeyError) as e:\n",
        "        # Catch any exception raised during the process.\n",
        "        # This provides a single, clear failure point for the entire task.\n",
        "        print(f\"ERROR: Configuration and validation failed.\")\n",
        "        # Re-raise the exception to halt execution and provide a detailed error message.\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "S69S1UA5a6n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: DataFrame Structure Validation and Quality Assessment\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2, Step 1: Core DataFrame Schema Validation\n",
        "# ==============================================================================\n",
        "\n",
        "def validate_dataframe_schema(\n",
        "    df: pd.DataFrame,\n",
        "    expected_columns: Dict[str, Type]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the core schema of the input DataFrame against expected columns and types.\n",
        "\n",
        "    This function performs a series of rigorous checks to ensure the DataFrame's\n",
        "    structure is perfectly aligned with the project's requirements. It validates:\n",
        "    1. The exact number of columns.\n",
        "    2. The exact names of all columns.\n",
        "    3. The data type of each column.\n",
        "    4. The format of ISO country codes and the DyadicPair identifier.\n",
        "    5. The integrity of the DatetimeIndex, including uniqueness within dyads.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to be validated.\n",
        "        expected_columns (Dict[str, Type]): A dictionary mapping column names\n",
        "                                            to their expected data types.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any schema validation check fails, with a detailed\n",
        "                    message explaining the discrepancy.\n",
        "        TypeError: If the input is not a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    # --- Initial Input Validation ---\n",
        "    # Ensure the input object is a pandas DataFrame.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Column Count and Name Validation ---\n",
        "    # Check if the number of columns matches the expected count (24).\n",
        "    if len(df.columns) != len(expected_columns):\n",
        "        raise ValueError(\n",
        "            f\"DataFrame must have exactly {len(expected_columns)} columns, but found {len(df.columns)}.\"\n",
        "        )\n",
        "    # Check if the set of column names matches the expected set.\n",
        "    if set(df.columns) != set(expected_columns.keys()):\n",
        "        missing = set(expected_columns.keys()) - set(df.columns)\n",
        "        extra = set(df.columns) - set(expected_columns.keys())\n",
        "        raise ValueError(\n",
        "            f\"Column name mismatch. Missing: {missing if missing else 'None'}. \"\n",
        "            f\"Extra: {extra if extra else 'None'}.\"\n",
        "        )\n",
        "\n",
        "    # --- Data Type Validation ---\n",
        "    # Iterate through each expected column and its type to validate dtypes.\n",
        "    for col, expected_dtype in expected_columns.items():\n",
        "        # Compare the actual dtype with the expected dtype.\n",
        "        if df[col].dtype != expected_dtype:\n",
        "            raise TypeError(\n",
        "                f\"Column '{col}' has incorrect dtype. Expected {expected_dtype}, \"\n",
        "                f\"but found {df[col].dtype}.\"\n",
        "            )\n",
        "\n",
        "    # --- Format Validation for Key Identifier Columns ---\n",
        "    # Define the regex pattern for ISO 3166-1 alpha-3 country codes.\n",
        "    iso_pattern = re.compile(r'^[A-Z]{3}$')\n",
        "    # Vectorized check for 'ExporterISO' format.\n",
        "    if not df['ExporterISO'].str.match(iso_pattern).all():\n",
        "        raise ValueError(\"Column 'ExporterISO' contains invalid ISO 3166-1 alpha-3 codes.\")\n",
        "    # Vectorized check for 'ImporterISO' format.\n",
        "    if not df['ImporterISO'].str.match(iso_pattern).all():\n",
        "        raise ValueError(\"Column 'ImporterISO' contains invalid ISO 3166-1 alpha-3 codes.\")\n",
        "\n",
        "    # --- DyadicPair Construction Validation ---\n",
        "    # Construct the expected 'DyadicPair' series from its components.\n",
        "    expected_dyadic_pair = df['ExporterISO'] + '_' + df['ImporterISO']\n",
        "    # Perform a byte-exact, efficient comparison with the existing column.\n",
        "    if not df['DyadicPair'].equals(expected_dyadic_pair):\n",
        "        raise ValueError(\"Column 'DyadicPair' is not correctly constructed as 'ExporterISO_ImporterISO'.\")\n",
        "\n",
        "    # --- Index Validation ---\n",
        "    # Check if the index is a DatetimeIndex and has the correct resolution.\n",
        "    if not isinstance(df.index, pd.DatetimeIndex) or df.index.dtype != 'datetime64[ns]':\n",
        "        raise TypeError(\"DataFrame index must be of type datetime64[ns].\")\n",
        "    # Check for duplicate timestamps within each dyadic pair.\n",
        "    if not df.groupby('DyadicPair').apply(lambda x: x.index.is_unique).all():\n",
        "        raise ValueError(\"Duplicate timestamps found within one or more dyadic pairs.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2, Step 2: Numerical Variables and Range Validation\n",
        "# ==============================================================================\n",
        "\n",
        "def validate_numerical_ranges(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the ranges and properties of key numerical variables.\n",
        "\n",
        "    This function performs checks on numerical columns to ensure their values\n",
        "    fall within theoretically or practically defined bounds. It handles NaN\n",
        "    values correctly by excluding them from the validation checks.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to be validated.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any numerical variable fails its range or property check.\n",
        "    \"\"\"\n",
        "    # --- Trade Value Validation (Non-negative) ---\n",
        "    # Check that all non-null trade values are greater than or equal to zero.\n",
        "    for col in ['BilateralTradeValue_USD', 'BACI_TotalValue_USD']:\n",
        "        if not (df.loc[df[col].notna(), col] >= 0).all():\n",
        "            raise ValueError(f\"Column '{col}' contains negative values.\")\n",
        "\n",
        "    # --- GDELT Variable Validation ---\n",
        "    # Check that Goldstein scores are within the [-10, +10] range.\n",
        "    if not df['GDELT_GoldsteinMean'].dropna().between(-10, 10).all():\n",
        "        raise ValueError(\"Column 'GDELT_GoldsteinMean' has values outside the [-10, 10] range.\")\n",
        "    # Check that event counts are non-negative integers.\n",
        "    if not (df.loc[df['GDELT_EventCount'].notna(), 'GDELT_EventCount'] >= 0).all():\n",
        "        raise ValueError(\"Column 'GDELT_EventCount' contains negative values.\")\n",
        "\n",
        "    # --- Governance Variable Validation ---\n",
        "    # Check that Polity scores are within the [-10, +10] integer range.\n",
        "    for col in ['PolityScore_Exporter', 'PolityScore_Importer']:\n",
        "        if not df[col].dropna().between(-10, 10).all():\n",
        "            raise ValueError(f\"Column '{col}' has values outside the [-10, 10] range.\")\n",
        "    # Check that Corruption Index values are within the [0, 1] range.\n",
        "    for col in ['CorruptionIndex_Exporter', 'CorruptionIndex_Importer']:\n",
        "        if not df[col].dropna().between(0, 1).all():\n",
        "            raise ValueError(f\"Column '{col}' has values outside the [0, 1] range.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2, Step 3: Derived and Constructed Variables Validation\n",
        "# ==============================================================================\n",
        "\n",
        "def validate_derived_variables(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the logical and mathematical consistency of derived variables.\n",
        "\n",
        "    This function cross-checks variables that are constructed from other columns\n",
        "    to ensure they have been calculated correctly. It uses tolerance-based\n",
        "    comparisons for floating-point arithmetic to avoid precision-related issues.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to be validated.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any derived variable is found to be inconsistent with\n",
        "                    its source variables.\n",
        "    \"\"\"\n",
        "    # --- DemocraticPair Validation ---\n",
        "    # Create the expected boolean mask based on the definition.\n",
        "    expected_democratic_mask = (df['PolityScore_Exporter'] > 6) & (df['PolityScore_Importer'] > 6)\n",
        "    # Compare the existing column with the expected integer representation of the mask.\n",
        "    if not df['DemocraticPair'].equals(expected_democratic_mask.astype(np.int64)):\n",
        "        raise ValueError(\"Column 'DemocraticPair' is not correctly derived from Polity scores.\")\n",
        "\n",
        "    # --- PoliticalDistance Validation ---\n",
        "    # Compute the expected political distance.\n",
        "    expected_pol_dist = np.abs(df['PolityScore_Exporter'] - df['PolityScore_Importer'])\n",
        "    # Use np.allclose for robust floating-point comparison, though source is int.\n",
        "    if not np.allclose(df['PoliticalDistance'], expected_pol_dist.astype(np.float64), equal_nan=True):\n",
        "        raise ValueError(\"Column 'PoliticalDistance' is not the absolute difference of Polity scores.\")\n",
        "\n",
        "    # --- DomesticTrade Validation (with 1% tolerance) ---\n",
        "    # Compute expected domestic trade for exporter.\n",
        "    expected_dom_trade_exp = df['GDP_USD_Exporter'] - df['TotalExports_USD_Exporter']\n",
        "    # Use np.allclose with a relative tolerance of 1% for robust comparison.\n",
        "    if not np.allclose(df['DomesticTrade_Exporter'], expected_dom_trade_exp, rtol=0.01, equal_nan=True):\n",
        "        raise ValueError(\"Column 'DomesticTrade_Exporter' is not consistent with GDP and Exports (within 1% tolerance).\")\n",
        "\n",
        "    # --- Binary Institutional Variable Validation ---\n",
        "    # Check that institutional indicators contain only 0s and 1s.\n",
        "    for col in ['GATTWTO_Both', 'GATTWTO_One', 'RTA']:\n",
        "        if not df[col].dropna().isin([0, 1]).all():\n",
        "            raise ValueError(f\"Column '{col}' must be a binary indicator (0 or 1).\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_dataframe_validation_and_assessment(\n",
        "    df: pd.DataFrame,\n",
        "    expected_columns: Dict[str, Type]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire DataFrame validation and quality assessment process.\n",
        "\n",
        "    This master function executes the three core validation steps for Task 2 in\n",
        "    a predefined sequence:\n",
        "    1. Core schema validation (columns, types, identifiers).\n",
        "    2. Numerical range validation.\n",
        "    3. Derived variable consistency checks.\n",
        "\n",
        "    It provides a single entry point for validating the input data, ensuring it\n",
        "    is fit for the subsequent analysis phases. The function is transactional,\n",
        "    halting and raising a detailed error upon the first validation failure.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame to be validated.\n",
        "        expected_columns (Dict[str, Type]): A dictionary mapping column names\n",
        "                                            to their expected data types.\n",
        "\n",
        "    Returns:\n",
        "        bool: Returns True if all validation checks pass successfully.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any validation check fails.\n",
        "        TypeError: If the input is not a DataFrame or contains dtype errors.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Validate the fundamental structure and schema of the DataFrame.\n",
        "        print(\"Step 1: Validating core DataFrame schema...\")\n",
        "        validate_dataframe_schema(df, expected_columns)\n",
        "        print(\"Step 1: Success. Core schema is valid.\")\n",
        "\n",
        "        # Step 2: Validate the ranges of key numerical variables.\n",
        "        print(\"Step 2: Validating numerical variable ranges...\")\n",
        "        validate_numerical_ranges(df)\n",
        "        print(\"Step 2: Success. Numerical ranges are valid.\")\n",
        "\n",
        "        # Step 3: Validate the consistency of all derived and constructed variables.\n",
        "        print(\"Step 3: Validating derived variable consistency...\")\n",
        "        validate_derived_variables(df)\n",
        "        print(\"Step 3: Success. Derived variables are consistent.\")\n",
        "\n",
        "        # If all steps complete without raising an exception, the validation is successful.\n",
        "        print(\"\\nSUCCESS: DataFrame has passed all validation and quality checks.\")\n",
        "        return True\n",
        "\n",
        "    except (ValueError, TypeError) as e:\n",
        "        # Catch any validation error from the helper functions.\n",
        "        print(f\"\\nERROR: DataFrame validation failed.\")\n",
        "        # Re-raise the specific error to provide a detailed failure report.\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "uYFJT6rAelc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Data Quality Assessment and Missing Value Analysis\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3, Step 1: Comprehensive Missing Value Audit\n",
        "# ==============================================================================\n",
        "\n",
        "def audit_missing_values(\n",
        "    df: pd.DataFrame,\n",
        "    high_missingness_threshold: float = 30.0\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs a comprehensive audit of missing values in the DataFrame.\n",
        "\n",
        "    This function calculates per-column missing value percentages, computes a\n",
        "    correlation matrix of missingness to identify patterns, and flags dyadic\n",
        "    pairs that exceed a specified threshold of missing data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame to audit.\n",
        "        high_missingness_threshold (float): The percentage threshold (0-100)\n",
        "                                            above which a dyadic pair is\n",
        "                                            flagged for high missingness.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the audit results:\n",
        "                        - 'missing_percentages': pd.Series with the percentage\n",
        "                          of missing values for each column.\n",
        "                        - 'missingness_correlation': pd.DataFrame showing the\n",
        "                          correlation of missingness between columns.\n",
        "                        - 'high_missingness_dyads': pd.Series of dyadic pairs\n",
        "                          that exceed the missingness threshold.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify the input is a pandas DataFrame.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    # Verify the threshold is a valid percentage.\n",
        "    if not (0.0 <= high_missingness_threshold <= 100.0):\n",
        "        raise ValueError(\"high_missingness_threshold must be between 0 and 100.\")\n",
        "\n",
        "    # --- Calculate Missing Value Percentages ---\n",
        "    # Use the efficient vectorized .isnull().mean() method.\n",
        "    missing_percentages = df.isnull().mean() * 100\n",
        "    missing_percentages.name = \"MissingValuePercentage\"\n",
        "\n",
        "    # --- Create Missingness Correlation Matrix ---\n",
        "    # Convert the boolean DataFrame of nulls to integers (0 or 1).\n",
        "    # Then, compute the Pearson correlation coefficient between these indicators.\n",
        "    missingness_correlation = df.isnull().astype(int).corr()\n",
        "\n",
        "    # --- Flag Dyadic Pairs with Excessive Missing Values ---\n",
        "    # Group by 'DyadicPair' to analyze each time series independently.\n",
        "    # For each group, calculate the mean of all boolean null indicators.\n",
        "    # This gives the overall percentage of missing cells for that dyad.\n",
        "    dyad_missingness = df.groupby('DyadicPair').apply(\n",
        "        lambda g: g.isnull().values.mean() * 100\n",
        "    )\n",
        "    dyad_missingness.name = \"DyadMissingnessPercentage\"\n",
        "    # Create a boolean mask to identify pairs exceeding the threshold.\n",
        "    high_missingness_dyads = dyad_missingness[dyad_missingness > high_missingness_threshold]\n",
        "\n",
        "    # --- Compile and Return Results ---\n",
        "    # Store the results in a structured dictionary.\n",
        "    audit_results = {\n",
        "        'missing_percentages': missing_percentages,\n",
        "        'missingness_correlation': missingness_correlation,\n",
        "        'high_missingness_dyads': high_missingness_dyads\n",
        "    }\n",
        "    return audit_results\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3, Step 2: Temporal Coverage and Panel Balance Assessment\n",
        "# ==============================================================================\n",
        "\n",
        "def assess_temporal_coverage(\n",
        "    df: pd.DataFrame,\n",
        "    max_gap_tolerance: pd.Timedelta = pd.Timedelta('366 days')\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Assesses the temporal coverage, continuity, and balance of the panel data.\n",
        "\n",
        "    This function calculates the length of the time series for each dyad,\n",
        "    validates that time is always moving forward (monotonicity), and identifies\n",
        "    any significant temporal gaps that might indicate data collection issues.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with a DatetimeIndex.\n",
        "        max_gap_tolerance (pd.Timedelta): The maximum allowable time gap\n",
        "                                          between consecutive observations\n",
        "                                          before it is flagged.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the assessment results:\n",
        "                        - 'timeseries_lengths': pd.Series with the number of\n",
        "                          observations for each dyadic pair.\n",
        "                        - 'is_monotonic': A boolean indicating if all dyadic\n",
        "                          time series are monotonically increasing.\n",
        "                        - 'temporal_gaps': A DataFrame detailing all identified\n",
        "                          gaps that exceed the tolerance.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify the input is a pandas DataFrame with a DatetimeIndex.\n",
        "    if not isinstance(df, pd.DataFrame) or not isinstance(df.index, pd.DatetimeIndex):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame with a DatetimeIndex.\")\n",
        "\n",
        "    # --- Calculate Time Series Length per Dyad ---\n",
        "    # Use the efficient .groupby().size() method.\n",
        "    timeseries_lengths = df.groupby('DyadicPair').size()\n",
        "    timeseries_lengths.name = \"ObservationCount\"\n",
        "\n",
        "    # --- Validate Temporal Monotonicity ---\n",
        "    # For each dyad, check if the DatetimeIndex is monotonically increasing.\n",
        "    # .all() confirms this holds true for every single dyad.\n",
        "    is_monotonic = df.groupby('DyadicPair').apply(\n",
        "        lambda g: g.index.is_monotonic_increasing\n",
        "    ).all()\n",
        "\n",
        "    # --- Identify Temporal Gaps ---\n",
        "    # Calculate the time difference between consecutive observations within each dyad.\n",
        "    time_diffs = df.groupby('DyadicPair').apply(lambda g: g.index.diff()).reset_index(level=0, drop=True)\n",
        "    # Create a boolean mask for gaps exceeding the specified tolerance.\n",
        "    gap_mask = time_diffs > max_gap_tolerance\n",
        "    # Select the rows where a significant gap begins.\n",
        "    temporal_gaps = df[gap_mask].copy()\n",
        "    # Add the size of the gap as a new column for context.\n",
        "    temporal_gaps['GapSize'] = time_diffs[gap_mask]\n",
        "\n",
        "    # --- Compile and Return Results ---\n",
        "    assessment_results = {\n",
        "        'timeseries_lengths': timeseries_lengths,\n",
        "        'is_monotonic': is_monotonic,\n",
        "        'temporal_gaps': temporal_gaps\n",
        "    }\n",
        "    return assessment_results\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3, Step 3: Cross-Variable Consistency Validation\n",
        "# ==============================================================================\n",
        "\n",
        "def validate_cross_variable_consistency(df: pd.DataFrame) -> Dict[str, pd.Series]:\n",
        "    \"\"\"\n",
        "    Performs consistency checks between related variables in the DataFrame.\n",
        "\n",
        "    This function validates logical and mathematical relationships that should\n",
        "    hold true in the data, such as trade value comparisons, GDP vs. exports,\n",
        "    and the coherence of institutional and event-based indicators.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame to validate.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.Series]: A dictionary where keys describe the check and\n",
        "                              values are boolean Series. `True` in a Series\n",
        "                              indicates a row that failed the consistency check.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Initialize Results Dictionary ---\n",
        "    consistency_failures = {}\n",
        "\n",
        "    # --- Trade Value Comparison (DOTS vs. BACI) ---\n",
        "    # Create a mask for rows where the comparison is valid (non-null, positive denominator).\n",
        "    valid_trade_mask = (df['BilateralTradeValue_USD'] > 0) & (df['BACI_TotalValue_USD'].notna())\n",
        "    # Calculate the percentage difference only on the valid subset.\n",
        "    pct_diff = np.abs(\n",
        "        df.loc[valid_trade_mask, 'BilateralTradeValue_USD'] - df.loc[valid_trade_mask, 'BACI_TotalValue_USD']\n",
        "    ) / df.loc[valid_trade_mask, 'BilateralTradeValue_USD']\n",
        "    # Identify rows where the discrepancy exceeds 10%.\n",
        "    consistency_failures['trade_value_discrepancies'] = pct_diff > 0.10\n",
        "\n",
        "    # --- GDP vs. Exports Validation ---\n",
        "    # Identify impossible cases where total exports exceed GDP.\n",
        "    # A small epsilon (1.0 USD) is added to GDP to handle rounding differences.\n",
        "    consistency_failures['impossible_gdp_export_ratio'] = (\n",
        "        df['TotalExports_USD_Exporter'] > (df['GDP_USD_Exporter'] + 1.0)\n",
        "    )\n",
        "\n",
        "    # --- Institutional Variable Consistency ---\n",
        "    # A dyad cannot have both one and two members in GATT/WTO simultaneously.\n",
        "    consistency_failures['institutional_exclusivity_violation'] = (\n",
        "        (df['GATTWTO_Both'] + df['GATTWTO_One']) > 1\n",
        "    )\n",
        "\n",
        "    # --- GDELT Data Consistency ---\n",
        "    # A non-null Goldstein score should not exist if the event count is zero.\n",
        "    consistency_failures['gdelt_event_score_inconsistency'] = (\n",
        "        (df['GDELT_EventCount'] == 0) & (df['GDELT_GoldsteinMean'].notna())\n",
        "    )\n",
        "\n",
        "    return consistency_failures\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_data_quality_assessment(\n",
        "    df: pd.DataFrame,\n",
        "    high_missingness_threshold: float = 30.0,\n",
        "    max_gap_tolerance: pd.Timedelta = pd.Timedelta('366 days')\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire data quality and missing value assessment process.\n",
        "\n",
        "    This master function executes the three core assessment steps for Task 3:\n",
        "    1. A comprehensive audit of missing values.\n",
        "    2. An assessment of temporal coverage and panel balance.\n",
        "    3. A series of cross-variable consistency checks.\n",
        "\n",
        "    It aggregates the results from these steps into a single, comprehensive\n",
        "    report, providing a deep understanding of the dataset's quality.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame to be assessed.\n",
        "        high_missingness_threshold (float): The percentage threshold for flagging\n",
        "                                            dyads with high missingness.\n",
        "        max_gap_tolerance (pd.Timedelta): The maximum allowable time gap between\n",
        "                                          consecutive observations.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing the detailed reports from\n",
        "                        each assessment step.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Perform a comprehensive audit of missing data.\n",
        "        print(\"Step 1: Auditing missing values...\")\n",
        "        missing_value_report = audit_missing_values(df, high_missingness_threshold)\n",
        "        print(\"Step 1: Success. Missing value audit complete.\")\n",
        "\n",
        "        # Step 2: Assess the temporal structure and continuity of the panel.\n",
        "        print(\"Step 2: Assessing temporal coverage and panel balance...\")\n",
        "        temporal_report = assess_temporal_coverage(df, max_gap_tolerance)\n",
        "        print(\"Step 2: Success. Temporal coverage assessment complete.\")\n",
        "\n",
        "        # Step 3: Validate the logical consistency between different variables.\n",
        "        print(\"Step 3: Validating cross-variable consistency...\")\n",
        "        consistency_report = validate_cross_variable_consistency(df)\n",
        "        print(\"Step 3: Success. Cross-variable consistency checks complete.\")\n",
        "\n",
        "        # Aggregate all reports into a single master dictionary.\n",
        "        master_report = {\n",
        "            \"missing_value_audit\": missing_value_report,\n",
        "            \"temporal_coverage_assessment\": temporal_report,\n",
        "            \"cross_variable_consistency_failures\": consistency_report\n",
        "        }\n",
        "\n",
        "        print(\"\\nSUCCESS: Data quality assessment is complete.\")\n",
        "        return master_report\n",
        "\n",
        "    except (ValueError, TypeError) as e:\n",
        "        # Catch any validation error from the helper functions.\n",
        "        print(f\"\\nERROR: Data quality assessment failed.\")\n",
        "        # Re-raise the specific error to provide a detailed failure report.\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "l4OEH3-egVXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: GDELT Event Data Preprocessing Pipeline\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4, Step 1: Filter Dyadic Pairs by Event Density\n",
        "# ==============================================================================\n",
        "\n",
        "def filter_dyads_by_event_density(\n",
        "    df: pd.DataFrame,\n",
        "    min_obs_threshold: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters dyadic pairs from the panel based on a minimum threshold of\n",
        "    non-zero event observations.\n",
        "\n",
        "    This function implements the sample selection criteria from Appendix A.1 of\n",
        "    the source paper. It identifies and retains only those country pairs that\n",
        "    have a sufficient number of months with reported political events, ensuring\n",
        "    that the subsequent time-series analysis is performed on data with adequate\n",
        "    density.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing GDELT event data,\n",
        "                           indexed by DatetimeIndex and including a 'DyadicPair'\n",
        "                           column.\n",
        "        min_obs_threshold (int): The minimum number of months with non-zero\n",
        "                                 event counts required for a dyad to be retained.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A filtered DataFrame containing only the dyadic pairs that\n",
        "                      meet or exceed the event density threshold.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Verify the input is a pandas DataFrame.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    # Check for the required columns.\n",
        "    if 'DyadicPair' not in df.columns or 'GDELT_EventCount' not in df.columns:\n",
        "        raise ValueError(\"DataFrame must contain 'DyadicPair' and 'GDELT_EventCount' columns.\")\n",
        "    # Verify the threshold is a non-negative integer.\n",
        "    if not isinstance(min_obs_threshold, int) or min_obs_threshold < 0:\n",
        "        raise ValueError(\"min_obs_threshold must be a non-negative integer.\")\n",
        "\n",
        "    # --- Calculate Event Density per Dyad ---\n",
        "    # Create a boolean series where True indicates a month with one or more events.\n",
        "    has_events = df['GDELT_EventCount'] > 0\n",
        "    # Use groupby().transform() for a highly efficient, vectorized calculation.\n",
        "    # It computes the sum of non-zero event months for each dyad and broadcasts\n",
        "    # this value back to a Series with the same index as the original DataFrame.\n",
        "    event_density = has_events.groupby(df['DyadicPair']).transform('sum')\n",
        "\n",
        "    # --- Filter the DataFrame ---\n",
        "    # Create a boolean mask to identify all rows belonging to qualifying dyads.\n",
        "    qualifying_dyads_mask = event_density >= min_obs_threshold\n",
        "    # Apply the mask to filter the DataFrame.\n",
        "    df_filtered = df[qualifying_dyads_mask].copy()\n",
        "\n",
        "    # --- Log and Return ---\n",
        "    # Report the number of dyads dropped and retained for audit purposes.\n",
        "    n_original = df['DyadicPair'].nunique()\n",
        "    n_filtered = df_filtered['DyadicPair'].nunique()\n",
        "    print(\n",
        "        f\"Filtered dyads by event density: Retained {n_filtered} of {n_original} \"\n",
        "        f\"dyads ({(n_original - n_filtered)} dropped).\"\n",
        "    )\n",
        "\n",
        "    return df_filtered\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4, Step 2: Prepare GDELT Series for Particle Filter Input\n",
        "# ==============================================================================\n",
        "\n",
        "def prepare_gdelt_series_for_filter(\n",
        "    df: pd.DataFrame\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Prepares and structures GDELT time series data for the Particle Filter.\n",
        "\n",
        "    This function sorts the data chronologically and then splits the main\n",
        "    DataFrame into a dictionary of smaller DataFrames, one for each dyadic\n",
        "    pair. This structure is the required input format for the subsequent\n",
        "    per-dyad processing steps (AR(1) estimation and particle filtering).\n",
        "    Crucially, it does NOT impute missing values, as the particle filter is\n",
        "    designed to handle them.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame, typically pre-filtered for\n",
        "                           event density. Must have a DatetimeIndex and a\n",
        "                           'DyadicPair' column.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary where keys are 'DyadicPair'\n",
        "                                 strings and values are DataFrames containing\n",
        "                                 the 'GDELT_GoldsteinMean' and\n",
        "                                 'GDELT_EventCount' time series for that pair.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame) or not isinstance(df.index, pd.DatetimeIndex):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame with a DatetimeIndex.\")\n",
        "    required_cols = ['DyadicPair', 'GDELT_GoldsteinMean', 'GDELT_EventCount']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        raise ValueError(f\"DataFrame must contain the columns: {required_cols}.\")\n",
        "\n",
        "    # --- Sort Data to Ensure Correct Temporal Order ---\n",
        "    # This is a critical step for any time-series operation.\n",
        "    df_sorted = df.sort_values(by=['DyadicPair', df.index.name])\n",
        "\n",
        "    # --- Split DataFrame into a Dictionary of Dyadic Time Series ---\n",
        "    # Use a dictionary comprehension over the groupby object for an efficient and\n",
        "    # readable way to partition the data.\n",
        "    gdelt_series_dict = {\n",
        "        dyad_name: dyad_group[['GDELT_GoldsteinMean', 'GDELT_EventCount']]\n",
        "        for dyad_name, dyad_group in df_sorted.groupby('DyadicPair')\n",
        "    }\n",
        "\n",
        "    return gdelt_series_dict\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4, Step 3: Estimate Process Variance Parameters (Q_ij)\n",
        "# ==============================================================================\n",
        "\n",
        "def estimate_process_variances(\n",
        "    gdelt_series_dict: Dict[str, pd.DataFrame],\n",
        "    q_floor: float,\n",
        "    q_default: float\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Estimates the process variance (Q_ij) for each dyadic pair.\n",
        "\n",
        "    As per Appendix A.1, this function fits an AR(1) model to the observed\n",
        "    GDELT Goldstein score series for each dyad. The residual variance of this\n",
        "    fit serves as the estimate for the process variance of the latent state in\n",
        "    the particle filter. The function includes robust error handling for cases\n",
        "    where the AR(1) model fails to converge.\n",
        "\n",
        "    Args:\n",
        "        gdelt_series_dict (Dict[str, pd.DataFrame]): A dictionary of dyadic\n",
        "                                                     time series from the\n",
        "                                                     previous step.\n",
        "        q_floor (float): The minimum allowed value for the process variance.\n",
        "        q_default (float): The default value to use if AR(1) estimation fails.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series of estimated process variances (Q_ij), indexed by\n",
        "                   'DyadicPair'.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(gdelt_series_dict, dict):\n",
        "        raise TypeError(\"Input 'gdelt_series_dict' must be a dictionary.\")\n",
        "\n",
        "    # --- Initialize Storage for Results ---\n",
        "    process_variances = {}\n",
        "\n",
        "    # --- Suppress ConvergenceWarning from statsmodels for cleaner output ---\n",
        "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "    # --- Iterate Over Each Dyad to Estimate its Q_ij ---\n",
        "    for dyad, series_df in gdelt_series_dict.items():\n",
        "        # Extract the Goldstein score series and drop missing values for the AR model.\n",
        "        y_ijt = series_df['GDELT_GoldsteinMean'].dropna()\n",
        "\n",
        "        # Ensure there are enough data points to fit a model.\n",
        "        if len(y_ijt) < 10:\n",
        "            # If insufficient data, assign the default variance and continue.\n",
        "            process_variances[dyad] = q_default\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # --- Fit the AR(1) Model ---\n",
        "            # Equation: y_t = c + phi * y_{t-1} + epsilon_t\n",
        "            # We need the variance of epsilon_t.\n",
        "            # Use statsmodels' AutoReg for robust AR model estimation.\n",
        "            model = AutoReg(y_ijt, lags=1, trend='c')\n",
        "            model_fit = model.fit()\n",
        "\n",
        "            # --- Extract, Constrain, and Store the Variance ---\n",
        "            # model_fit.sigma2 provides an unbiased estimate of the residual variance.\n",
        "            q_estimate = model_fit.sigma2\n",
        "            # Apply the floor constraint to ensure numerical stability.\n",
        "            process_variances[dyad] = max(q_estimate, q_floor)\n",
        "\n",
        "        except Exception:\n",
        "            # If the model fails to fit for any reason (e.g., convergence,\n",
        "            # perfect collinearity), assign the default variance.\n",
        "            process_variances[dyad] = q_default\n",
        "\n",
        "    # --- Reset Warning Filters ---\n",
        "    warnings.resetwarnings()\n",
        "\n",
        "    # --- Convert Results to a Pandas Series ---\n",
        "    # A Series is the ideal structure for easy lookup in the next task.\n",
        "    q_series = pd.Series(process_variances, name=\"ProcessVariance_Q\")\n",
        "    q_series.index.name = \"DyadicPair\"\n",
        "\n",
        "    return q_series\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_gdelt_preprocessing_pipeline(\n",
        "    df: pd.DataFrame,\n",
        "    constants: 'ProjectConstants'\n",
        ") -> Tuple[Dict[str, pd.DataFrame], pd.Series]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire GDELT data preprocessing pipeline for the Particle Filter.\n",
        "\n",
        "    This master function executes the three core preprocessing steps for Task 4:\n",
        "    1. Filters the dataset to include only dyads with sufficient event density.\n",
        "    2. Structures the filtered data into the required dictionary format.\n",
        "    3. Estimates the process variance parameter (Q_ij) for each dyad.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw, validated DataFrame.\n",
        "        constants (ProjectConstants): The immutable dataclass containing all\n",
        "                                      project parameters.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, pd.DataFrame], pd.Series]: A tuple containing:\n",
        "            - The dictionary of prepared GDELT time series for the filter.\n",
        "            - The Series of estimated process variances (Q_ij).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Filter dyads based on the baseline event density threshold.\n",
        "        print(\"Step 1: Filtering dyads by event density...\")\n",
        "        df_filtered = filter_dyads_by_event_density(\n",
        "            df=df,\n",
        "            min_obs_threshold=constants.BASELINE_MIN_OBS\n",
        "        )\n",
        "        print(f\"Step 1: Success. Filtered DataFrame contains {df_filtered.shape[0]} observations.\")\n",
        "\n",
        "        # Step 2: Prepare the filtered data into the dictionary structure.\n",
        "        print(\"Step 2: Preparing GDELT series for particle filter input...\")\n",
        "        gdelt_series_dict = prepare_gdelt_series_for_filter(df_filtered)\n",
        "        print(f\"Step 2: Success. Prepared data for {len(gdelt_series_dict)} dyads.\")\n",
        "\n",
        "        # Step 3: Estimate the process variance for each dyad.\n",
        "        print(\"Step 3: Estimating process variances (Q_ij)...\")\n",
        "        q_variances = estimate_process_variances(\n",
        "            gdelt_series_dict=gdelt_series_dict,\n",
        "            q_floor=constants.Q_FLOOR,\n",
        "            q_default=constants.Q_DEFAULT\n",
        "        )\n",
        "        print(\"Step 3: Success. Estimated process variances.\")\n",
        "\n",
        "        # Return the final, prepared data structures.\n",
        "        print(\"\\nSUCCESS: GDELT preprocessing pipeline complete.\")\n",
        "        return gdelt_series_dict, q_variances\n",
        "\n",
        "    except (ValueError, TypeError, KeyError) as e:\n",
        "        # Catch any exception raised during the process for a clean failure.\n",
        "        print(f\"\\nERROR: GDELT preprocessing pipeline failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "aM_6OgshhVNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Particle Filter Implementation for Political Distance\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5, Step 1-3: Core Particle Filter Implementation\n",
        "# ==============================================================================\n",
        "\n",
        "def _systematic_resample(\n",
        "    particles: np.ndarray,\n",
        "    weights: np.ndarray,\n",
        "    rng: np.random.Generator\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Performs systematic resampling to mitigate particle degeneracy.\n",
        "\n",
        "    This method is a low-variance resampling technique that is generally\n",
        "    preferred over multinomial resampling. It ensures a more representative\n",
        "    selection of particles for the next generation.\n",
        "\n",
        "    Args:\n",
        "        particles (np.ndarray): The array of particle states (shape [M,]).\n",
        "        weights (np.ndarray): The corresponding normalized particle weights\n",
        "                              (shape [M,]).\n",
        "        rng (np.random.Generator): The random number generator for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The new array of resampled particles (shape [M,]).\n",
        "    \"\"\"\n",
        "    # Get the number of particles.\n",
        "    M = len(particles)\n",
        "    # Calculate the cumulative sum of weights for the resampling algorithm.\n",
        "    cumulative_sum = np.cumsum(weights)\n",
        "    # Generate a single random number to determine the starting point of the grid.\n",
        "    u_start = rng.uniform(0, 1 / M)\n",
        "    # Create a uniform grid of M points for resampling.\n",
        "    positions = u_start + np.arange(M) / M\n",
        "\n",
        "    # Find the indices of the particles to be selected.\n",
        "    # searchsorted is a highly efficient (O(M log M)) way to perform this lookup.\n",
        "    indices = np.searchsorted(cumulative_sum, positions)\n",
        "\n",
        "    # Return the new set of particles based on the selected indices.\n",
        "    return particles[indices]\n",
        "\n",
        "\n",
        "def run_single_particle_filter(\n",
        "    y_series: np.ndarray,\n",
        "    n_series: np.ndarray,\n",
        "    q_ij: float,\n",
        "    constants: 'ProjectConstants',\n",
        "    seed: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Executes the core Particle Filter algorithm for a single dyadic time series.\n",
        "\n",
        "    This function implements the complete Sequential Monte Carlo filter as\n",
        "    described in Appendix A.1. It estimates the latent 'true' political\n",
        "    distance from the noisy observed GDELT Goldstein scores.\n",
        "\n",
        "    The process involves:\n",
        "    1. Initialization: Drawing initial particles from the empirical distribution.\n",
        "    2. Sequential Loop (for each time step):\n",
        "       a. Propagation: Evolving particles according to the random walk model.\n",
        "       b. Weighting: Updating particle weights based on the likelihood of the\n",
        "          current observation. Handles missing observations correctly.\n",
        "       c. Resampling: Mitigating particle degeneracy using systematic resampling\n",
        "          if the effective sample size drops below a threshold.\n",
        "    3. State Extraction: Calculating the final filtered series as the mean of\n",
        "       the particles at each time step.\n",
        "    4. Sign Reversal: Converting the cooperation score into a distance metric.\n",
        "\n",
        "    Args:\n",
        "        y_series (np.ndarray): The time series of observed Goldstein scores (y_ijt).\n",
        "                               May contain NaNs.\n",
        "        n_series (np.ndarray): The time series of event counts (n_ijt).\n",
        "        q_ij (float): The estimated process variance for this specific dyad.\n",
        "        constants (ProjectConstants): The immutable dataclass of project parameters.\n",
        "        seed (int): A seed for the random number generator for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The final filtered time series of the latent political\n",
        "                    distance estimate.\n",
        "    \"\"\"\n",
        "    # --- 0. Initialization ---\n",
        "    # Initialize a dedicated random number generator for this run.\n",
        "    rng = np.random.default_rng(seed)\n",
        "    # Extract constants for clarity.\n",
        "    M = constants.M_PARTICLES\n",
        "    R_func = constants.R_FUNCTION\n",
        "    # Get the length of the time series.\n",
        "    T = len(y_series)\n",
        "\n",
        "    # Get the non-NaN observations to sample from for initialization.\n",
        "    y_observed = y_series[~np.isnan(y_series)]\n",
        "    if len(y_observed) == 0:\n",
        "        # If there are no observations at all, return a series of NaNs.\n",
        "        return np.full(T, np.nan)\n",
        "\n",
        "    # Initialize particles by sampling from the empirical distribution of observed data.\n",
        "    particles = rng.choice(y_observed, size=M, replace=True)\n",
        "    # Initialize weights uniformly.\n",
        "    weights = np.full(M, 1 / M)\n",
        "    # Create an array to store the final filtered state estimates.\n",
        "    filtered_state = np.zeros(T)\n",
        "\n",
        "    # --- 1. Sequential Filtering Loop ---\n",
        "    for t in range(T):\n",
        "        # --- a. Propagation (State Transition) ---\n",
        "        # Equation: s_ijt|t-1 = s_ij,t-1 + v_t, where v_t ~ N(0, Q_ij)\n",
        "        # Evolve each particle forward according to the random walk model.\n",
        "        process_noise = rng.normal(scale=np.sqrt(q_ij), size=M)\n",
        "        particles += process_noise\n",
        "\n",
        "        # --- b. Weight Update (Likelihood Calculation) ---\n",
        "        # Get the current observation and event count.\n",
        "        y_t = y_series[t]\n",
        "        n_t = n_series[t]\n",
        "\n",
        "        # Check if the current observation is missing.\n",
        "        if not np.isnan(y_t):\n",
        "            # If an observation exists, update weights based on likelihood.\n",
        "            # Equation: R_ijt = 1 / (n_ijt + 1)\n",
        "            r_t = R_func(n_t)\n",
        "\n",
        "            # To prevent numerical underflow, calculate weights in log-space.\n",
        "            # Equation: log(w_t) = log(p(y_t | s_t)) = -0.5*log(2*pi*R_t) - (y_t - s_t)^2 / (2*R_t)\n",
        "            log_likelihoods = norm.logpdf(y_t, loc=particles, scale=np.sqrt(r_t))\n",
        "\n",
        "            # Add to previous log weights (if they existed). Here we assume uniform prior.\n",
        "            log_weights = log_likelihoods\n",
        "\n",
        "            # Normalize log weights using logsumexp for numerical stability.\n",
        "            log_weights_normalized = log_weights - logsumexp(log_weights)\n",
        "\n",
        "            # Convert back to linear space for resampling.\n",
        "            weights = np.exp(log_weights_normalized)\n",
        "        # If y_t is NaN, weights remain unchanged from the previous step (effectively uniform\n",
        "        # after resampling), reflecting no new information.\n",
        "\n",
        "        # --- c. Resampling ---\n",
        "        # Calculate Effective Sample Size (ESS) to check for particle degeneracy.\n",
        "        # Equation: ESS = 1 / sum(w_i^2)\n",
        "        ess = 1.0 / np.sum(weights**2)\n",
        "\n",
        "        # Resample only if ESS falls below a threshold (e.g., M/2).\n",
        "        if ess < M / 2:\n",
        "            particles = _systematic_resample(particles, weights, rng)\n",
        "            # Reset weights to uniform after resampling.\n",
        "            weights.fill(1 / M)\n",
        "\n",
        "        # --- d. State Storage ---\n",
        "        # The estimate for time t is the mean of the current particle set.\n",
        "        filtered_state[t] = np.mean(particles)\n",
        "\n",
        "    # --- 2. Final Transformation ---\n",
        "    # As per the paper, multiply by -1 to convert the cooperation score\n",
        "    # into a political distance measure (higher values = more distant).\n",
        "    final_filtered_series = -filtered_state\n",
        "\n",
        "    return final_filtered_series\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_particle_filter_pipeline(\n",
        "    gdelt_series_dict: Dict[str, pd.DataFrame],\n",
        "    q_variances: pd.Series,\n",
        "    constants: 'ProjectConstants',\n",
        "    base_seed: int = 20250921\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of the Particle Filter across all dyadic pairs.\n",
        "\n",
        "    This function iterates through each dyadic pair, applies the core particle\n",
        "    filter algorithm to its time series, and aggregates the results into a\n",
        "    single, tidy DataFrame. It ensures reproducibility by seeding the random\n",
        "    number generator for each dyad deterministically.\n",
        "\n",
        "    Args:\n",
        "        gdelt_series_dict (Dict[str, pd.DataFrame]): The dictionary of prepared\n",
        "                                                     GDELT time series.\n",
        "        q_variances (pd.Series): The Series of estimated process variances (Q_ij).\n",
        "        constants (ProjectConstants): The immutable dataclass of project parameters.\n",
        "        base_seed (int): A base seed to generate deterministic, unique seeds\n",
        "                         for each dyad's filter run, ensuring overall\n",
        "                         reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with a MultiIndex ('DyadicPair', 'DateTime')\n",
        "                      containing the final, filtered political distance series\n",
        "                      named 'PD_GDELT_Filtered'.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(gdelt_series_dict, dict) or not isinstance(q_variances, pd.Series):\n",
        "        raise TypeError(\"Inputs must be a dictionary of DataFrames and a pandas Series.\")\n",
        "\n",
        "    # --- Initialize Storage for Results ---\n",
        "    all_filtered_series = []\n",
        "\n",
        "    # --- Iterate over all dyads with a progress bar ---\n",
        "    print(\"Running Particle Filter for all dyads...\")\n",
        "    for i, (dyad, series_df) in enumerate(tqdm(gdelt_series_dict.items())):\n",
        "        # --- Prepare Inputs for the Core Filter ---\n",
        "        # Extract the required numpy arrays from the DataFrame.\n",
        "        y_series = series_df['GDELT_GoldsteinMean'].values\n",
        "        n_series = series_df['GDELT_EventCount'].values\n",
        "        # Look up the process variance for the current dyad.\n",
        "        q_ij = q_variances.get(dyad)\n",
        "\n",
        "        # Check if Q_ij was successfully estimated.\n",
        "        if q_ij is None:\n",
        "            print(f\"Warning: No process variance Q_ij found for dyad {dyad}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # --- Run the Core Particle Filter ---\n",
        "        # Generate a unique, deterministic seed for each dyad for reproducibility.\n",
        "        dyad_seed = base_seed + i\n",
        "        filtered_values = run_single_particle_filter(\n",
        "            y_series=y_series,\n",
        "            n_series=n_series,\n",
        "            q_ij=q_ij,\n",
        "            constants=constants,\n",
        "            seed=dyad_seed\n",
        "        )\n",
        "\n",
        "        # --- Store the Result ---\n",
        "        # Create a pandas Series from the result with the correct index.\n",
        "        filtered_series = pd.Series(\n",
        "            filtered_values,\n",
        "            index=series_df.index,\n",
        "            name='PD_GDELT_Filtered'\n",
        "        )\n",
        "        # Add the dyad name to the series for later concatenation.\n",
        "        filtered_series.name = dyad\n",
        "        all_filtered_series.append(filtered_series)\n",
        "\n",
        "    # --- Concatenate All Results into a Single DataFrame ---\n",
        "    if not all_filtered_series:\n",
        "        print(\"Warning: No series were filtered. Returning an empty DataFrame.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Concatenate all individual series into a single DataFrame.\n",
        "    # The keys of the list become the columns.\n",
        "    result_df = pd.concat(all_filtered_series, axis=1)\n",
        "\n",
        "    # Stack the DataFrame to create the final tidy format with a MultiIndex.\n",
        "    # This transforms the data from wide (dyads as columns) to long format.\n",
        "    final_df = result_df.stack().to_frame(name='PD_GDELT_Filtered')\n",
        "    final_df.index.names = ['DateTime', 'DyadicPair']\n",
        "    # Reorder index to match standard panel format.\n",
        "    final_df = final_df.reorder_levels(['DyadicPair', 'DateTime'])\n",
        "\n",
        "    print(\"\\nSUCCESS: Particle filter pipeline complete.\")\n",
        "    return final_df\n"
      ],
      "metadata": {
        "id": "h2ES6CUnicJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Political Distance and Trade Variable Construction\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6, Step 1: UNGA Political Distance Computation\n",
        "# ==============================================================================\n",
        "\n",
        "def compute_unga_political_distance(\n",
        "    df: pd.DataFrame,\n",
        "    unga_ideal_points_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes UNGA-based political distance and merges it into the main DataFrame.\n",
        "\n",
        "    This function enriches a dyadic panel DataFrame with country-year specific\n",
        "    UN General Assembly (UNGA) ideal point estimates. It performs two sequential\n",
        "    left joins to merge the ideal points for the exporter and importer in each\n",
        "    dyad-year observation. Finally, it calculates the political distance as the\n",
        "    absolute difference between these two ideal points.\n",
        "\n",
        "    The implementation follows a streamlined and robust logic to ensure clarity,\n",
        "    efficiency, and accurate handling of potential data mismatches.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The main dyadic panel DataFrame. Must have a\n",
        "                           DatetimeIndex and 'ExporterISO', 'ImporterISO' columns.\n",
        "        unga_ideal_points_df (pd.DataFrame): A DataFrame with columns\n",
        "                                             ['ISO', 'Year', 'IdealPoint'],\n",
        "                                             containing the annual ideal point\n",
        "                                             estimates for each country.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The main DataFrame with three new columns added:\n",
        "                      'IdealPoint_Exporter', 'IdealPoint_Importer', and 'PD_UNGA'.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If either input is not a pandas DataFrame.\n",
        "        ValueError: If required columns are missing from the input DataFrames,\n",
        "                    or if a merge operation fails completely (indicating a\n",
        "                    likely key mismatch between the datasets).\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    # Verify that both inputs are pandas DataFrames.\n",
        "    if not isinstance(df, pd.DataFrame) or not isinstance(unga_ideal_points_df, pd.DataFrame):\n",
        "        raise TypeError(\"Inputs 'df' and 'unga_ideal_points_df' must be pandas DataFrames.\")\n",
        "\n",
        "    # Define and check for required columns in the main DataFrame.\n",
        "    required_main_cols = ['ExporterISO', 'ImporterISO']\n",
        "    if not all(col in df.columns for col in required_main_cols):\n",
        "        raise ValueError(f\"Main DataFrame must contain columns: {required_main_cols}.\")\n",
        "\n",
        "    # Define and check for required columns in the UNGA data.\n",
        "    required_unga_cols = ['ISO', 'Year', 'IdealPoint']\n",
        "    if not all(col in unga_ideal_points_df.columns for col in required_unga_cols):\n",
        "        raise ValueError(f\"UNGA DataFrame must contain columns: {required_unga_cols}.\")\n",
        "\n",
        "    # --- 2. Prepare Merge Key ---\n",
        "    # Create a temporary copy to avoid modifying the original DataFrame in place.\n",
        "    df_out = df.copy()\n",
        "    # Create a 'Year' column from the DatetimeIndex to use as a merge key.\n",
        "    df_out['Year'] = df_out.index.year\n",
        "\n",
        "    # --- 3. First Merge (for Exporter) ---\n",
        "    # Perform a left merge to join the exporter's ideal point for each year.\n",
        "    # A left join ensures that all original observations are preserved.\n",
        "    df_out = pd.merge(\n",
        "        df_out,\n",
        "        unga_ideal_points_df,\n",
        "        left_on=['ExporterISO', 'Year'],\n",
        "        right_on=['ISO', 'Year'],\n",
        "        how='left'\n",
        "    )\n",
        "    # Rename the merged 'IdealPoint' column to be specific to the exporter.\n",
        "    df_out.rename(columns={'IdealPoint': 'IdealPoint_Exporter'}, inplace=True)\n",
        "    # Drop the redundant 'ISO' key column from the UNGA data.\n",
        "    df_out.drop(columns='ISO', inplace=True)\n",
        "\n",
        "    # --- 4. Post-Merge Validation (Exporter) ---\n",
        "    # Check if the merge was successful. If the new column is all NaN, it\n",
        "    # indicates a total failure to match keys.\n",
        "    if df_out['IdealPoint_Exporter'].isnull().all():\n",
        "        raise ValueError(\n",
        "            \"Exporter ideal point merge failed completely. Check for mismatch \"\n",
        "            \"in country codes ('ExporterISO' vs 'ISO') or year ranges.\"\n",
        "        )\n",
        "\n",
        "    # --- 5. Second Merge (for Importer) ---\n",
        "    # Perform a second left merge to join the importer's ideal point.\n",
        "    df_out = pd.merge(\n",
        "        df_out,\n",
        "        unga_ideal_points_df,\n",
        "        left_on=['ImporterISO', 'Year'],\n",
        "        right_on=['ISO', 'Year'],\n",
        "        how='left'\n",
        "    )\n",
        "    # Rename the newly merged 'IdealPoint' column for the importer.\n",
        "    df_out.rename(columns={'IdealPoint': 'IdealPoint_Importer'}, inplace=True)\n",
        "    # Drop the redundant 'ISO' key column.\n",
        "    df_out.drop(columns='ISO', inplace=True)\n",
        "\n",
        "    # --- 6. Post-Merge Validation (Importer) ---\n",
        "    # Perform the same check for the importer merge.\n",
        "    if df_out['IdealPoint_Importer'].isnull().all():\n",
        "        raise ValueError(\n",
        "            \"Importer ideal point merge failed completely. Check for mismatch \"\n",
        "            \"in country codes ('ImporterISO' vs 'ISO') or year ranges.\"\n",
        "        )\n",
        "\n",
        "    # --- 7. Political Distance Calculation ---\n",
        "    # Equation: PD_ijt^UNGA = |IdealPoint_it - IdealPoint_jt|\n",
        "    # This vectorized operation calculates the absolute difference. It correctly\n",
        "    # propagates NaNs, resulting in NaN for any dyad-year where at least one\n",
        "    # partner's ideal point is missing.\n",
        "    df_out['PD_UNGA'] = np.abs(df_out['IdealPoint_Exporter'] - df_out['IdealPoint_Importer'])\n",
        "\n",
        "    # --- 8. Final Cleanup ---\n",
        "    # Drop the temporary 'Year' column to restore the DataFrame's original schema.\n",
        "    df_out.drop(columns='Year', inplace=True)\n",
        "\n",
        "    # Return the final, enriched DataFrame.\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6, Step 2: Apply Inverse Hyperbolic Sine Transformation\n",
        "# ==============================================================================\n",
        "\n",
        "def apply_ihs_transformation(\n",
        "    df: pd.DataFrame,\n",
        "    columns_to_transform: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies the Inverse Hyperbolic Sine (IHS) transformation to specified columns.\n",
        "\n",
        "    The IHS transformation, sinh⁻¹(x), is used for variables that can take\n",
        "    negative or zero values (like Polity scores or centered GDELT scores), as it\n",
        "    is defined for all real numbers and approximates the natural log for large\n",
        "    positive values.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        columns_to_transform (List[str]): A list of column names to which the\n",
        "                                          IHS transformation will be applied.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with new columns containing the transformed\n",
        "                      values, suffixed with '_IHS'.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    if not all(col in df.columns for col in columns_to_transform):\n",
        "        missing = set(columns_to_transform) - set(df.columns)\n",
        "        raise ValueError(f\"Columns to transform not found in DataFrame: {missing}\")\n",
        "\n",
        "    # --- Apply Transformation ---\n",
        "    df_transformed = df.copy()\n",
        "    # Iterate through the specified list of columns.\n",
        "    for col in columns_to_transform:\n",
        "        # Define the new column name.\n",
        "        new_col_name = f\"{col}_IHS\"\n",
        "        # Equation: x_transformed = sinh⁻¹(x) = ln(x + sqrt(x² + 1))\n",
        "        # Apply the vectorized np.arcsinh function.\n",
        "        df_transformed[new_col_name] = np.arcsinh(df_transformed[col])\n",
        "\n",
        "    return df_transformed\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6, Step 3: Construct Trade Margin Variables\n",
        "# ==============================================================================\n",
        "\n",
        "def construct_trade_margins(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs variables for analyzing the extensive and intensive margins of trade.\n",
        "\n",
        "    This function creates three key variables based on the source paper's\n",
        "    methodology, using TradeProd sector counts (S_ijt) as the basis for the\n",
        "    extensive margin.\n",
        "\n",
        "    1.  Intensive Margin: Average trade value per traded sector (X_ijt / S_ijt).\n",
        "    2.  Any-Trade Dummy: A binary indicator for whether any trade occurs (S_ijt > 0).\n",
        "    3.  Sector Share: The fraction of possible sectors traded (S_ijt / 9), for\n",
        "        use in bounded (e.g., Logit) models.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame. Must contain the columns\n",
        "                           'BilateralTradeValue_USD' and 'TradeProd_SectorCount'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with new columns for the trade margins.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # This check is critical because the paper's methodology for margins\n",
        "    # relies on sector counts, not the HS-6 product counts from the initial schema.\n",
        "    required_cols = ['BilateralTradeValue_USD', 'TradeProd_SectorCount']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        raise ValueError(f\"DataFrame must contain columns for margin construction: {required_cols}.\")\n",
        "\n",
        "    df_margins = df.copy()\n",
        "\n",
        "    # --- Construct Intensive Margin ---\n",
        "    # Equation: IntensiveMargin_ijt = X_ijt / S_ijt\n",
        "    # Use np.where to safely handle division by zero for dyads with no trade.\n",
        "    df_margins['IntensiveMargin_USD_per_Sector'] = np.where(\n",
        "        df_margins['TradeProd_SectorCount'] > 0,\n",
        "        df_margins['BilateralTradeValue_USD'] / df_margins['TradeProd_SectorCount'],\n",
        "        0  # Assign 0 if no sectors are traded.\n",
        "    )\n",
        "\n",
        "    # --- Construct Any-Trade Dummy (Extensive Margin) ---\n",
        "    # Equation: AnyTrade_ijt = 1(S_ijt > 0)\n",
        "    # Create a binary indicator (0 or 1).\n",
        "    df_margins['AnyTrade_Sector_Dummy'] = (df_margins['TradeProd_SectorCount'] > 0).astype(int)\n",
        "\n",
        "    # --- Construct Sector Share (Extensive Margin for Logit) ---\n",
        "    # Equation: SectorShare_ijt = S_ijt / 9\n",
        "    # Assuming 9 total industrial sectors as per the TradeProd database.\n",
        "    MAX_SECTORS = 9\n",
        "    df_margins['SectorShare'] = df_margins['TradeProd_SectorCount'] / MAX_SECTORS\n",
        "\n",
        "    return df_margins\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_variable_construction_pipeline(\n",
        "    main_df: pd.DataFrame,\n",
        "    filtered_gdelt_df: pd.DataFrame,\n",
        "    unga_ideal_points_df: pd.DataFrame,\n",
        "    ihs_columns: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire variable construction and transformation pipeline.\n",
        "\n",
        "    This master function executes the three core steps for Task 6:\n",
        "    1. Merges the filtered GDELT series into the main panel.\n",
        "    2. Computes and merges the UNGA-based political distance.\n",
        "    3. Applies the IHS transformation to specified variables.\n",
        "    4. Constructs the intensive and extensive trade margin variables.\n",
        "\n",
        "    Args:\n",
        "        main_df (pd.DataFrame): The primary, validated panel DataFrame.\n",
        "        filtered_gdelt_df (pd.DataFrame): The DataFrame containing the filtered\n",
        "                                          GDELT series from Task 5.\n",
        "        unga_ideal_points_df (pd.DataFrame): DataFrame of UNGA ideal points.\n",
        "        ihs_columns (List[str]): A list of columns to apply the IHS transform to.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The final, fully processed DataFrame with all constructed\n",
        "                      variables ready for the estimation phase.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- Merge Filtered GDELT Data ---\n",
        "        # Join the filtered series back to the main DataFrame.\n",
        "        print(\"Step 0: Merging filtered GDELT political distance series...\")\n",
        "        # A left join ensures we keep all original observations.\n",
        "        df_processed = main_df.join(filtered_gdelt_df, on=['DyadicPair', main_df.index.name])\n",
        "        print(\"Step 0: Success. GDELT series merged.\")\n",
        "\n",
        "        # --- Step 1: Compute UNGA Political Distance ---\n",
        "        print(\"Step 1: Computing UNGA political distance...\")\n",
        "        df_processed = compute_unga_political_distance(df_processed, unga_ideal_points_df)\n",
        "        print(\"Step 1: Success. UNGA distance computed and merged.\")\n",
        "\n",
        "        # --- Step 2: Apply IHS Transformation ---\n",
        "        # Add the newly created UNGA distance to the list of columns to transform.\n",
        "        all_ihs_columns = ihs_columns + ['PD_UNGA']\n",
        "        print(f\"Step 2: Applying IHS transformation to {all_ihs_columns}...\")\n",
        "        df_processed = apply_ihs_transformation(df_processed, all_ihs_columns)\n",
        "        print(\"Step 2: Success. IHS transformation applied.\")\n",
        "\n",
        "        # --- Step 3: Construct Trade Margins ---\n",
        "        # This step requires a 'TradeProd_SectorCount' column, which we assume\n",
        "        # exists on the main_df for this function to work.\n",
        "        print(\"Step 3: Constructing trade margin variables...\")\n",
        "        df_processed = construct_trade_margins(df_processed)\n",
        "        print(\"Step 3: Success. Trade margins constructed.\")\n",
        "\n",
        "        print(\"\\nSUCCESS: Variable construction pipeline complete.\")\n",
        "        return df_processed\n",
        "\n",
        "    except (ValueError, TypeError, KeyError) as e:\n",
        "        print(f\"\\nERROR: Variable construction pipeline failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "YF_2g-qyjV8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Panel Data Index Construction and Validation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7, Step 1: Create Hierarchical Panel Index Structure\n",
        "# ==============================================================================\n",
        "\n",
        "def create_fixed_effects_identifiers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates categorical identifiers required for high-dimensional fixed effects estimation.\n",
        "\n",
        "    This function generates the specific categorical columns that modern econometric\n",
        "    packages use to absorb fixed effects algorithmically. It creates identifiers for:\n",
        "    1. Exporter-Year effects.\n",
        "    2. Importer-Year effects.\n",
        "    3. Dyadic Pair effects (already present as 'DyadicPair').\n",
        "    4. Border-Year effects (distinguishing international vs. domestic trade per year).\n",
        "\n",
        "    For memory efficiency, the newly created identifier columns are converted to\n",
        "    the 'category' dtype.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The processed DataFrame. Must have a DatetimeIndex\n",
        "                           and 'ExporterISO', 'ImporterISO' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with new categorical columns for fixed effects.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame) or not isinstance(df.index, pd.DatetimeIndex):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame with a DatetimeIndex.\")\n",
        "    required_cols = ['ExporterISO', 'ImporterISO']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        raise ValueError(f\"DataFrame must contain columns: {required_cols}.\")\n",
        "\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # --- Create Year Column ---\n",
        "    # Extract the year from the DatetimeIndex for creating time-based effects.\n",
        "    df_out['Year'] = df_out.index.year\n",
        "\n",
        "    # --- Create Exporter-Year and Importer-Year Identifiers ---\n",
        "    # These identifiers capture all time-varying, country-specific shocks.\n",
        "    df_out['FE_ExporterYear'] = (df_out['ExporterISO'] + '_' + df_out['Year'].astype(str))\n",
        "    df_out['FE_ImporterYear'] = (df_out['ImporterISO'] + '_' + df_out['Year'].astype(str))\n",
        "\n",
        "    # --- Create Border-Year Identifier ---\n",
        "    # This identifier creates a separate dummy for international trade in each year,\n",
        "    # allowing the effect of crossing a border to vary over time.\n",
        "    # Domestic observations are grouped into a single 'Domestic' category.\n",
        "    df_out['FE_BorderYear'] = np.where(\n",
        "        df_out['ExporterISO'] != df_out['ImporterISO'],\n",
        "        'Border_' + df_out['Year'].astype(str),\n",
        "        'Domestic'\n",
        "    )\n",
        "\n",
        "    # --- Convert to Category Dtype for Memory Efficiency ---\n",
        "    # This is a critical optimization for high-dimensional fixed effects.\n",
        "    categorical_fe_cols = ['FE_ExporterYear', 'FE_ImporterYear', 'FE_BorderYear', 'DyadicPair']\n",
        "    for col in categorical_fe_cols:\n",
        "        # Ensure the column exists before attempting conversion.\n",
        "        if col in df_out.columns:\n",
        "            df_out[col] = df_out[col].astype('category')\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7, Step 2: Fixed Effects Dummy Variable Construction (Validation)\n",
        "# ==============================================================================\n",
        "\n",
        "def validate_fixed_effects_identifiers(\n",
        "    df: pd.DataFrame,\n",
        "    fe_identifier_cols: List[str]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Validates that the fixed effects identifier columns are correctly prepared.\n",
        "\n",
        "    NOTE: This function does NOT create dummy variable matrices. In modern\n",
        "    econometrics, high-dimensional fixed effects are absorbed algorithmically\n",
        "    to avoid creating enormous, memory-intensive matrices. This function\n",
        "    instead validates that the categorical IDENTIFIERS are present and correctly\n",
        "    formatted for use with packages like `statsmodels` or `fixest`.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with FE identifier columns.\n",
        "        fe_identifier_cols (List[str]): A list of the names of the FE\n",
        "                                        identifier columns to validate.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all validations pass.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any identifier column is missing or has an incorrect dtype.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "\n",
        "    # --- Check for Presence and Type ---\n",
        "    for col in fe_identifier_cols:\n",
        "        # Verify that each required identifier column exists.\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Fixed effects identifier column '{col}' is missing.\")\n",
        "        # Verify that the column has been converted to the efficient 'category' dtype.\n",
        "        if not pd.api.types.is_categorical_dtype(df[col]):\n",
        "            raise TypeError(f\"Identifier column '{col}' must be of 'category' dtype for efficiency.\")\n",
        "        # Verify there are no missing values in the identifier columns.\n",
        "        if df[col].isnull().any():\n",
        "            raise ValueError(f\"Identifier column '{col}' contains null values.\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7, Step 3: Interaction Term Construction\n",
        "# ==============================================================================\n",
        "\n",
        "def create_interaction_terms(\n",
        "    df: pd.DataFrame,\n",
        "    interactions_to_create: List[Tuple[str, str, str]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates interaction terms between specified pairs of variables.\n",
        "\n",
        "    This function takes a list of desired interactions and creates them using\n",
        "    efficient, vectorized multiplication.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        interactions_to_create (List[Tuple[str, str, str]]):\n",
        "            A list of tuples, where each tuple contains:\n",
        "            (name of variable 1, name of variable 2, name for new interaction column).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with the new interaction term columns added.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # --- Iterate and Create Interactions ---\n",
        "    for var1, var2, new_name in interactions_to_create:\n",
        "        # Check that both source columns exist before proceeding.\n",
        "        if var1 not in df_out.columns or var2 not in df_out.columns:\n",
        "            raise ValueError(f\"Cannot create interaction '{new_name}': \"\n",
        "                             f\"Source column '{var1}' or '{var2}' not found.\")\n",
        "\n",
        "        # Create the new interaction term using vectorized multiplication.\n",
        "        # This correctly propagates NaNs.\n",
        "        df_out[new_name] = df_out[var1] * df_out[var2]\n",
        "\n",
        "    return df_out\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_panel_data_preparation(\n",
        "    df: pd.DataFrame,\n",
        "    interactions_spec: List[Tuple[str, str, str]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the full pipeline for preparing panel data for estimation.\n",
        "\n",
        "    This master function executes the three core steps for Task 7:\n",
        "    1. Creates the necessary categorical identifiers for fixed effects.\n",
        "    2. Validates that these identifiers are correctly formatted.\n",
        "    3. Creates all specified interaction terms between variables.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The fully processed DataFrame from the previous task.\n",
        "        interactions_spec (List[Tuple[str, str, str]]): The specification for\n",
        "                                                        all interaction terms to\n",
        "                                                        be created.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The final DataFrame with all identifiers and interaction\n",
        "                      terms, ready for the estimation phase.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Create the categorical identifiers for fixed effects.\n",
        "        print(\"Step 1: Creating fixed effects identifiers...\")\n",
        "        df_prepared = create_fixed_effects_identifiers(df)\n",
        "        print(\"Step 1: Success. FE identifiers created and converted to category dtype.\")\n",
        "\n",
        "        # Step 2: Validate the created identifiers.\n",
        "        print(\"Step 2: Validating fixed effects identifiers...\")\n",
        "        fe_cols = ['FE_ExporterYear', 'FE_ImporterYear', 'FE_BorderYear', 'DyadicPair']\n",
        "        validate_fixed_effects_identifiers(df_prepared, fe_cols)\n",
        "        print(\"Step 2: Success. FE identifiers are valid and ready for absorption.\")\n",
        "\n",
        "        # Step 3: Create all required interaction terms.\n",
        "        print(\"Step 3: Creating interaction terms...\")\n",
        "        df_final = create_interaction_terms(df_prepared, interactions_spec)\n",
        "        print(f\"Step 3: Success. Created {len(interactions_spec)} interaction terms.\")\n",
        "\n",
        "        print(\"\\nSUCCESS: Panel data preparation pipeline complete.\")\n",
        "        return df_final\n",
        "\n",
        "    except (ValueError, TypeError, KeyError) as e:\n",
        "        print(f\"\\nERROR: Panel data preparation pipeline failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "ZPyk2Wq1kU8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Sample Preparation for Different Estimation Methods\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8, Step 1: PPML Sample Preparation Including Domestic Trade\n",
        "# ==============================================================================\n",
        "\n",
        "def prepare_ppml_sample(\n",
        "    dyadic_df: pd.DataFrame,\n",
        "    country_df: pd.DataFrame,\n",
        "    core_regression_vars: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepares the final sample for PPML estimation by incorporating domestic trade\n",
        "    flows and performing listwise deletion of missing core variables.\n",
        "\n",
        "    This function implements a critical step from modern structural gravity\n",
        "    theory: including domestic trade (production for the home market) as an\n",
        "    observation for each country-year. The implementation follows a clean and\n",
        "    modern pandas workflow, avoiding deprecated arguments and complex methods in\n",
        "    favor of explicit and readable data manipulation steps.\n",
        "\n",
        "    The process is as follows:\n",
        "    1. Constructs a DataFrame of domestic trade observations from country-level data.\n",
        "    2. Explicitly aligns the columns of the domestic and international DataFrames.\n",
        "    3. Concatenates the two DataFrames into a single panel.\n",
        "    4. Sorts the combined panel chronologically.\n",
        "    5. Performs listwise deletion on rows with missing values in essential\n",
        "       regression variables.\n",
        "\n",
        "    Args:\n",
        "        dyadic_df (pd.DataFrame): The main panel of international dyadic trade flows.\n",
        "                                  Must have a DatetimeIndex.\n",
        "        country_df (pd.DataFrame): A DataFrame with country-level annual data,\n",
        "                                   including ['ISO', 'Year', 'GDP_USD',\n",
        "                                   'TotalExports_USD'].\n",
        "        core_regression_vars (List[str]): A list of essential column names. Any\n",
        "                                          row with a missing value in these\n",
        "                                          columns will be dropped from the final\n",
        "                                          sample.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The final estimation-ready DataFrame including both\n",
        "                      international and domestic trade observations, sorted by\n",
        "                      index and cleaned of missing values in core variables.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If inputs are not pandas DataFrames.\n",
        "        ValueError: If required columns are missing from the input DataFrames.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    # Verify that both inputs are pandas DataFrames.\n",
        "    if not isinstance(dyadic_df, pd.DataFrame) or not isinstance(country_df, pd.DataFrame):\n",
        "        raise TypeError(\"Inputs 'dyadic_df' and 'country_df' must be pandas DataFrames.\")\n",
        "    # Check for required columns in the country-level data.\n",
        "    required_country_cols = ['ISO', 'Year', 'GDP_USD', 'TotalExports_USD']\n",
        "    if not all(col in country_df.columns for col in required_country_cols):\n",
        "        raise ValueError(f\"Country DataFrame must contain columns: {required_country_cols}.\")\n",
        "    # Check that the main DataFrame has a DatetimeIndex.\n",
        "    if not isinstance(dyadic_df.index, pd.DatetimeIndex):\n",
        "        raise TypeError(\"Main DataFrame 'dyadic_df' must have a DatetimeIndex.\")\n",
        "\n",
        "    # --- 2. Construct Domestic Trade Observations ---\n",
        "    # Create a copy to avoid modifying the original country data DataFrame.\n",
        "    domestic = country_df.copy()\n",
        "\n",
        "    # Equation: DomesticTrade_i = max(0, GDP_i - TotalExports_i)\n",
        "    # Calculate domestic trade. .clip(lower=0) is a robust, vectorized\n",
        "    # way to enforce the non-negativity constraint.\n",
        "    domestic['BilateralTradeValue_USD'] = (\n",
        "        domestic['GDP_USD'] - domestic['TotalExports_USD']\n",
        "    ).clip(lower=0)\n",
        "\n",
        "    # For domestic observations, the exporter and importer are the same country.\n",
        "    domestic['ExporterISO'] = domestic['ISO']\n",
        "    domestic['ImporterISO'] = domestic['ISO']\n",
        "\n",
        "    # Create the corresponding 'DyadicPair' identifier (e.g., 'USA_USA').\n",
        "    domestic['DyadicPair'] = domestic['ISO'] + '_' + domestic['ISO']\n",
        "\n",
        "    # Create a DatetimeIndex to match the dyadic panel's structure.\n",
        "    # By convention, annual data is placed at the start of the year.\n",
        "    domestic.index = pd.to_datetime(domestic['Year'].astype(str) + '-01-01')\n",
        "\n",
        "    # --- 3. Align Columns for Clean Concatenation ---\n",
        "    # Identify the common columns between the two dataframes.\n",
        "    common_columns = dyadic_df.columns.intersection(domestic.columns)\n",
        "    # Subset the domestic dataframe to only these common columns.\n",
        "    domestic_aligned = domestic[common_columns]\n",
        "    # Ensure the column order is identical to the dyadic dataframe.\n",
        "    domestic_aligned = domestic_aligned[dyadic_df.columns]\n",
        "\n",
        "    # --- 4. Concatenate International and Domestic Data ---\n",
        "    # Combine the international and newly created domestic observations.\n",
        "    # `ignore_index=False` preserves the DatetimeIndex.\n",
        "    full_panel = pd.concat([dyadic_df, domestic_aligned])\n",
        "\n",
        "    # Explicitly sort the index to restore a clean chronological order.\n",
        "    full_panel.sort_index(inplace=True)\n",
        "\n",
        "    # --- 5. Final Listwise Deletion ---\n",
        "    # Drop any row with missing values in the core set of variables\n",
        "    # required for the main regression specification.\n",
        "    n_before = len(full_panel)\n",
        "    final_sample = full_panel.dropna(subset=core_regression_vars)\n",
        "    n_after = len(final_sample)\n",
        "\n",
        "    # Provide a clear log of the filtering action.\n",
        "    print(\n",
        "        f\"PPML sample preparation: Dropped {n_before - n_after} of {n_before} \"\n",
        "        f\"rows with missing core variables. Final sample size: {n_after}.\"\n",
        "    )\n",
        "\n",
        "    # Return the final, analysis-ready DataFrame.\n",
        "    return final_sample\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8, Step 2: Logit Sample Preparation for Extensive Margin\n",
        "# ==============================================================================\n",
        "\n",
        "def prepare_logit_sample(\n",
        "    df: pd.DataFrame,\n",
        "    dependent_var: str,\n",
        "    handle_gattwto_collinearity: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepares a sample for Logit estimation by removing perfect prediction cases\n",
        "    and optionally handling a specific collinearity issue.\n",
        "\n",
        "    This function performs two critical data preparation steps for binary choice\n",
        "    models on panel data:\n",
        "    1.  It identifies and removes dyadic pairs where the outcome variable is\n",
        "        constant over time (always 0 or always 1), as these groups provide no\n",
        "        information for estimation with fixed effects.\n",
        "    2.  As per the source paper (p. 12), it can programmatically address a\n",
        "        potential collinearity issue that arises when the \"no GATT/WTO member\"\n",
        "        category becomes too sparse after the first filter. If triggered, it\n",
        "        collapses the \"no-member\" and \"one-member\" categories by dropping the\n",
        "        'GATTWTO_One' indicator.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame for estimation.\n",
        "        dependent_var (str): The name of the binary (0/1) dependent variable,\n",
        "                             e.g., 'AnyTrade_Sector_Dummy'.\n",
        "        handle_gattwto_collinearity (bool): If True, performs the check for\n",
        "                                            sparsity in the baseline GATT/WTO\n",
        "                                            category and drops the 'GATTWTO_One'\n",
        "                                            column if necessary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A filtered and potentially modified DataFrame that is\n",
        "                      suitable for Logit estimation.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input is not a pandas DataFrame.\n",
        "        ValueError: If required columns are missing from the DataFrame.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    # Verify the input is a pandas DataFrame.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    # Define the list of columns required for the function's logic.\n",
        "    required_cols = [dependent_var, 'DyadicPair', 'GATTWTO_One', 'GATTWTO_Both']\n",
        "    # Check if all required columns are present.\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        raise ValueError(f\"DataFrame must contain required columns: {required_cols}.\")\n",
        "\n",
        "    # --- 2. Filter for Perfect Prediction ---\n",
        "    # Use groupby().transform('nunique') to efficiently find the number of\n",
        "    # unique outcomes for the dependent variable within each dyad.\n",
        "    outcome_variation = df.groupby('DyadicPair')[dependent_var].transform('nunique')\n",
        "    # Keep only the dyads where the number of unique outcomes is greater than 1\n",
        "    # (i.e., where the outcome varies over time).\n",
        "    logit_sample = df[outcome_variation > 1].copy()\n",
        "\n",
        "    # Provide a clear log of the filtering action.\n",
        "    n_dyads_before = df['DyadicPair'].nunique()\n",
        "    n_dyads_after = logit_sample['DyadicPair'].nunique()\n",
        "    print(\n",
        "        f\"Logit sample preparation: Dropped {n_dyads_before - n_dyads_after} of \"\n",
        "        f\"{n_dyads_before} dyads due to perfect prediction.\"\n",
        "    )\n",
        "\n",
        "    # --- 3. Conditional Collinearity Handling ---\n",
        "    # Proceed with this step only if the flag is enabled.\n",
        "    if handle_gattwto_collinearity:\n",
        "        # Define the baseline category: dyads with no GATT/WTO members.\n",
        "        is_baseline = (logit_sample['GATTWTO_One'] == 0) & (logit_sample['GATTWTO_Both'] == 0)\n",
        "\n",
        "        # Count the number of unique dyads in this baseline category.\n",
        "        n_baseline_dyads = logit_sample.loc[is_baseline, 'DyadicPair'].nunique()\n",
        "\n",
        "        # Define a pragmatic threshold for severe sparsity. If fewer than 2\n",
        "        # dyads exist in the baseline, it's highly likely to be collinear\n",
        "        # with the dyadic fixed effects.\n",
        "        MIN_BASELINE_DYADS = 2\n",
        "        if n_baseline_dyads < MIN_BASELINE_DYADS:\n",
        "            # If the baseline is too sparse, drop the 'GATTWTO_One' column.\n",
        "            logit_sample.drop(columns=['GATTWTO_One'], inplace=True)\n",
        "            # Issue a clear warning to the user explaining the action taken.\n",
        "            # The `warnings` module is preferred over `print` for such notifications.\n",
        "            warnings.warn(\n",
        "                f\"Baseline 'no-member' category has only {n_baseline_dyads} dyad(s) \"\n",
        "                f\"after filtering. Dropping 'GATTWTO_One' to collapse categories and \"\n",
        "                f\"avoid collinearity, as per source paper methodology.\"\n",
        "            )\n",
        "\n",
        "    # Return the fully prepared sample.\n",
        "    return logit_sample\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8, Step 3: Create Temporal Subsample Datasets\n",
        "# ==============================================================================\n",
        "\n",
        "def create_temporal_subsamples(\n",
        "    df: pd.DataFrame,\n",
        "    periods_spec: Dict[str, Tuple[int, int]]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Creates a dictionary of DataFrames, each corresponding to a specific time period.\n",
        "\n",
        "    This function slices the main DataFrame into temporal subsamples based on a\n",
        "    provided specification, facilitating analysis of how effects vary over time.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame. Must have a 'Year' column.\n",
        "        periods_spec (Dict[str, Tuple[int, int]]): A dictionary where keys are\n",
        "            period labels (e.g., \"1995-2008\") and values are tuples of\n",
        "            (start_year, end_year).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary of the temporal subsample DataFrames.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    if 'Year' not in df.columns:\n",
        "        raise ValueError(\"DataFrame must have a 'Year' column for temporal slicing.\")\n",
        "\n",
        "    # --- Create Subsamples ---\n",
        "    subsamples = {}\n",
        "    for period_label, (start_year, end_year) in periods_spec.items():\n",
        "        # Use a boolean mask for efficient slicing.\n",
        "        mask = (df['Year'] >= start_year) & (df['Year'] <= end_year)\n",
        "        subsample = df[mask].copy()\n",
        "\n",
        "        # --- Validate Subsample ---\n",
        "        if subsample.empty:\n",
        "            print(f\"Warning: Temporal subsample '{period_label}' is empty.\")\n",
        "\n",
        "        subsamples[period_label] = subsample\n",
        "\n",
        "    return subsamples\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_sample_preparation_pipeline(\n",
        "    main_df: pd.DataFrame,\n",
        "    country_level_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the creation of all analytical samples for the study.\n",
        "\n",
        "    This master function executes the three core sample preparation steps:\n",
        "    1. Creates the main PPML sample, crucially including domestic trade flows.\n",
        "    2. Creates a subsample for Logit estimation, handling perfect prediction.\n",
        "    3. Creates a dictionary of temporal subsamples for historical analysis.\n",
        "\n",
        "    Args:\n",
        "        main_df (pd.DataFrame): The fully processed international dyadic panel.\n",
        "        country_level_df (pd.DataFrame): Annual country-level data for domestic trade.\n",
        "        config (Dict[str, Any]): The project configuration dictionary containing\n",
        "                                 specifications for variables and periods.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing all prepared samples:\n",
        "                        - 'ppml_sample': The main estimation DataFrame.\n",
        "                        - 'logit_sample': The sample for extensive margin analysis.\n",
        "                        - 'temporal_subsamples': A dict of historical subsamples.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define core variables needed for listwise deletion from config or hardcode.\n",
        "        # This should include the dependent var, main regressors, and interactions.\n",
        "        # For demonstration, a simplified list is used.\n",
        "        core_vars = [\n",
        "            'BilateralTradeValue_USD', 'PD_GDELT_Filtered_IHS', 'PD_UNGA_IHS',\n",
        "            'GATTWTO_Both', 'GATTWTO_One', 'RTA'\n",
        "        ]\n",
        "\n",
        "        # Step 1: Prepare the primary sample for PPML estimation.\n",
        "        print(\"Step 1: Preparing PPML sample with domestic trade...\")\n",
        "        ppml_sample = prepare_ppml_sample(main_df, country_level_df, core_vars)\n",
        "        print(\"Step 1: Success. PPML sample created.\")\n",
        "\n",
        "        # Step 2: Prepare the sample for Logit estimation.\n",
        "        print(\"Step 2: Preparing Logit sample (handling perfect prediction)...\")\n",
        "        logit_sample = prepare_logit_sample(ppml_sample, 'AnyTrade_Sector_Dummy')\n",
        "        print(\"Step 2: Success. Logit sample created.\")\n",
        "\n",
        "        # Step 3: Create temporal subsamples from the main PPML sample.\n",
        "        print(\"Step 3: Creating temporal subsamples...\")\n",
        "        # This specification should ideally come from the config dictionary.\n",
        "        periods = {\n",
        "            \"Pre_WTO\": (1980, 1994),\n",
        "            \"Early_WTO\": (1995, 2008),\n",
        "            \"Post_Crisis\": (2009, 2023)\n",
        "        }\n",
        "        temporal_subsamples = create_temporal_subsamples(ppml_sample, periods)\n",
        "        print(\"Step 3: Success. Temporal subsamples created.\")\n",
        "\n",
        "        # --- Compile and Return All Samples ---\n",
        "        analytical_samples = {\n",
        "            'ppml_sample': ppml_sample,\n",
        "            'logit_sample': logit_sample,\n",
        "            'temporal_subsamples': temporal_subsamples\n",
        "        }\n",
        "\n",
        "        print(\"\\nSUCCESS: All analytical samples have been prepared.\")\n",
        "        return analytical_samples\n",
        "\n",
        "    except (ValueError, TypeError, KeyError) as e:\n",
        "        print(f\"\\nERROR: Sample preparation pipeline failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "o15prAB7lBUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Clustering and Standard Error Preparation.\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9, Step 1: Cluster Group Construction and Validation\n",
        "# ==============================================================================\n",
        "\n",
        "def validate_cluster_structure(\n",
        "    df: pd.DataFrame,\n",
        "    cluster_col: str,\n",
        "    small_cluster_warning_frac: float = 0.05\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates the structure of clustering units in the panel data.\n",
        "\n",
        "    This function checks that the specified clustering column exists and is of a\n",
        "    suitable type. It then calculates the size of each cluster and issues a\n",
        "    warning if a significant fraction of clusters are \"small,\" which could\n",
        "    potentially affect the reliability of cluster-robust standard errors.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The analytical DataFrame.\n",
        "        cluster_col (str): The name of the column that identifies the clusters\n",
        "                           (e.g., 'DyadicPair').\n",
        "        small_cluster_warning_frac (float): The fraction of clusters that can be\n",
        "                                            \"small\" before a warning is issued.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame, passed through if validation succeeds.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the cluster column is missing or contains null values.\n",
        "        TypeError: If the cluster column is not of a suitable dtype.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    if cluster_col not in df.columns:\n",
        "        raise ValueError(f\"Cluster column '{cluster_col}' not found in DataFrame.\")\n",
        "    if df[cluster_col].isnull().any():\n",
        "        raise ValueError(f\"Cluster column '{cluster_col}' contains null values.\")\n",
        "\n",
        "    # --- Calculate Cluster Sizes ---\n",
        "    # Use the efficient .groupby().size() method to count observations per cluster.\n",
        "    cluster_sizes = df.groupby(cluster_col).size()\n",
        "\n",
        "    # --- Validate Cluster Structure ---\n",
        "    # Define a robust threshold for what constitutes a \"small\" cluster.\n",
        "    # This heuristic is more data-driven than a fixed number.\n",
        "    N = len(df)\n",
        "    G = len(cluster_sizes)\n",
        "    min_size_threshold = max(5, math.sqrt(N / G))\n",
        "\n",
        "    # Identify clusters that fall below this threshold.\n",
        "    small_clusters = cluster_sizes[cluster_sizes < min_size_threshold]\n",
        "\n",
        "    # Check if the fraction of small clusters exceeds the warning threshold.\n",
        "    if not small_clusters.empty and (len(small_clusters) / G > small_cluster_warning_frac):\n",
        "        warnings.warn(\n",
        "            f\"{len(small_clusters)} of {G} clusters ({len(small_clusters)/G:.1%}) \"\n",
        "            f\"have fewer than {min_size_threshold:.1f} observations. \"\n",
        "            f\"Asymptotic properties of cluster-robust standard errors may not hold.\"\n",
        "        )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9, Step 2: Bootstrap Sample Preparation\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_bootstrap_samples(\n",
        "    df: pd.DataFrame,\n",
        "    cluster_col: str,\n",
        "    n_replications: int,\n",
        "    seed: int\n",
        ") -> Iterator[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Generates bootstrap samples using cluster-based resampling.\n",
        "\n",
        "    This function is a generator that yields a specified number of bootstrap\n",
        "    samples. Crucially, it performs a cluster bootstrap, meaning it resamples\n",
        "    the clusters (e.g., dyadic pairs) with replacement, preserving the within-\n",
        "    cluster correlation structure of the panel data. This is the methodologically\n",
        "    correct way to bootstrap standard errors for clustered data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The analytical DataFrame.\n",
        "        cluster_col (str): The column identifying the clusters to resample.\n",
        "        n_replications (int): The number of bootstrap samples to generate.\n",
        "        seed (int): A seed for the random number generator for reproducibility.\n",
        "\n",
        "    Yields:\n",
        "        Iterator[pd.DataFrame]: A generator that yields one bootstrap sample\n",
        "                                DataFrame at a time.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    if cluster_col not in df.columns:\n",
        "        raise ValueError(f\"Cluster column '{cluster_col}' not found in DataFrame.\")\n",
        "\n",
        "    # --- Setup ---\n",
        "    # Initialize a dedicated random number generator for reproducibility.\n",
        "    rng = np.random.default_rng(seed)\n",
        "    # Get the array of unique cluster identifiers.\n",
        "    unique_clusters = df[cluster_col].unique()\n",
        "    n_clusters = len(unique_clusters)\n",
        "\n",
        "    # --- Generation Loop ---\n",
        "    for _ in range(n_replications):\n",
        "        # --- Resample Clusters ---\n",
        "        # Resample the CLUSTER IDENTIFIERS with replacement.\n",
        "        resampled_clusters_ids = rng.choice(\n",
        "            unique_clusters, size=n_clusters, replace=True\n",
        "        )\n",
        "\n",
        "        # --- Construct Bootstrap Sample ---\n",
        "        # An efficient way to build the sample is to create a temporary DataFrame\n",
        "        # of the resampled cluster IDs and merge it back to the original data.\n",
        "        # This is often faster than concatenating many small groups in a loop.\n",
        "        resampled_df_list = []\n",
        "        # Create a Series from the resampled IDs to count occurrences of each cluster.\n",
        "        resampled_counts = pd.Series(resampled_clusters_ids).value_counts()\n",
        "\n",
        "        # Get the data for each unique cluster that was selected.\n",
        "        # Grouping once is more efficient than multiple selections.\n",
        "        grouped_data = df.groupby(cluster_col)\n",
        "        for cluster_id, count in resampled_counts.items():\n",
        "            # Append the data for the cluster `count` times.\n",
        "            resampled_df_list.extend([grouped_data.get_group(cluster_id)] * count)\n",
        "\n",
        "        # Concatenate all the pieces to form the final bootstrap sample.\n",
        "        bootstrap_sample = pd.concat(resampled_df_list)\n",
        "\n",
        "        # Yield the complete bootstrap sample.\n",
        "        yield bootstrap_sample\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9, Step 3: Split-Panel Jackknife Setup\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_split_panel_jackknife_samples(\n",
        "    df: pd.DataFrame,\n",
        "    cluster_col: str,\n",
        "    n_groups: int\n",
        ") -> Iterator[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Generates samples for the split-panel jackknife procedure.\n",
        "\n",
        "    This function is a generator that implements the data-splitting step of the\n",
        "    split-panel jackknife estimator (Hinz, Stammann, and Wanner, 2021). It\n",
        "    divides the clusters (dyads) into a specified number of groups and then,\n",
        "    for each group, yields a DataFrame containing all data *except* for the\n",
        "    observations belonging to that group.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The analytical DataFrame.\n",
        "        cluster_col (str): The column identifying the clusters.\n",
        "        n_groups (int): The number of groups (G) to split the clusters into.\n",
        "\n",
        "    Yields:\n",
        "        Iterator[pd.DataFrame]: A generator that yields one jackknife sample\n",
        "                                DataFrame at a time.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    if cluster_col not in df.columns:\n",
        "        raise ValueError(f\"Cluster column '{cluster_col}' not found in DataFrame.\")\n",
        "    if not (isinstance(n_groups, int) and n_groups > 1):\n",
        "        raise ValueError(\"'n_groups' must be an integer greater than 1.\")\n",
        "\n",
        "    # --- Setup ---\n",
        "    # Get the unique cluster identifiers and sort them for deterministic grouping.\n",
        "    unique_clusters = np.sort(df[cluster_col].unique())\n",
        "\n",
        "    # Check if there are enough clusters to form the requested number of groups.\n",
        "    if n_groups > len(unique_clusters):\n",
        "        raise ValueError(f\"Cannot create {n_groups} groups from only {len(unique_clusters)} clusters.\")\n",
        "\n",
        "    # Split the sorted cluster IDs into G approximately equal-sized groups.\n",
        "    cluster_groups = np.array_split(unique_clusters, n_groups)\n",
        "\n",
        "    # --- Generation Loop ---\n",
        "    # Iterate through each group of clusters to be deleted.\n",
        "    for g in range(n_groups):\n",
        "        # Identify the clusters to be dropped for this jackknife replication.\n",
        "        clusters_to_drop = cluster_groups[g]\n",
        "\n",
        "        # Use a boolean mask with .isin() for an efficient filtering operation.\n",
        "        # This selects all rows where the cluster ID is NOT in the drop list.\n",
        "        jackknife_sample = df[~df[cluster_col].isin(clusters_to_drop)].copy()\n",
        "\n",
        "        # Yield the resulting jackknife sample.\n",
        "        yield jackknife_sample\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_inference_preparation(\n",
        "    df: pd.DataFrame,\n",
        "    cluster_col: str,\n",
        "    n_bootstrap: int,\n",
        "    n_jackknife_groups: int,\n",
        "    seed: int\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the preparation of data structures for advanced inference.\n",
        "\n",
        "    This master function executes the three core steps for Task 9:\n",
        "    1. Validates the cluster structure of the analytical data.\n",
        "    2. Creates a generator for producing cluster-bootstrap samples.\n",
        "    3. Creates a generator for producing split-panel jackknife samples.\n",
        "\n",
        "    It returns the validated DataFrame and the generators, which can then be\n",
        "    consumed by the estimation functions in subsequent tasks.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The final, analysis-ready DataFrame.\n",
        "        cluster_col (str): The name of the column identifying clusters.\n",
        "        n_bootstrap (int): The number of bootstrap replications to prepare for.\n",
        "        n_jackknife_groups (int): The number of groups for the split-panel jackknife.\n",
        "        seed (int): A random seed for the bootstrap generator's reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing:\n",
        "                        - 'validated_df': The input DataFrame after passing cluster validation.\n",
        "                        - 'bootstrap_generator': An iterator for bootstrap samples.\n",
        "                        - 'jackknife_generator': An iterator for jackknife samples.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Validate the cluster structure.\n",
        "        print(\"Step 1: Validating cluster structure...\")\n",
        "        validated_df = validate_cluster_structure(df, cluster_col)\n",
        "        print(\"Step 1: Success. Cluster structure is valid.\")\n",
        "\n",
        "        # Step 2: Create the bootstrap sample generator.\n",
        "        print(f\"Step 2: Preparing generator for {n_bootstrap} bootstrap samples...\")\n",
        "        bootstrap_generator = generate_bootstrap_samples(\n",
        "            validated_df, cluster_col, n_bootstrap, seed\n",
        "        )\n",
        "        print(\"Step 2: Success. Bootstrap generator created.\")\n",
        "\n",
        "        # Step 3: Create the split-panel jackknife sample generator.\n",
        "        print(f\"Step 3: Preparing generator for {n_jackknife_groups} jackknife groups...\")\n",
        "        jackknife_generator = generate_split_panel_jackknife_samples(\n",
        "            validated_df, cluster_col, n_jackknife_groups\n",
        "        )\n",
        "        print(\"Step 3: Success. Jackknife generator created.\")\n",
        "\n",
        "        # --- Compile and Return Results ---\n",
        "        inference_preparations = {\n",
        "            'validated_df': validated_df,\n",
        "            'bootstrap_generator': bootstrap_generator,\n",
        "            'jackknife_generator': jackknife_generator\n",
        "        }\n",
        "\n",
        "        print(\"\\nSUCCESS: All preparations for inference are complete.\")\n",
        "        return inference_preparations\n",
        "\n",
        "    except (ValueError, TypeError) as e:\n",
        "        print(f\"\\nERROR: Inference preparation pipeline failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "U6KMYx-KsDsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: PPML Gravity Model Estimation Implementation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10, Step 1: Construct PPML Specification Formula\n",
        "# ==============================================================================\n",
        "\n",
        "def construct_gravity_model_formula(\n",
        "    dependent_var: str,\n",
        "    regressors: List[str],\n",
        "    fixed_effects: List[str],\n",
        "    interaction_vars: List[str],\n",
        "    interaction_term: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Constructs a model specification formula for use with pyfixest.\n",
        "\n",
        "    This function programmatically builds a formula string that precisely\n",
        "    specifies the structural gravity model. It handles the critical requirement\n",
        "    that political distance variables only apply to international, not domestic,\n",
        "    trade flows by interacting them with an international trade dummy.\n",
        "\n",
        "    Args:\n",
        "        dependent_var (str): The name of the dependent variable.\n",
        "        regressors (List[str]): A list of main regressor variable names that\n",
        "                                apply to all observations.\n",
        "        fixed_effects (List[str]): A list of categorical variables to be used\n",
        "                                   as high-dimensional fixed effects.\n",
        "        interaction_vars (List[str]): A list of variables (e.g., political\n",
        "                                      distance) that should be interacted with\n",
        "                                      the international dummy.\n",
        "        interaction_term (str): The name of the binary variable that indicates\n",
        "                                international trade (1) vs. domestic trade (0).\n",
        "\n",
        "    Returns:\n",
        "        str: A complete model formula string in patsy/fixest syntax.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    if not all(isinstance(arg, list) for arg in [regressors, fixed_effects, interaction_vars]):\n",
        "        raise TypeError(\"Regressors, fixed_effects, and interaction_vars must be lists of strings.\")\n",
        "\n",
        "    # --- 2. Construct Regressor Part of the Formula ---\n",
        "    # Combine main regressors with the interacted terms.\n",
        "    # The syntax 'A:B' creates the interaction between A and B.\n",
        "    interacted_part = [f\"{interaction_term}:{var}\" for var in interaction_vars]\n",
        "    all_regressors = regressors + interacted_part\n",
        "    regressor_str = \" + \".join(all_regressors)\n",
        "\n",
        "    # --- 3. Construct Fixed Effects Part of the Formula ---\n",
        "    fe_str = \" + \".join(fixed_effects)\n",
        "\n",
        "    # --- 4. Assemble the Final Formula ---\n",
        "    # The full formula combines dependent var, regressors, and fixed effects,\n",
        "    # separated by '~' and '|'.\n",
        "    formula = f\"{dependent_var} ~ {regressor_str} | {fe_str}\"\n",
        "\n",
        "    return formula\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10, Step 2 & 3: Execute PPML Estimation with Cluster-Robust SEs\n",
        "# ==============================================================================\n",
        "\n",
        "def estimate_ppml_gravity_model(\n",
        "    df: pd.DataFrame,\n",
        "    formula: str,\n",
        "    cluster_col: str\n",
        ") -> Any:\n",
        "    \"\"\"\n",
        "    Estimates a PPML gravity model with high-dimensional fixed effects.\n",
        "\n",
        "    This function uses the `pyfixest` library, which is highly optimized for\n",
        "    estimating models with multiple fixed effects. It specifies a Poisson\n",
        "    model (PPML) and computes cluster-robust standard errors in a single,\n",
        "    efficient call.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The analytical DataFrame, prepared for estimation.\n",
        "        formula (str): The model specification formula from the previous step.\n",
        "        cluster_col (str): The name of the column to cluster standard errors on.\n",
        "\n",
        "    Returns:\n",
        "        Fixest: A fitted model object from pyfixest. This object contains\n",
        "                coefficients, standard errors, p-values, and other\n",
        "                estimation diagnostics.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any variables in the formula are not in the DataFrame.\n",
        "        RuntimeError: If the estimation algorithm fails to converge.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "    # pyfixest internally checks for missing columns, so we don't need to repeat it.\n",
        "\n",
        "    try:\n",
        "        # --- 2. Run Estimation ---\n",
        "        # The feols function is the core of pyfixest.\n",
        "        # `family='poisson'` specifies the PPML estimator.\n",
        "        # `vcov={'CRV1': cluster_col}` specifies one-way cluster-robust\n",
        "        # standard errors, clustered by the given column. pyfixest handles\n",
        "        # the sandwich estimation and finite-sample corrections internally.\n",
        "        model_fit = feols(\n",
        "            fml=formula,\n",
        "            data=df,\n",
        "            family='poisson',\n",
        "            vcov={'CRV1': cluster_col}\n",
        "        )\n",
        "\n",
        "        # Check for convergence (pyfixest stores this in the object).\n",
        "        if not model_fit._is_converged:\n",
        "             raise RuntimeError(\"PPML estimation failed to converge.\")\n",
        "\n",
        "        return model_fit\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any other errors during estimation.\n",
        "        print(f\"An error occurred during PPML estimation: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_ppml_estimation_pipeline(\n",
        "    df: pd.DataFrame,\n",
        "    spec: Dict[str, Any]\n",
        ") -> Any:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire PPML gravity model estimation pipeline.\n",
        "\n",
        "    This master function prepares the data and formula, runs the estimation,\n",
        "    and returns the final fitted model object.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The fully prepared analytical DataFrame.\n",
        "        spec (Dict[str, Any]): A dictionary defining the model specification,\n",
        "                               containing keys like 'dependent_var', 'regressors',\n",
        "                               'fixed_effects', etc.\n",
        "\n",
        "    Returns:\n",
        "        Fixest: The final fitted model object from pyfixest.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Prepare Data for Formula ---\n",
        "        # Create the international trade dummy required for the formula.\n",
        "        df_est = df.copy()\n",
        "        df_est['is_international'] = (df_est['ExporterISO'] != df_est['ImporterISO']).astype(int)\n",
        "\n",
        "        # --- 2. Construct the Model Formula ---\n",
        "        print(\"Step 1: Constructing model formula...\")\n",
        "        formula = construct_gravity_model_formula(\n",
        "            dependent_var=spec['dependent_var'],\n",
        "            regressors=spec['main_regressors'],\n",
        "            fixed_effects=spec['fixed_effects'],\n",
        "            interaction_vars=spec['interaction_regressors'],\n",
        "            interaction_term='is_international'\n",
        "        )\n",
        "        print(f\"Step 1: Success. Formula created:\\n{formula}\")\n",
        "\n",
        "        # --- 3. Estimate the Model ---\n",
        "        print(\"Step 2: Estimating PPML model with high-dimensional fixed effects...\")\n",
        "        model_results = estimate_ppml_gravity_model(\n",
        "            df=df_est,\n",
        "            formula=formula,\n",
        "            cluster_col=spec['cluster_col']\n",
        "        )\n",
        "        print(\"Step 2: Success. Estimation complete.\")\n",
        "\n",
        "        print(\"\\nSUCCESS: PPML estimation pipeline complete.\")\n",
        "        return model_results\n",
        "\n",
        "    except (ValueError, TypeError, KeyError, RuntimeError) as e:\n",
        "        print(f\"\\nERROR: PPML estimation pipeline failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "bjPV_F15t4xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Logit Model Estimation for Extensive Margin\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11, Step 2: Implement Split-Panel Jackknife Estimation\n",
        "# ==============================================================================\n",
        "\n",
        "def estimate_logit_split_panel_jackknife(\n",
        "    df: pd.DataFrame,\n",
        "    formula: str,\n",
        "    jackknife_generator: Iterator[pd.DataFrame]\n",
        ") -> Tuple[pd.Series, Any]:\n",
        "    \"\"\"\n",
        "    Estimates a Logit model using the split-panel jackknife for bias correction.\n",
        "\n",
        "    This function implements the bias correction procedure from Hinz, Stammann,\n",
        "    and Wanner (2021) for non-linear fixed effects models. It first estimates\n",
        "    the model on the full sample, then iteratively re-estimates it on samples\n",
        "    where one group of clusters has been removed. The final coefficients are\n",
        "    bias-corrected based on these estimates.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The analytical sample prepared for Logit estimation.\n",
        "        formula (str): The model specification in pyfixest format.\n",
        "        jackknife_generator (Iterator[pd.DataFrame]): A generator that yields\n",
        "            the jackknife subsamples (from Task 9).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Series, Any]: A tuple containing:\n",
        "            - The bias-corrected coefficient estimates as a pandas Series.\n",
        "            - The unfitted model object from the full-sample estimation for diagnostics.\n",
        "    \"\"\"\n",
        "    # --- 1. Full Sample Estimation ---\n",
        "    # Estimate the model on the complete dataset to get the initial,\n",
        "    # potentially biased coefficients (beta_full).\n",
        "    full_model_fit = feglm(fml=formula, data=df, family='logit')\n",
        "    beta_full = full_model_fit.coef()\n",
        "\n",
        "    # --- 2. Jackknife Loop for Sub-Sample Estimations ---\n",
        "    # Collect the coefficient estimates from each jackknife replication.\n",
        "    beta_jackknife_list = []\n",
        "    n_groups = 0\n",
        "    for jackknife_sample in jackknife_generator:\n",
        "        # Estimate the model on the subsample where one group is deleted.\n",
        "        sub_model_fit = feglm(fml=formula, data=jackknife_sample, family='logit')\n",
        "        beta_jackknife_list.append(sub_model_fit.coef())\n",
        "        n_groups += 1\n",
        "\n",
        "    # --- 3. Bias Correction Calculation ---\n",
        "    # Concatenate the list of coefficient series into a DataFrame.\n",
        "    beta_jackknife_df = pd.concat(beta_jackknife_list, axis=1)\n",
        "\n",
        "    # Equation: beta_BC = beta_full - (G-1)/G * sum(beta_{-g} - beta_full)\n",
        "    # Calculate the mean of the jackknife estimates.\n",
        "    mean_beta_jackknife = beta_jackknife_df.mean(axis=1)\n",
        "    # Calculate the bias term.\n",
        "    bias_term = (n_groups - 1) * (mean_beta_jackknife - beta_full)\n",
        "    # Compute the bias-corrected coefficients.\n",
        "    beta_bias_corrected = beta_full - bias_term\n",
        "\n",
        "    return beta_bias_corrected, full_model_fit\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11, Step 3: Bootstrap Standard Error Computation\n",
        "# ==============================================================================\n",
        "\n",
        "def _bootstrap_replication_worker(\n",
        "    bootstrap_sample: pd.DataFrame,\n",
        "    formula: str,\n",
        "    cluster_col: str,\n",
        "    n_jackknife_groups: int\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    A helper function to run one full bootstrap replication.\n",
        "\n",
        "    This function takes a single bootstrap sample and runs the entire\n",
        "    split-panel jackknife estimation procedure on it. It is designed to be\n",
        "    called in parallel by the main bootstrap function.\n",
        "\n",
        "    Args:\n",
        "        bootstrap_sample (pd.DataFrame): A single bootstrap sample.\n",
        "        formula (str): The model specification formula.\n",
        "        cluster_col (str): The name of the clustering column.\n",
        "        n_jackknife_groups (int): The number of groups for the jackknife.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: The bias-corrected coefficient estimates for this replication.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a jackknife generator for this specific bootstrap sample.\n",
        "        jackknife_gen = generate_split_panel_jackknife_samples(\n",
        "            bootstrap_sample, cluster_col, n_jackknife_groups\n",
        "        )\n",
        "        # Run the jackknife estimation procedure.\n",
        "        beta_bc, _ = estimate_logit_split_panel_jackknife(\n",
        "            bootstrap_sample, formula, jackknife_gen\n",
        "        )\n",
        "        return beta_bc\n",
        "    except Exception:\n",
        "        # In case of a convergence failure on a specific bootstrap sample,\n",
        "        # return NaNs. This is a robust way to handle problematic resamples.\n",
        "        return pd.Series(dtype=np.float64)\n",
        "\n",
        "\n",
        "def compute_logit_bootstrap_standard_errors(\n",
        "    df: pd.DataFrame,\n",
        "    formula: str,\n",
        "    bootstrap_generator: Iterator[pd.DataFrame],\n",
        "    cluster_col: str,\n",
        "    n_jackknife_groups: int,\n",
        "    n_jobs: int = -1\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes standard errors for the Logit model using a bootstrap of the\n",
        "    entire split-panel jackknife procedure.\n",
        "\n",
        "    This function implements the full, computationally intensive inference\n",
        "    procedure. For each bootstrap sample, it re-runs the entire split-panel\n",
        "    jackknife estimation to get a set of bias-corrected coefficients. The\n",
        "    standard deviation of these coefficients across all bootstrap replications\n",
        "    is the bootstrap standard error. The process is parallelized for efficiency.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The analytical sample for Logit estimation.\n",
        "        formula (str): The model specification formula.\n",
        "        bootstrap_generator (Iterator[pd.DataFrame]): A generator for bootstrap samples.\n",
        "        cluster_col (str): The name of the clustering column.\n",
        "        n_jackknife_groups (int): The number of groups for the jackknife.\n",
        "        n_jobs (int): The number of CPU cores to use for parallelization.\n",
        "                      -1 means use all available cores.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: The bootstrap standard errors for the coefficients.\n",
        "    \"\"\"\n",
        "    # Use joblib.Parallel to distribute the bootstrap replications across CPU cores.\n",
        "    # This is essential for making this computationally demanding task feasible.\n",
        "    with Parallel(n_jobs=n_jobs) as parallel:\n",
        "        # The `delayed` function creates a tuple of (function, args, kwargs)\n",
        "        # for each task to be executed in parallel.\n",
        "        results = parallel(\n",
        "            delayed(_bootstrap_replication_worker)(\n",
        "                sample, formula, cluster_col, n_jackknife_groups\n",
        "            ) for sample in bootstrap_generator\n",
        "        )\n",
        "\n",
        "    # Concatenate the list of coefficient Series into a single DataFrame.\n",
        "    # Each column represents a bootstrap replication.\n",
        "    bootstrap_coefs_df = pd.concat(results, axis=1)\n",
        "\n",
        "    # Drop any replications that failed to converge (resulted in NaNs).\n",
        "    n_failures = bootstrap_coefs_df.isnull().any(axis=0).sum()\n",
        "    if n_failures > 0:\n",
        "        warnings.warn(\n",
        "            f\"{n_failures} of {len(results)} bootstrap replications failed to \"\n",
        "            \"converge and were dropped.\"\n",
        "        )\n",
        "        bootstrap_coefs_df.dropna(axis=1, inplace=True)\n",
        "\n",
        "    # The bootstrap standard error is the standard deviation across replications.\n",
        "    # ddof=1 provides the sample standard deviation.\n",
        "    standard_errors = bootstrap_coefs_df.std(axis=1, ddof=1)\n",
        "    standard_errors.name = \"BootstrapSE\"\n",
        "\n",
        "    return standard_errors\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_logit_estimation_pipeline(\n",
        "    df: pd.DataFrame,\n",
        "    spec: Dict[str, Any],\n",
        "    constants: 'ProjectConstants',\n",
        "    seed: int\n",
        ") -> Dict[str, pd.Series]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire Logit model estimation and inference pipeline.\n",
        "\n",
        "    This master function executes the full, advanced procedure for the extensive\n",
        "    margin analysis:\n",
        "    1. Prepares the data and generators for jackknife and bootstrap.\n",
        "    2. Estimates the model coefficients using the split-panel jackknife for bias correction.\n",
        "    3. Computes valid standard errors using a bootstrap of the entire jackknife procedure.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The analytical sample prepared for Logit estimation.\n",
        "        spec (Dict[str, Any]): A dictionary defining the model specification.\n",
        "        constants (ProjectConstants): The dataclass of project constants.\n",
        "        seed (int): A random seed for the bootstrap generator.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.Series]: A dictionary containing the final results:\n",
        "                              - 'coefficients': The bias-corrected coefficients.\n",
        "                              - 'standard_errors': The bootstrap standard errors.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Preparation ---\n",
        "        print(\"Step 1: Preparing data and generators for Logit estimation...\")\n",
        "        # The formula construction function from Task 10 can be reused.\n",
        "        formula = construct_gravity_model_formula(\n",
        "            dependent_var=spec['dependent_var'],\n",
        "            regressors=spec['main_regressors'],\n",
        "            fixed_effects=spec['fixed_effects'],\n",
        "            interaction_vars=spec['interaction_regressors'],\n",
        "            interaction_term='is_international'\n",
        "        )\n",
        "        # Create the necessary sample generators from Task 9.\n",
        "        n_jackknife_groups = 50 # A reasonable default\n",
        "        jackknife_gen = generate_split_panel_jackknife_samples(\n",
        "            df, spec['cluster_col'], n_jackknife_groups\n",
        "        )\n",
        "        bootstrap_gen = generate_bootstrap_samples(\n",
        "            df, spec['cluster_col'], constants.N_BOOTSTRAP, seed\n",
        "        )\n",
        "        print(\"Step 1: Success. Formula and generators are ready.\")\n",
        "\n",
        "        # --- 2. Point Estimation (Split-Panel Jackknife) ---\n",
        "        print(f\"Step 2: Estimating coefficients via split-panel jackknife ({n_jackknife_groups} groups)...\")\n",
        "        coefficients, _ = estimate_logit_split_panel_jackknife(df, formula, jackknife_gen)\n",
        "        print(\"Step 2: Success. Bias-corrected coefficients estimated.\")\n",
        "\n",
        "        # --- 3. Inference (Bootstrap) ---\n",
        "        print(f\"Step 3: Computing standard errors via bootstrap ({constants.N_BOOTSTRAP} replications)...\")\n",
        "        standard_errors = compute_logit_bootstrap_standard_errors(\n",
        "            df, formula, bootstrap_gen, spec['cluster_col'], n_jackknife_groups\n",
        "        )\n",
        "        print(\"Step 3: Success. Bootstrap standard errors computed.\")\n",
        "\n",
        "        # --- 4. Compile and Return Results ---\n",
        "        results = {\n",
        "            'coefficients': coefficients,\n",
        "            'standard_errors': standard_errors\n",
        "        }\n",
        "\n",
        "        print(\"\\nSUCCESS: Logit estimation pipeline complete.\")\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Logit estimation pipeline failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "kBkUxJL9u89K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Model Diagnosis and Specification Testing\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12, Step 1: PPML Model Diagnostics\n",
        "# ==============================================================================\n",
        "\n",
        "def diagnose_ppml_model(\n",
        "    ppml_results: Fixest,\n",
        "    ppml_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs diagnostic checks on a fitted PPML (Poisson) model.\n",
        "\n",
        "    This function assesses the validity and properties of the PPML estimation by:\n",
        "    1. Verifying that the estimation algorithm converged.\n",
        "    2. Calculating a dispersion parameter (phi) to test for overdispersion,\n",
        "       a common issue in count data models.\n",
        "\n",
        "    Args:\n",
        "        ppml_results (Fixest): The fitted model object from pyfixest.\n",
        "        ppml_df (pd.DataFrame): The DataFrame used for the PPML estimation.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing key diagnostic statistics.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    if not isinstance(ppml_results, Fixest):\n",
        "        raise TypeError(\"Input 'ppml_results' must be a pyfixest Fixest object.\")\n",
        "    if not isinstance(ppml_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'ppml_df' must be a pandas DataFrame.\")\n",
        "\n",
        "    diagnostics = {}\n",
        "\n",
        "    # --- 2. Check Convergence Status ---\n",
        "    # The _is_converged attribute is a boolean flag set by the pyfixest engine.\n",
        "    diagnostics['is_converged'] = ppml_results._is_converged\n",
        "\n",
        "    # --- 3. Test for Overdispersion ---\n",
        "    # Extract the dependent variable (y) and fitted values (mu_hat).\n",
        "    y_true = ppml_results.dependent_var.values.flatten()\n",
        "    mu_hat = ppml_results.predict(newdata=ppml_df)\n",
        "\n",
        "    # Calculate the number of observations and estimated parameters.\n",
        "    n_obs = ppml_results.nobs\n",
        "    # Note: nparams is the number of regression coefficients, not including\n",
        "    # the absorbed fixed effects. This is a standard approximation for this test.\n",
        "    k_params = ppml_results.nparams\n",
        "\n",
        "    # Calculate residual degrees of freedom.\n",
        "    rdf = n_obs - k_params\n",
        "    diagnostics['residual_degrees_of_freedom'] = rdf\n",
        "\n",
        "    # Equation: Pearson Chi-Squared = sum( (y_i - mu_hat_i)^2 / mu_hat_i )\n",
        "    # Calculate the Pearson chi-squared statistic. A small epsilon is added\n",
        "    # to the denominator for numerical stability in case of mu_hat = 0.\n",
        "    pearson_chi2 = np.sum((y_true - mu_hat)**2 / (mu_hat + 1e-8))\n",
        "    diagnostics['pearson_chi2'] = pearson_chi2\n",
        "\n",
        "    # Equation: Dispersion (phi) = Pearson Chi-Squared / (N - K)\n",
        "    # A value >> 1 suggests overdispersion.\n",
        "    if rdf > 0:\n",
        "        dispersion_phi = pearson_chi2 / rdf\n",
        "        diagnostics['dispersion_phi'] = dispersion_phi\n",
        "    else:\n",
        "        diagnostics['dispersion_phi'] = np.nan\n",
        "\n",
        "    return diagnostics\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12, Step 2: Logit Model Diagnostics\n",
        "# ==============================================================================\n",
        "\n",
        "def diagnose_logit_model(\n",
        "    logit_full_sample_results: Fixest\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs diagnostic checks on a fitted high-dimensional fixed effects Logit model.\n",
        "\n",
        "    Standard goodness-of-fit tests are often invalid with many fixed effects.\n",
        "    This function focuses on two key, informative diagnostics:\n",
        "    1. Convergence status, which is a strong indicator of potential separation issues.\n",
        "    2. McFadden's Pseudo R-squared, which measures the improvement in model fit\n",
        "       over a null model with only an intercept.\n",
        "\n",
        "    Args:\n",
        "        logit_full_sample_results (Fixest): The fitted model object from the\n",
        "            full-sample Logit estimation (before jackknifing).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing key diagnostic statistics.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    if not isinstance(logit_full_sample_results, Fixest):\n",
        "        raise TypeError(\"Input must be a pyfixest Fixest object from a Logit model.\")\n",
        "\n",
        "    diagnostics = {}\n",
        "\n",
        "    # --- 2. Check Convergence Status (for Separation) ---\n",
        "    # For Logit models, failure to converge often indicates quasi-complete\n",
        "    # separation, even after filtering for perfect prediction.\n",
        "    diagnostics['is_converged'] = logit_full_sample_results._is_converged\n",
        "\n",
        "    # --- 3. Calculate McFadden's Pseudo R-squared ---\n",
        "    # Get the log-likelihood of the fully specified model.\n",
        "    logL_full = logit_full_sample_results.loglik()\n",
        "\n",
        "    # Calculate the log-likelihood of a null (intercept-only) model.\n",
        "    y = logit_full_sample_results.dependent_var.values.flatten()\n",
        "    p_mean = np.mean(y)\n",
        "    logL_null = np.sum(y * np.log(p_mean) + (1 - y) * np.log(1 - p_mean))\n",
        "\n",
        "    # Equation: R^2_McFadden = 1 - (logL_full / logL_null)\n",
        "    if logL_null != 0:\n",
        "        pseudo_r2_mcfadden = 1 - (logL_full / logL_null)\n",
        "        diagnostics['pseudo_r2_mcfadden'] = pseudo_r2_mcfadden\n",
        "    else:\n",
        "        diagnostics['pseudo_r2_mcfadden'] = np.nan\n",
        "\n",
        "    return diagnostics\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12, Step 3: Cross-Model Consistency Validation\n",
        "# ==============================================================================\n",
        "\n",
        "def validate_cross_model_consistency(\n",
        "    ppml_results: Fixest,\n",
        "    logit_results: Dict[str, pd.Series]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compares key results between the PPML and Logit models for consistency.\n",
        "\n",
        "    This function creates a summary table comparing the estimated coefficients,\n",
        "    standard errors, and p-values for variables common to both models. It also\n",
        "    adds a check for sign consistency, which is a key indicator of a robust\n",
        "    underlying relationship, even if coefficient magnitudes are not directly\n",
        "    comparable.\n",
        "\n",
        "    Args:\n",
        "        ppml_results (Fixest): The fitted model object from the PPML estimation.\n",
        "        logit_results (Dict[str, pd.Series]): The results dictionary from the\n",
        "            Logit estimation pipeline, containing 'coefficients' and\n",
        "            'standard_errors'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by regressor name, summarizing and\n",
        "                      comparing the results from the two models.\n",
        "    \"\"\"\n",
        "    # --- 1. Extract Results from Model Objects ---\n",
        "    ppml_summary = ppml_results.summary().reset_index().rename(columns={'index': 'Variable'})\n",
        "    logit_coefs = logit_results['coefficients'].rename('Coefficient_Logit')\n",
        "    logit_ses = logit_results['standard_errors'].rename('StdError_Logit')\n",
        "\n",
        "    # Calculate Logit z-scores and p-values.\n",
        "    logit_z = logit_coefs / logit_ses\n",
        "    logit_p = (2 * (1 - norm.cdf(np.abs(logit_z)))).rename('PValue_Logit')\n",
        "\n",
        "    # Combine Logit results into a single DataFrame.\n",
        "    logit_summary = pd.concat([logit_coefs, logit_ses, logit_p], axis=1).reset_index().rename(columns={'index': 'Variable'})\n",
        "\n",
        "    # --- 2. Merge Results for Comparison ---\n",
        "    # Perform an inner merge to keep only the variables present in both models.\n",
        "    comparison_df = pd.merge(\n",
        "        ppml_summary[['Variable', 'Estimate', 'Std. Error', 'Pr(>|t|)']],\n",
        "        logit_summary,\n",
        "        on='Variable',\n",
        "        how='inner'\n",
        "    ).rename(columns={\n",
        "        'Estimate': 'Coefficient_PPML',\n",
        "        'Std. Error': 'StdError_PPML',\n",
        "        'Pr(>|t|)': 'PValue_PPML'\n",
        "    }).set_index('Variable')\n",
        "\n",
        "    # --- 3. Check for Sign Consistency ---\n",
        "    # A robust finding should have the same sign in both models.\n",
        "    comparison_df['Sign_Consistent'] = (\n",
        "        np.sign(comparison_df['Coefficient_PPML']) == np.sign(comparison_df['Coefficient_Logit'])\n",
        "    )\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_diagnostics_pipeline(\n",
        "    ppml_results: Fixest,\n",
        "    ppml_df: pd.DataFrame,\n",
        "    logit_full_sample_results: Fixest,\n",
        "    logit_final_results: Dict[str, pd.Series]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire model diagnosis and specification testing pipeline.\n",
        "\n",
        "    Args:\n",
        "        ppml_results (Fixest): The fitted PPML model object.\n",
        "        ppml_df (pd.DataFrame): The DataFrame used for the PPML estimation.\n",
        "        logit_full_sample_results (Fixest): The full-sample Logit model fit,\n",
        "            used for diagnostics before jackknifing.\n",
        "        logit_final_results (Dict[str, pd.Series]): The final bias-corrected\n",
        "            and bootstrapped results from the Logit pipeline.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the diagnostic reports for\n",
        "                        each model and the cross-model consistency check.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Run diagnostics on the PPML model.\n",
        "        print(\"Step 1: Running diagnostics on PPML model...\")\n",
        "        ppml_diagnostics = diagnose_ppml_model(ppml_results, ppml_df)\n",
        "        print(\"Step 1: Success. PPML diagnostics complete.\")\n",
        "\n",
        "        # Step 2: Run diagnostics on the Logit model.\n",
        "        print(\"Step 2: Running diagnostics on Logit model...\")\n",
        "        logit_diagnostics = diagnose_logit_model(logit_full_sample_results)\n",
        "        print(\"Step 2: Success. Logit diagnostics complete.\")\n",
        "\n",
        "        # Step 3: Perform cross-model consistency validation.\n",
        "        print(\"Step 3: Performing cross-model consistency validation...\")\n",
        "        consistency_report = validate_cross_model_consistency(\n",
        "            ppml_results, logit_final_results\n",
        "        )\n",
        "        print(\"Step 3: Success. Cross-model consistency validated.\")\n",
        "\n",
        "        # --- Compile and Return All Reports ---\n",
        "        master_report = {\n",
        "            'ppml_diagnostics': ppml_diagnostics,\n",
        "            'logit_diagnostics': logit_diagnostics,\n",
        "            'consistency_report': consistency_report\n",
        "        }\n",
        "\n",
        "        print(\"\\nSUCCESS: Diagnostics pipeline complete.\")\n",
        "        return master_report\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Diagnostics pipeline failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "YMeMy1QVwL58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Delta Method Implementation for Nonlinear Effect Sizes\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13, Step 1: Calculate Political Distance Standard Deviations\n",
        "# ==============================================================================\n",
        "\n",
        "def calculate_pd_standard_deviations(\n",
        "    df: pd.DataFrame,\n",
        "    pd_variables: List[str]\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculates the sample standard deviation for specified political distance variables.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The analytical DataFrame.\n",
        "        pd_variables (List[str]): A list of the political distance column names\n",
        "                                  for which to calculate the standard deviation.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A dictionary mapping variable names to their standard deviation.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
        "    if not all(col in df.columns for col in pd_variables):\n",
        "        raise ValueError(\"One or more specified pd_variables not in DataFrame.\")\n",
        "\n",
        "    # --- Calculation ---\n",
        "    # Use a dictionary comprehension for a concise implementation.\n",
        "    # .std(ddof=1) correctly calculates the sample standard deviation,\n",
        "    # automatically ignoring NaN values.\n",
        "    std_devs = {col: df[col].std(ddof=1) for col in pd_variables}\n",
        "\n",
        "    return std_devs\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13, Step 2: Compute Marginal Effects for PPML Models\n",
        "# ==============================================================================\n",
        "\n",
        "def compute_ppml_marginal_effects(\n",
        "    ppml_results: Fixest,\n",
        "    std_dev: float,\n",
        "    main_pd_term: str,\n",
        "    interaction_scenarios: Dict[str, List[str]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes one-std-deviation marginal effects and standard errors for a PPML model.\n",
        "\n",
        "    This function calculates the percentage change in the dependent variable for a\n",
        "    one-standard-deviation increase in political distance under different\n",
        "    institutional scenarios (e.g., non-WTO members vs. WTO members). It uses the\n",
        "    Delta Method to calculate the standard errors for these non-linear transformations.\n",
        "\n",
        "    Args:\n",
        "        ppml_results (Fixest): The fitted model object from the PPML estimation.\n",
        "        std_dev (float): The standard deviation of the political distance variable.\n",
        "        main_pd_term (str): The name of the main political distance regressor.\n",
        "        interaction_scenarios (Dict[str, List[str]]): A dictionary defining the\n",
        "            scenarios. Keys are scenario names (e.g., \"Both WTO Members\"), and\n",
        "            values are lists of additional interaction term coefficients to\n",
        "            include for that scenario.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by scenario, with columns for the\n",
        "                      point estimate ('EffectPct') and its standard error ('StdError').\n",
        "    \"\"\"\n",
        "    # --- 1. Extract Model Components ---\n",
        "    # Get the coefficient estimates as a Series.\n",
        "    beta = ppml_results.coef()\n",
        "    # Get the variance-covariance matrix as a DataFrame.\n",
        "    vcov = ppml_results.vcov()\n",
        "    # Ensure consistent ordering.\n",
        "    vcov = vcov.loc[beta.index, beta.index]\n",
        "\n",
        "    results = {}\n",
        "    # --- 2. Iterate Through Scenarios ---\n",
        "    for scenario_name, interaction_terms in interaction_scenarios.items():\n",
        "        # --- 3. Calculate Point Estimate ---\n",
        "        # Identify all relevant coefficient names for this scenario.\n",
        "        relevant_terms = [main_pd_term] + interaction_terms\n",
        "        # Calculate the combined linear effect (c' * beta).\n",
        "        linear_effect = beta[relevant_terms].sum()\n",
        "\n",
        "        # Equation: Effect = 100 * (exp(linear_effect * std_dev) - 1)\n",
        "        # This is the percentage change for a one-standard-deviation shock.\n",
        "        point_estimate = 100 * (np.exp(linear_effect * std_dev) - 1)\n",
        "\n",
        "        # --- 4. Calculate Standard Error via Delta Method ---\n",
        "        # Create the selection vector 'c' for the gradient.\n",
        "        c_vector = pd.Series(0, index=beta.index)\n",
        "        c_vector[relevant_terms] = 1\n",
        "\n",
        "        # Equation: grad(g(beta)) = 100 * exp(c' * beta * std_dev) * std_dev * c\n",
        "        # Calculate the gradient of the transformation function w.r.t. beta.\n",
        "        gradient = 100 * np.exp(linear_effect * std_dev) * std_dev * c_vector.values\n",
        "\n",
        "        # Equation: Var(g(beta)) = grad' * VCV(beta) * grad\n",
        "        # Calculate the variance of the transformed effect.\n",
        "        effect_variance = gradient.T @ vcov.values @ gradient\n",
        "        # The standard error is the square root of the variance.\n",
        "        std_error = np.sqrt(effect_variance)\n",
        "\n",
        "        # Store the results.\n",
        "        results[scenario_name] = {'EffectPct': point_estimate, 'StdError': std_error}\n",
        "\n",
        "    # --- 5. Format and Return Output ---\n",
        "    return pd.DataFrame.from_dict(results, orient='index')\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13, Step 3: Compute Marginal Effects for Logit Models\n",
        "# ==============================================================================\n",
        "\n",
        "def compute_logit_marginal_effects(\n",
        "    df: pd.DataFrame,\n",
        "    formula: str,\n",
        "    logit_results: Dict[str, Any],\n",
        "    std_dev: float,\n",
        "    main_pd_term: str,\n",
        "    interaction_scenarios: Dict[str, List[str]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes average marginal effects (AMEs) for a Logit model using the\n",
        "    full bootstrap distribution for inference.\n",
        "\n",
        "    This function calculates the average change in probability for a one-std-dev\n",
        "    increase in political distance. Instead of the Delta Method, it robustly\n",
        "    calculates the standard error by re-computing the AME for each bootstrap\n",
        "    replication of the coefficients, then taking the standard deviation of that\n",
        "    distribution.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The analytical DataFrame used for the Logit estimation.\n",
        "        formula (str): The model specification formula.\n",
        "        logit_results (Dict[str, Any]): The results dictionary from the Logit\n",
        "            pipeline, containing 'coefficients' and 'bootstrap_coefs_df'.\n",
        "        std_dev (float): The standard deviation of the political distance variable.\n",
        "        main_pd_term (str): The name of the main political distance regressor.\n",
        "        interaction_scenarios (Dict[str, List[str]]): A dictionary defining the\n",
        "            scenarios for which to calculate effects.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by scenario, with columns for the\n",
        "                      point estimate ('AME_pp') and its bootstrap standard error ('StdError').\n",
        "    \"\"\"\n",
        "    # --- 1. Prepare Design Matrix ---\n",
        "    # Use pyfixest's internal tools to create the design matrix from the formula.\n",
        "    # This correctly handles all formula syntax.\n",
        "    Y, X, _, _, _, _ = model_matrix_fixest(formula, df, family='logit')\n",
        "\n",
        "    # --- 2. Define Helper for AME Calculation ---\n",
        "    def calculate_ame(beta_vector: pd.Series) -> Dict[str, float]:\n",
        "        \"\"\"Calculates the AME for all scenarios given one coefficient vector.\"\"\"\n",
        "        # Calculate the linear prediction for all observations.\n",
        "        eta = X @ beta_vector\n",
        "        # Calculate the PDF of the logistic distribution at each prediction.\n",
        "        pdf_eta = logistic.pdf(eta)\n",
        "\n",
        "        ames = {}\n",
        "        for name, terms in interaction_scenarios.items():\n",
        "            # Get the relevant sum of coefficients for this scenario.\n",
        "            beta_sum = beta_vector[[main_pd_term] + terms].sum()\n",
        "            # Calculate the marginal effect for each observation.\n",
        "            marginal_effects = pdf_eta * beta_sum\n",
        "            # The AME is the mean of these individual effects.\n",
        "            # Multiply by std_dev to get the effect in percentage points (pp).\n",
        "            ames[name] = np.mean(marginal_effects) * std_dev * 100\n",
        "        return ames\n",
        "\n",
        "    # --- 3. Calculate Point Estimate (AME) ---\n",
        "    # Calculate the AME using the main, bias-corrected coefficient estimates.\n",
        "    point_estimates = calculate_ame(logit_results['coefficients'])\n",
        "\n",
        "    # --- 4. Calculate Standard Errors (Bootstrap) ---\n",
        "    # Get the DataFrame of bootstrap coefficients (each column is a replication).\n",
        "    bootstrap_coefs = logit_results['bootstrap_coefs_df']\n",
        "    # Apply the AME calculation to each column (each bootstrap replication).\n",
        "    bootstrap_ames = bootstrap_coefs.apply(calculate_ame, axis=0)\n",
        "    # The result is a DataFrame where rows are scenarios and columns are replications.\n",
        "    # The standard error is the standard deviation across the rows.\n",
        "    std_errors = bootstrap_ames.std(axis=1, ddof=1)\n",
        "\n",
        "    # --- 5. Format and Return Output ---\n",
        "    results_df = pd.DataFrame.from_dict(point_estimates, orient='index', columns=['AME_pp'])\n",
        "    results_df['StdError'] = std_errors\n",
        "\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "UtHfHYj5xKzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Institutional Moderation Effect Quantification\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14, Step 1: WTO Attenuation Effect Quantification\n",
        "# ==============================================================================\n",
        "\n",
        "def quantify_wto_attenuation_effect(\n",
        "    model_results: Fixest,\n",
        "    std_dev: float,\n",
        "    main_pd_term: str,\n",
        "    wto_interaction_term: str\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Quantifies the WTO's attenuation effect and its standard error via the Delta Method.\n",
        "\n",
        "    This function provides a complete and rigorous calculation of one of the\n",
        "    paper's key findings: the extent to which WTO membership mitigates the\n",
        "    negative impact of political distance on trade. It calculates:\n",
        "    1. The marginal effect for non-members (baseline).\n",
        "    2. The marginal effect for WTO members.\n",
        "    3. The attenuation effect, which is the difference between (1) and (2).\n",
        "    4. The standard error of this difference, calculated precisely using the\n",
        "       multivariate Delta Method and the model's full variance-covariance matrix.\n",
        "\n",
        "    Args:\n",
        "        model_results (Fixest): The fitted model object from pyfixest.\n",
        "        std_dev (float): The standard deviation of the political distance variable.\n",
        "        main_pd_term (str): The name of the main political distance regressor\n",
        "                            (e.g., 'is_international:PD_IHS').\n",
        "        wto_interaction_term (str): The name of the interaction term between\n",
        "                                    political distance and WTO membership\n",
        "                                    (e.g., 'is_international:PD_IHS:GATTWTO_Both').\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A dictionary containing the point estimate of the\n",
        "                          attenuation effect ('Attenuation_Effect_Pct') and its\n",
        "                          standard error ('Attenuation_StdError').\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    if not isinstance(model_results, Fixest):\n",
        "        raise TypeError(\"Input 'model_results' must be a pyfixest Fixest object.\")\n",
        "    required_terms = [main_pd_term, wto_interaction_term]\n",
        "    if not all(term in model_results.coef().index for term in required_terms):\n",
        "        raise ValueError(\"One or more specified terms not found in model coefficients.\")\n",
        "\n",
        "    # --- 2. Extract Model Components ---\n",
        "    # Get the coefficient estimates as a pandas Series.\n",
        "    beta = model_results.coef()\n",
        "    # Get the full variance-covariance matrix.\n",
        "    vcov = model_results.vcov()\n",
        "    # Ensure the VCV matrix is aligned with the coefficient vector.\n",
        "    vcov = vcov.loc[beta.index, beta.index]\n",
        "\n",
        "    # --- 3. Define Scenarios and Selection Vectors ---\n",
        "    # Create a zero vector 'c' for selecting coefficients.\n",
        "    c_base = pd.Series(0.0, index=beta.index)\n",
        "    # For the baseline (non-WTO) scenario, only the main PD term is active.\n",
        "    c_base[main_pd_term] = 1.0\n",
        "\n",
        "    # For the WTO scenario, both the main term and the interaction term are active.\n",
        "    c_wto = c_base.copy()\n",
        "    c_wto[wto_interaction_term] = 1.0\n",
        "\n",
        "    # --- 4. Calculate Point Estimates for Each Effect ---\n",
        "    # Calculate the linear combination of coefficients for the baseline.\n",
        "    lin_eff_base = c_base.T @ beta\n",
        "    # Calculate the linear combination for the WTO scenario.\n",
        "    lin_eff_wto = c_wto.T @ beta\n",
        "\n",
        "    # Equation for percentage effect: g(beta) = 100 * (exp(c' * beta * std_dev) - 1)\n",
        "    # Calculate the non-linear percentage effect for the baseline.\n",
        "    effect_base = 100 * (np.exp(lin_eff_base * std_dev) - 1)\n",
        "    # Calculate the non-linear percentage effect for the WTO scenario.\n",
        "    effect_wto = 100 * (np.exp(lin_eff_wto * std_dev) - 1)\n",
        "\n",
        "    # The attenuation is the (negative) baseline effect minus the (less negative) WTO effect.\n",
        "    attenuation_estimate = effect_base - effect_wto\n",
        "\n",
        "    # --- 5. Calculate Standard Error of the Difference via Delta Method ---\n",
        "    # The function of interest is g_diff = g_base(beta) - g_wto(beta).\n",
        "    # The gradient of this difference is grad(g_base) - grad(g_wto).\n",
        "\n",
        "    # Equation for gradient: grad(g(beta)) = 100 * exp(c' * beta * std_dev) * std_dev * c\n",
        "    # Calculate the gradient vector for the baseline effect.\n",
        "    grad_base = 100 * np.exp(lin_eff_base * std_dev) * std_dev * c_base.values\n",
        "    # Calculate the gradient vector for the WTO effect.\n",
        "    grad_wto = 100 * np.exp(lin_eff_wto * std_dev) * std_dev * c_wto.values\n",
        "\n",
        "    # The gradient of the difference is the difference of the gradients.\n",
        "    grad_diff = grad_base - grad_wto\n",
        "\n",
        "    # Equation for variance: Var(g_diff) = grad_diff' * VCV(beta) * grad_diff\n",
        "    # Calculate the variance of the attenuation effect using the full VCV matrix.\n",
        "    attenuation_variance = grad_diff.T @ vcov.values @ grad_diff\n",
        "\n",
        "    # The standard error is the square root of the variance.\n",
        "    attenuation_se = np.sqrt(attenuation_variance)\n",
        "\n",
        "    # Return the final, fully calculated results.\n",
        "    return {\n",
        "        'Attenuation_Effect_Pct': attenuation_estimate,\n",
        "        'Attenuation_StdError': attenuation_se\n",
        "    }\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14, Step 2 & 3: Hypothesis Testing Function\n",
        "# ==============================================================================\n",
        "\n",
        "def test_institutional_effect_equality(\n",
        "    model_results: Fixest,\n",
        "    coef1_name: str,\n",
        "    coef2_name: str\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Tests the equality of two institutional interaction coefficients using a Wald test.\n",
        "\n",
        "    This function performs a formal statistical test of the null hypothesis that\n",
        "    two coefficients are equal (e.g., H0: beta_1 - beta_2 = 0). It constructs\n",
        "    the appropriate restriction vector and computes the Wald statistic, which\n",
        "    is compared against a Chi-squared distribution to obtain a p-value.\n",
        "\n",
        "    Args:\n",
        "        model_results (Fixest): The fitted model object from pyfixest.\n",
        "        coef1_name (str): The name of the first coefficient in the hypothesis.\n",
        "        coef2_name (str): The name of the second coefficient to be subtracted.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A dictionary containing the Wald statistic and its p-value.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    if not isinstance(model_results, Fixest):\n",
        "        raise TypeError(\"Input 'model_results' must be a pyfixest Fixest object.\")\n",
        "    if not all(c in model_results.coef().index for c in [coef1_name, coef2_name]):\n",
        "        raise ValueError(\"One or both specified coefficients not found in model results.\")\n",
        "\n",
        "    # --- 2. Extract Model Components ---\n",
        "    # Get the coefficient estimates and the variance-covariance matrix.\n",
        "    beta = model_results.coef()\n",
        "    vcov = model_results.vcov()\n",
        "\n",
        "    # --- 3. Construct Restriction Vector (R) ---\n",
        "    # The vector R defines the linear combination for the test: R * beta = beta_1 - beta_2.\n",
        "    R = pd.Series(0.0, index=beta.index)\n",
        "    R[coef1_name] = 1.0\n",
        "    R[coef2_name] = -1.0\n",
        "\n",
        "    # --- 4. Calculate Wald Statistic ---\n",
        "    # Equation: W = (R*beta)' * [R * VCV(beta) * R']^-1 * (R*beta)\n",
        "    # For a single restriction (1xK vector R), this simplifies.\n",
        "    r_beta = R.T @ beta\n",
        "    r_vcov_r = R.T @ vcov @ R\n",
        "\n",
        "    # Ensure the denominator is not zero to avoid division errors.\n",
        "    if r_vcov_r <= 1e-12:\n",
        "        return {'wald_statistic': np.nan, 'p_value': np.nan}\n",
        "\n",
        "    wald_stat = (r_beta**2) / r_vcov_r\n",
        "\n",
        "    # --- 5. Calculate p-value ---\n",
        "    # The Wald statistic for a single restriction follows a Chi-squared\n",
        "    # distribution with 1 degree of freedom under the null hypothesis.\n",
        "    p_value = 1.0 - chi2.cdf(wald_stat, df=1)\n",
        "\n",
        "    return {'wald_statistic': wald_stat, 'p_value': p_value}\n",
        "\n",
        "\n",
        "def test_joint_governance_effects(\n",
        "    model_results: Fixest,\n",
        "    governance_interaction_terms: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs an F-test for the joint significance of governance interaction terms.\n",
        "\n",
        "    This function tests the null hypothesis that all specified governance-related\n",
        "    interaction coefficients are jointly equal to zero. It leverages the\n",
        "    built-in `.f_test()` method of the pyfixest results object for a robust\n",
        "    and convenient implementation that correctly handles the model's degrees of freedom.\n",
        "\n",
        "    Args:\n",
        "        model_results (Fixest): The fitted model object from pyfixest.\n",
        "        governance_interaction_terms (List[str]): A list of the governance\n",
        "            interaction coefficients to be jointly tested against zero.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the F-test results, including\n",
        "                      the F-statistic, p-value, and degrees of freedom, as\n",
        "                      returned by pyfixest.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    if not isinstance(model_results, Fixest):\n",
        "        raise TypeError(\"Input 'model_results' must be a pyfixest Fixest object.\")\n",
        "    if not all(term in model_results.coef().index for term in governance_interaction_terms):\n",
        "        missing = set(governance_interaction_terms) - set(model_results.coef().index)\n",
        "        raise ValueError(f\"Governance terms not found in model coefficients: {missing}\")\n",
        "\n",
        "    # --- 2. Perform F-test ---\n",
        "    # The pyfixest `.f_test()` method is the most direct and robust way to\n",
        "    # perform this joint hypothesis test. It correctly uses the model's\n",
        "    # degrees of freedom and VCV matrix.\n",
        "    f_test_results = model_results.f_test(governance_interaction_terms)\n",
        "\n",
        "    return f_test_results\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_institutional_quantification_pipeline(\n",
        "    model_results: Fixest,\n",
        "    std_dev: float,\n",
        "    spec: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the quantification and testing of institutional effects.\n",
        "\n",
        "    This master function executes the three core steps for Task 14:\n",
        "    1. Quantifies the WTO attenuation effect and its standard error via Delta Method.\n",
        "    2. Tests for equality between the WTO and RTA moderation effects via Wald test.\n",
        "    3. Tests for the joint significance of governance channel effects via F-test.\n",
        "\n",
        "    Args:\n",
        "        model_results (Fixest): The fitted PPML or Logit model object.\n",
        "        std_dev (float): The standard deviation of the relevant political distance variable.\n",
        "        spec (Dict[str, Any]): The model specification dictionary, containing\n",
        "                               lists of variable names for testing.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the results of all quantification\n",
        "                        and hypothesis testing steps.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Quantify the WTO attenuation effect.\n",
        "        print(\"Step 1: Quantifying WTO attenuation effect...\")\n",
        "        wto_attenuation = quantify_wto_attenuation_effect(\n",
        "            model_results,\n",
        "            std_dev,\n",
        "            main_pd_term=spec['main_pd_term'],\n",
        "            wto_interaction_term=spec['wto_interaction_term']\n",
        "        )\n",
        "        print(\"Step 1: Success. WTO attenuation quantified.\")\n",
        "\n",
        "        # Step 2: Test equality of WTO and RTA effects.\n",
        "        print(\"Step 2: Testing equality of WTO vs RTA moderation effects...\")\n",
        "        equality_test = test_institutional_effect_equality(\n",
        "            model_results,\n",
        "            coef1_name=spec['wto_interaction_term'],\n",
        "            coef2_name=spec['rta_interaction_term']\n",
        "        )\n",
        "        print(\"Step 2: Success. Equality test complete.\")\n",
        "\n",
        "        # Step 3: Test joint significance of governance effects.\n",
        "        print(\"Step 3: Testing joint significance of governance channels...\")\n",
        "        governance_test = test_joint_governance_effects(\n",
        "            model_results,\n",
        "            governance_interaction_terms=spec['governance_interaction_terms']\n",
        "        )\n",
        "        print(\"Step 3: Success. Joint governance test complete.\")\n",
        "\n",
        "        # --- Compile and Return All Results ---\n",
        "        quantification_report = {\n",
        "            'wto_attenuation': wto_attenuation,\n",
        "            'wto_vs_rta_equality_test': equality_test,\n",
        "            'joint_governance_test': governance_test\n",
        "        }\n",
        "\n",
        "        print(\"\\nSUCCESS: Institutional effect quantification pipeline complete.\")\n",
        "        return quantification_report\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Institutional quantification pipeline failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "R9rpHM0ayN3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Temporal Heterogeneity Analysis\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15, Step 1 & 2: Period-Specific Effect Calculation\n",
        "# ==============================================================================\n",
        "\n",
        "def calculate_period_specific_effects(\n",
        "    temporal_subsamples: Dict[str, pd.DataFrame],\n",
        "    pd_variable: str,\n",
        "    spec: Dict[str, Any],\n",
        "    interaction_scenarios: Dict[str, List[str]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Estimates the model and calculates marginal effects for each temporal subsample.\n",
        "\n",
        "    This function iterates through a dictionary of pre-defined time periods,\n",
        "    re-runs the entire PPML estimation pipeline on each subsample, and then\n",
        "    calculates the period-specific marginal effects and their confidence intervals.\n",
        "\n",
        "    Args:\n",
        "        temporal_subsamples (Dict[str, pd.DataFrame]): A dictionary of DataFrames,\n",
        "            where each key is a period label and each value is the data for that period.\n",
        "        pd_variable (str): The name of the political distance variable.\n",
        "        spec (Dict[str, Any]): The model specification dictionary for estimation.\n",
        "        interaction_scenarios (Dict[str, List[str]]): The scenarios for which\n",
        "            to calculate marginal effects.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with a MultiIndex (Period, Scenario) summarizing\n",
        "                      the marginal effect, standard error, and 95% confidence\n",
        "                      interval for each scenario in each time period.\n",
        "    \"\"\"\n",
        "    # --- 1. Initialize Storage ---\n",
        "    all_period_effects = []\n",
        "\n",
        "    # --- 2. Iterate Through Each Time Period ---\n",
        "    for period_label, subsample_df in temporal_subsamples.items():\n",
        "        print(f\"--- Analyzing Period: {period_label} ---\")\n",
        "\n",
        "        # --- 3. Re-estimate Model on Subsample ---\n",
        "        # Run the full estimation pipeline on the data for this period only.\n",
        "        period_model_results = run_ppml_estimation_pipeline(subsample_df, spec)\n",
        "\n",
        "        # --- 4. Calculate Period-Specific Standard Deviation ---\n",
        "        # The variance of the regressor can change over time, so it must be\n",
        "        # recalculated for each subsample for accurate marginal effects.\n",
        "        period_std_devs = calculate_pd_standard_deviations(subsample_df, [pd_variable])\n",
        "        period_std_dev = period_std_devs[pd_variable]\n",
        "\n",
        "        # --- 5. Calculate Period-Specific Marginal Effects ---\n",
        "        # Use the function from Task 13 with the period-specific results.\n",
        "        period_effects = compute_ppml_marginal_effects(\n",
        "            period_model_results,\n",
        "            period_std_dev,\n",
        "            spec['main_pd_term'],\n",
        "            interaction_scenarios\n",
        "        )\n",
        "\n",
        "        # --- 6. Calculate Confidence Intervals ---\n",
        "        # Calculate the lower and upper bounds of the 95% CI.\n",
        "        period_effects['CI_Lower'] = period_effects['EffectPct'] - 1.96 * period_effects['StdError']\n",
        "        period_effects['CI_Upper'] = period_effects['EffectPct'] + 1.96 * period_effects['StdError']\n",
        "\n",
        "        # Add the period label for aggregation.\n",
        "        period_effects['Period'] = period_label\n",
        "        all_period_effects.append(period_effects)\n",
        "\n",
        "    # --- 7. Aggregate and Format Results ---\n",
        "    # Concatenate all results into a single DataFrame.\n",
        "    final_results_df = pd.concat(all_period_effects)\n",
        "    # Set a descriptive MultiIndex.\n",
        "    final_results_df = final_results_df.reset_index().rename(\n",
        "        columns={'index': 'Scenario'}\n",
        "    ).set_index(['Period', 'Scenario'])\n",
        "\n",
        "    return final_results_df\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15, Step 3: Test for Structural Break in Institutional Effect\n",
        "# ==============================================================================\n",
        "\n",
        "def test_temporal_stability_of_wto_effect(\n",
        "    df: pd.DataFrame,\n",
        "    spec: Dict[str, Any],\n",
        "    break_year: int\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Tests for a structural break in the WTO attenuation effect using a pooled\n",
        "    regression with a triple-interaction term.\n",
        "\n",
        "    This is the statistically rigorous method for testing if the moderating\n",
        "    effect of the WTO on political distance changed significantly after a\n",
        "    certain point in time.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The full analytical DataFrame.\n",
        "        spec (Dict[str, Any]): The base model specification.\n",
        "        break_year (int): The year that defines the start of the \"post\" period.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: The coefficient, standard error, and p-value for the\n",
        "                   triple-interaction term, which represents the change in the\n",
        "                   WTO effect.\n",
        "    \"\"\"\n",
        "    # --- 1. Prepare Data for Pooled Regression ---\n",
        "    df_pooled = df.copy()\n",
        "    # Create a dummy variable for the post-break period.\n",
        "    post_period_dummy = f\"post_{break_year}\"\n",
        "    df_pooled[post_period_dummy] = (df_pooled['Year'] >= break_year).astype(int)\n",
        "\n",
        "    # --- 2. Construct the Triple-Interaction Term Name ---\n",
        "    # This term captures the change in the WTO interaction effect in the post period.\n",
        "    wto_interaction_term = spec['wto_interaction_term']\n",
        "    triple_interaction_term = f\"{post_period_dummy}:{wto_interaction_term}\"\n",
        "\n",
        "    # --- 3. Create the Pooled Model Specification ---\n",
        "    # Start with the base specification.\n",
        "    pooled_spec = spec.copy()\n",
        "    # Add the post-period dummy, the original interaction interacted with the\n",
        "    # dummy, and the new triple-interaction term to the list of regressors.\n",
        "    # Note: pyfixest handles the creation of the main effects automatically.\n",
        "    pooled_spec['interaction_regressors'] = spec['interaction_regressors'] + [\n",
        "        f\"{post_period_dummy}:{spec['main_pd_term']}\",\n",
        "        triple_interaction_term\n",
        "    ]\n",
        "    # Also add the post-period dummy as a main effect.\n",
        "    pooled_spec['main_regressors'] = spec['main_regressors'] + [post_period_dummy]\n",
        "\n",
        "    # --- 4. Estimate the Pooled Model ---\n",
        "    # Run the full PPML pipeline with the new, expanded specification.\n",
        "    pooled_model_results = run_ppml_estimation_pipeline(df_pooled, pooled_spec)\n",
        "\n",
        "    # --- 5. Extract and Return the Key Coefficient ---\n",
        "    # The coefficient on the triple-interaction term is the result of interest.\n",
        "    # It directly measures the change in the WTO's effect.\n",
        "    summary = pooled_model_results.summary()\n",
        "    structural_break_result = summary.loc[triple_interaction_term]\n",
        "\n",
        "    return structural_break_result\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_temporal_analysis_pipeline(\n",
        "    temporal_subsamples: Dict[str, pd.DataFrame],\n",
        "    full_df: pd.DataFrame,\n",
        "    spec: Dict[str, Any],\n",
        "    interaction_scenarios: Dict[str, List[str]],\n",
        "    break_year: int = 2009\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire temporal heterogeneity analysis.\n",
        "\n",
        "    This master function performs two types of temporal analysis:\n",
        "    1. It estimates the model separately on each sub-period to calculate\n",
        "       period-specific marginal effects.\n",
        "    2. It runs a formal statistical test for a structural break in the key\n",
        "       institutional effect using a pooled regression model.\n",
        "\n",
        "    Args:\n",
        "        temporal_subsamples (Dict[str, pd.DataFrame]): Dictionary of subsample DataFrames.\n",
        "        full_df (pd.DataFrame): The complete analytical DataFrame for the pooled model.\n",
        "        spec (Dict[str, Any]): The model specification dictionary.\n",
        "        interaction_scenarios (Dict[str, List[str]]): Scenarios for marginal effects.\n",
        "        break_year (int): The year to test for a structural break.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing:\n",
        "            - 'period_specific_effects': A DataFrame of effects for each period.\n",
        "            - 'structural_break_test': The results of the formal test for a\n",
        "              change in the WTO effect.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1 & 2: Calculate period-specific effects and CIs.\n",
        "        print(\"Step 1: Calculating period-specific marginal effects...\")\n",
        "        # Assume the main PD variable is the first in the interaction list.\n",
        "        pd_var = spec['interaction_regressors'][0]\n",
        "        period_effects = calculate_period_specific_effects(\n",
        "            temporal_subsamples, pd_var, spec, interaction_scenarios\n",
        "        )\n",
        "        print(\"Step 1: Success. Period-specific effects calculated.\")\n",
        "\n",
        "        # Step 3: Test for a structural break.\n",
        "        print(\"Step 2: Testing for structural break in WTO effect...\")\n",
        "        break_test_results = test_temporal_stability_of_wto_effect(\n",
        "            full_df, spec, break_year\n",
        "        )\n",
        "        print(\"Step 2: Success. Structural break test complete.\")\n",
        "\n",
        "        # --- Compile and Return All Results ---\n",
        "        temporal_report = {\n",
        "            'period_specific_effects': period_effects,\n",
        "            'structural_break_test': break_test_results\n",
        "        }\n",
        "\n",
        "        print(\"\\nSUCCESS: Temporal heterogeneity analysis complete.\")\n",
        "        return temporal_report\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Temporal analysis pipeline failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "-aJ1M1rm04Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Create Master Robustness Orchestrator Function\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Worker Functions for Specific Robustness Checks\n",
        "# ==============================================================================\n",
        "\n",
        "def _apply_row_filter(df: pd.DataFrame, query: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies a filter to a DataFrame's rows using a pandas query string.\n",
        "\n",
        "    This is a simple, single-purpose worker function designed to be called by\n",
        "    the robustness orchestrator. It takes a DataFrame and a query string,\n",
        "    applies the filter, and returns a new DataFrame containing only the rows\n",
        "    that satisfy the query condition.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame to be filtered.\n",
        "        query (str): A valid pandas query string (e.g., \"Country == 'USA'\").\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame containing the filtered rows. A copy is\n",
        "                      returned to prevent unintended side effects on the\n",
        "                      original DataFrame.\n",
        "    \"\"\"\n",
        "    # Use the pandas .query() method, which is highly efficient and readable\n",
        "    # for complex boolean filtering operations.\n",
        "    # A .copy() is made to ensure the returned DataFrame is independent of the\n",
        "    # original, preventing slice-warning issues in downstream modifications.\n",
        "    return df.query(query).copy()\n",
        "\n",
        "\n",
        "def _apply_spec_change(\n",
        "    spec: Dict[str, Any],\n",
        "    var_map: Dict[str, str]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Modifies a model specification dictionary by replacing variable names.\n",
        "\n",
        "    This worker function is designed to be called by the robustness orchestrator\n",
        "    to facilitate robustness checks that involve using alternative variables\n",
        "    (e.g., swapping one measure of political distance for another). It iterates\n",
        "    through the lists of regressors in the specification and replaces any\n",
        "    variable found in the `var_map` key with its corresponding value.\n",
        "\n",
        "    Args:\n",
        "        spec (Dict[str, Any]): The original model specification dictionary.\n",
        "        var_map (Dict[str, str]): A dictionary mapping old variable names (keys)\n",
        "                                  to new variable names (values).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A new, deep-copied specification dictionary with the\n",
        "                        variable names updated according to the var_map.\n",
        "    \"\"\"\n",
        "    # Create a deep copy of the specification to ensure the original is not\n",
        "    # mutated, allowing it to be reused for other robustness checks.\n",
        "    spec_new = deepcopy(spec)\n",
        "\n",
        "    # Iterate through the keys in the specification that contain lists of regressors.\n",
        "    for key in ['main_regressors', 'interaction_regressors']:\n",
        "        # Use a list comprehension to build the new list of regressors.\n",
        "        # For each variable, `var_map.get(var, var)` will return the new name\n",
        "        # if the variable is in the map, otherwise it returns the original name.\n",
        "        spec_new[key] = [var_map.get(var, var) for var in spec_new[key]]\n",
        "\n",
        "    # It is also crucial to update the 'main_pd_term' if it is one of the\n",
        "    # variables being swapped, as this is used in downstream effect calculations.\n",
        "    spec_new['main_pd_term'] = var_map.get(\n",
        "        spec_new['main_pd_term'],\n",
        "        spec_new['main_pd_term']\n",
        "    )\n",
        "\n",
        "    # Return the modified specification dictionary.\n",
        "    return spec_new\n",
        "\n",
        "\n",
        "# A registry (dictionary) that maps the string identifiers for robustness check\n",
        "# types to the actual worker functions that implement them. This is the core of\n",
        "# the dispatcher pattern, allowing the orchestrator to be easily extended with\n",
        "# new types of robustness checks without modifying its core logic.\n",
        "ROBUSTNESS_WORKERS: Dict[str, Callable] = {\n",
        "    # Maps the 'filter_rows' check type to its corresponding worker function.\n",
        "    'filter_rows': _apply_row_filter,\n",
        "    # Maps the 'change_spec' check type to its corresponding worker function.\n",
        "    'change_spec': _apply_spec_change,\n",
        "}\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def _run_single_robustness_check(\n",
        "    check_name: str,\n",
        "    check_config: Dict[str, Any],\n",
        "    base_df: pd.DataFrame,\n",
        "    base_spec: Dict[str, Any]\n",
        ") -> Tuple[str, Any]:\n",
        "    \"\"\"\n",
        "    Worker function to execute a single, complete robustness check.\n",
        "\n",
        "    This function is designed to be called in parallel. It takes the configuration\n",
        "    for one check, applies the necessary modifications to the data or specification,\n",
        "    runs the estimation pipeline, and returns the results.\n",
        "\n",
        "    Args:\n",
        "        check_name (str): The name of the robustness check.\n",
        "        check_config (Dict[str, Any]): The configuration for this specific check.\n",
        "        base_df (pd.DataFrame): The baseline analytical DataFrame.\n",
        "        base_spec (Dict[str, Any]): The baseline model specification.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[str, Any]: A tuple of the check name and the resulting model object.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting robustness check: {check_name} ---\")\n",
        "\n",
        "    # --- 1. Apply the specified modification ---\n",
        "    check_type = check_config['type']\n",
        "    check_params = check_config['params']\n",
        "\n",
        "    # Start with copies of the baseline data and spec.\n",
        "    modified_df = base_df\n",
        "    modified_spec = base_spec\n",
        "\n",
        "    # Dispatch to the correct worker function based on the check type.\n",
        "    if check_type == 'filter_rows':\n",
        "        modified_df = _apply_row_filter(base_df, **check_params)\n",
        "    elif check_type == 'change_spec':\n",
        "        modified_spec = _apply_spec_change(base_spec, **check_params)\n",
        "    else:\n",
        "        # If a new check type is added, it must have a worker in the registry.\n",
        "        raise ValueError(f\"Unknown robustness check type: {check_type}\")\n",
        "\n",
        "    # --- 2. Re-run the estimation pipeline ---\n",
        "    try:\n",
        "        # Use the (potentially modified) data and spec to run the estimation.\n",
        "        model_results = run_ppml_estimation_pipeline(modified_df, modified_spec)\n",
        "        print(f\"--- Successfully completed robustness check: {check_name} ---\")\n",
        "        return (check_name, model_results)\n",
        "    except Exception as e:\n",
        "        # If a specific check fails (e.g., due to convergence on a smaller\n",
        "        # sample), report it and return None for the result.\n",
        "        print(f\"--- FAILED robustness check: {check_name}. Reason: {e} ---\")\n",
        "        return (check_name, None)\n",
        "\n",
        "\n",
        "def run_robustness_checks_pipeline(\n",
        "    base_df: pd.DataFrame,\n",
        "    base_spec: Dict[str, Any],\n",
        "    robustness_config: Dict[str, Dict[str, Any]],\n",
        "    n_jobs: int = -1\n",
        ") -> Dict[str, Fixest]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of a suite of robustness checks in parallel.\n",
        "\n",
        "    This master function manages the entire robustness analysis phase. It takes a\n",
        "    baseline DataFrame and model specification, along with a configuration\n",
        "    dictionary defining all robustness checks. It then dispatches each check\n",
        "    to a worker function that runs in parallel, collecting and returning all results.\n",
        "\n",
        "    This dispatcher pattern is highly modular and extensible. To add a new type\n",
        "    of robustness check, one only needs to write a small worker function and\n",
        "    add it to the `ROBUSTNESS_WORKERS` registry.\n",
        "\n",
        "    Args:\n",
        "        base_df (pd.DataFrame): The baseline, fully prepared analytical DataFrame.\n",
        "        base_spec (Dict[str, Any]): The baseline model specification dictionary.\n",
        "        robustness_config (Dict[str, Dict[str, Any]]): A dictionary defining all\n",
        "            robustness checks to be performed.\n",
        "        n_jobs (int): The number of CPU cores to use for parallel execution.\n",
        "                      -1 means use all available cores.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Fixest]: A dictionary where keys are the names of the\n",
        "                           robustness checks and values are the fitted `Fixest`\n",
        "                           model objects. Failed checks will have a value of None.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    if not isinstance(robustness_config, dict):\n",
        "        raise TypeError(\"Robustness configuration must be a dictionary.\")\n",
        "\n",
        "    # --- 2. Parallel Execution of Robustness Checks ---\n",
        "    # Use joblib.Parallel for efficient, cross-platform parallel processing.\n",
        "    with Parallel(n_jobs=n_jobs) as parallel:\n",
        "        # The `delayed` function wraps the worker function and its arguments.\n",
        "        # joblib handles distributing these tasks to the worker pool.\n",
        "        results_list = parallel(\n",
        "            delayed(_run_single_robustness_check)(\n",
        "                check_name, check_config, base_df, base_spec\n",
        "            ) for check_name, check_config in robustness_config.items()\n",
        "        )\n",
        "\n",
        "    # --- 3. Aggregate and Return Results ---\n",
        "    # Convert the list of (name, result) tuples back into a dictionary.\n",
        "    final_results = {name: result for name, result in results_list}\n",
        "\n",
        "    print(\"\\nSUCCESS: All robustness checks have been executed.\")\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "KBuPrBdB3xKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Sample Restriction Robustness Tests\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: New, Specialized Worker Functions for the Robustness Framework\n",
        "# ==============================================================================\n",
        "\n",
        "def _apply_quantile_trim(\n",
        "    df: pd.DataFrame,\n",
        "    column: str,\n",
        "    lower_quantile: float,\n",
        "    upper_quantile: float\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters a DataFrame by trimming the tails of a specified column's distribution.\n",
        "\n",
        "    This specialized worker function is for robustness checks that involve\n",
        "    removing outliers based on quantiles (e.g., trimming the top and bottom 1%\n",
        "    of the political distance distribution).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        column (str): The name of the column to trim.\n",
        "        lower_quantile (float): The lower quantile boundary (e.g., 0.01 for 1%).\n",
        "        upper_quantile (float): The upper quantile boundary (e.g., 0.99 for 99%).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The filtered DataFrame with the outliers removed.\n",
        "    \"\"\"\n",
        "    # Calculate the lower and upper bounds from the data itself.\n",
        "    lower_bound = df[column].quantile(lower_quantile)\n",
        "    upper_bound = df[column].quantile(upper_quantile)\n",
        "\n",
        "    # Apply the filter using the calculated bounds.\n",
        "    return df[df[column].between(lower_bound, upper_bound)].copy()\n",
        "\n",
        "\n",
        "def _apply_event_density_filter(\n",
        "    df: pd.DataFrame,\n",
        "    min_obs_threshold: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters dyadic pairs based on a minimum threshold of non-zero event observations.\n",
        "\n",
        "    This worker function re-uses the logic from Task 4 to allow for robustness\n",
        "    checks with alternative event density thresholds.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        min_obs_threshold (int): The minimum number of months with non-zero\n",
        "                                 event counts for a dyad to be retained.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The filtered DataFrame.\n",
        "    \"\"\"\n",
        "    # This logic is identical to the function `filter_dyads_by_event_density`\n",
        "    # from Task 4, refactored here as a worker.\n",
        "    has_events = df['GDELT_EventCount'] > 0\n",
        "    event_density = has_events.groupby(df['DyadicPair']).transform('sum')\n",
        "    return df[event_density >= min_obs_threshold].copy()\n",
        "\n",
        "\n",
        "# --- Extend the Worker Registry from Task 16 with the new functions ---\n",
        "# This step is crucial for making the new check types available to the orchestrator.\n",
        "# In a real application, this would be defined in a single, shared location.\n",
        "ROBUSTNESS_WORKERS: Dict[str, Callable] = {\n",
        "    'filter_rows': _apply_row_filter,\n",
        "    'change_spec': _apply_spec_change,\n",
        "    'trim_quantiles': _apply_quantile_trim,\n",
        "    'filter_event_density': _apply_event_density_filter,\n",
        "}\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Configuration Definition Function\n",
        "# ==============================================================================\n",
        "\n",
        "def define_sample_restriction_checks(\n",
        "    pd_var_for_trimming: str,\n",
        "    robustness_min_obs: int\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Defines the configuration dictionary for all sample restriction robustness checks.\n",
        "\n",
        "    This function encapsulates the specification of the robustness checks as a\n",
        "    data structure. This configuration is then passed to the master robustness\n",
        "    orchestrator (from Task 16) to be executed. This approach separates the\n",
        "    *definition* of the checks from their *execution*, making the analysis\n",
        "    highly transparent, modular, and easy to modify.\n",
        "\n",
        "    Args:\n",
        "        pd_var_for_trimming (str): The name of the political distance variable\n",
        "                                   that will be used for the quantile trimming check.\n",
        "        robustness_min_obs (int): The stricter threshold for the high event\n",
        "                                  density check.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: The configuration dictionary for the\n",
        "                                   robustness orchestrator.\n",
        "    \"\"\"\n",
        "    # --- Define the dictionary of robustness checks ---\n",
        "    # Each key is a unique, descriptive name for the check.\n",
        "    # Each value is a dictionary specifying the 'type' of check (which maps\n",
        "    # to a worker function in the registry) and the 'params' for that worker.\n",
        "\n",
        "    robustness_config = {\n",
        "\n",
        "        # --- Geographic and Political Restrictions ---\n",
        "\n",
        "        \"Exclude China\": {\n",
        "            \"type\": \"filter_rows\",\n",
        "            \"params\": {\n",
        "                \"query\": \"ExporterISO != 'CHN' and ImporterISO != 'CHN'\"\n",
        "            }\n",
        "        },\n",
        "\n",
        "        \"Democratic Pairs Only\": {\n",
        "            \"type\": \"filter_rows\",\n",
        "            \"params\": {\n",
        "                \"query\": \"DemocraticPair == 1\"\n",
        "            }\n",
        "        },\n",
        "\n",
        "        \"Non-Conflict Pairs\": {\n",
        "            \"type\": \"filter_rows\",\n",
        "            \"params\": {\n",
        "                # This assumes a 'ConflictPair' dummy (0 or 1) has been\n",
        "                # merged into the DataFrame from an external source like UCDP.\n",
        "                \"query\": \"ConflictPair == 0\"\n",
        "            }\n",
        "        },\n",
        "\n",
        "        # --- Data Quality Restrictions ---\n",
        "\n",
        "        \"Trim Political Distance Outliers\": {\n",
        "            \"type\": \"trim_quantiles\",\n",
        "            \"params\": {\n",
        "                \"column\": pd_var_for_trimming,\n",
        "                \"lower_quantile\": 0.01,\n",
        "                \"upper_quantile\": 0.99\n",
        "            }\n",
        "        },\n",
        "\n",
        "        \"High Event Density Only\": {\n",
        "            \"type\": \"filter_event_density\",\n",
        "            \"params\": {\n",
        "                \"min_obs_threshold\": robustness_min_obs\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return robustness_config\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Master Orchestrator Function (Illustrative Usage)\n",
        "# ==============================================================================\n",
        "\n",
        "def run_sample_restriction_robustness_pipeline(\n",
        "    base_df: pd.DataFrame,\n",
        "    base_spec: Dict[str, Any],\n",
        "    pd_var_for_trimming: str,\n",
        "    robustness_min_obs: int,\n",
        "    n_jobs: int = -1\n",
        ") -> Dict[str, Fixest]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of all sample restriction robustness checks.\n",
        "\n",
        "    This function serves as the entry point for Task 17. It first calls the\n",
        "    configuration function to define all the checks, and then passes this\n",
        "    configuration to the master robustness orchestrator from Task 16 to\n",
        "    execute the analysis in parallel.\n",
        "\n",
        "    Args:\n",
        "        base_df (pd.DataFrame): The baseline, fully prepared analytical DataFrame.\n",
        "        base_spec (Dict[str, Any]): The baseline model specification dictionary.\n",
        "        pd_var_for_trimming (str): The PD variable to use for the trimming check.\n",
        "        robustness_min_obs (int): The stricter threshold for the density check.\n",
        "        n_jobs (int): The number of CPU cores to use for parallel execution.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Fixest]: A dictionary where keys are the names of the\n",
        "                           robustness checks and values are the fitted `Fixest`\n",
        "                           model objects.\n",
        "    \"\"\"\n",
        "    # Step 1: Define the full suite of sample restriction checks.\n",
        "    print(\"Step 1: Defining sample restriction robustness checks...\")\n",
        "    sample_restriction_config = define_sample_restriction_checks(\n",
        "        pd_var_for_trimming,\n",
        "        robustness_min_obs\n",
        "    )\n",
        "    print(f\"Step 1: Success. Defined {len(sample_restriction_config)} checks.\")\n",
        "\n",
        "    # Step 2: Execute all defined checks using the master orchestrator.\n",
        "    # This re-uses the powerful, parallelized framework from Task 16.\n",
        "    print(\"Step 2: Executing all sample restriction checks in parallel...\")\n",
        "    # Note: `run_robustness_checks_pipeline` is the orchestrator from Task 16.\n",
        "    # It is assumed to be available in the same scope.\n",
        "    results = run_robustness_checks_pipeline(\n",
        "        base_df=base_df,\n",
        "        base_spec=base_spec,\n",
        "        robustness_config=sample_restriction_config,\n",
        "        n_jobs=n_jobs\n",
        "    )\n",
        "\n",
        "    print(\"\\nSUCCESS: Sample restriction robustness pipeline complete.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Gn5u2mXlGvTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18: Alternative Specification and Measurement Tests\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: New, Specialized Worker Functions and Systematic Mapper\n",
        "# ==============================================================================\n",
        "\n",
        "def _apply_temporal_aggregation(\n",
        "    df: pd.DataFrame,\n",
        "    freq: str,\n",
        "    agg_rules: Dict[str, str],\n",
        "    gdelt_rule: Dict[str, str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Resamples the DataFrame to a lower frequency and prepares it for estimation.\n",
        "\n",
        "    This worker handles robustness checks involving temporal aggregation (e.g.,\n",
        "    moving from monthly to quarterly data). It applies specified aggregation\n",
        "    rules and correctly handles the GDELT 'first-month' rule from the paper.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with a DatetimeIndex.\n",
        "        freq (str): The target frequency string (e.g., 'Q' for quarterly).\n",
        "        agg_rules (Dict[str, str]): A dictionary mapping column patterns to\n",
        "                                    aggregation functions (e.g., {'_USD': 'sum'}).\n",
        "        gdelt_rule (Dict[str, str]): The specific rule for the GDELT measure.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The resampled and aggregated DataFrame.\n",
        "    \"\"\"\n",
        "    # Group by dyad to resample each time series independently.\n",
        "    grouped = df.groupby('DyadicPair')\n",
        "    resampled_list = []\n",
        "\n",
        "    for name, group in grouped:\n",
        "        # Apply the specific GDELT rule first.\n",
        "        resampled_group = group.resample(freq).agg(gdelt_rule)\n",
        "\n",
        "        # Identify other columns and apply general rules.\n",
        "        for pattern, func in agg_rules.items():\n",
        "            cols_to_agg = [c for c in group.columns if pattern in c and c not in gdelt_rule]\n",
        "            if cols_to_agg:\n",
        "                resampled_group[cols_to_agg] = group[cols_to_agg].resample(freq).agg(func)\n",
        "\n",
        "        # For remaining columns (like dummies), forward-fill is a reasonable choice.\n",
        "        resampled_group.fillna(method='ffill', inplace=True)\n",
        "        resampled_group['DyadicPair'] = name\n",
        "        resampled_list.append(resampled_group)\n",
        "\n",
        "    return pd.concat(resampled_list).reset_index().set_index('DateTime')\n",
        "\n",
        "\n",
        "def _create_systematic_var_map(\n",
        "    regressor_list: List[str],\n",
        "    simple_replace_map: Dict[str, str]\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Systematically creates a comprehensive variable map for specification changes.\n",
        "\n",
        "    This helper function takes a simple one-to-one map (e.g., {'A': 'B'}) and\n",
        "    applies it to a list of complex regressors (including interactions like 'C:A:D')\n",
        "    to generate a full mapping for all affected terms.\n",
        "\n",
        "    Args:\n",
        "        regressor_list (List[str]): The list of all regressor names from the spec.\n",
        "        simple_replace_map (Dict[str, str]): A simple dict of one-to-one replacements.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: A complete map of old term -> new term.\n",
        "    \"\"\"\n",
        "    full_var_map = {}\n",
        "    for var in regressor_list:\n",
        "        new_var = var\n",
        "        # Perform replacements for all specified swaps.\n",
        "        for old, new in simple_replace_map.items():\n",
        "            # Use a robust replacement that respects interaction term boundaries.\n",
        "            # This splits 'A:B', replaces 'A' with 'C' -> ['C', 'B'], then rejoins.\n",
        "            components = new_var.split(':')\n",
        "            new_components = [new if comp == old else comp for comp in components]\n",
        "            new_var = ':'.join(new_components)\n",
        "\n",
        "        # If a change was made, add it to the map.\n",
        "        if new_var != var:\n",
        "            full_var_map[var] = new_var\n",
        "\n",
        "    return full_var_map\n",
        "\n",
        "\n",
        "# --- Extend the Worker Registry from Task 16/17 with the new function ---\n",
        "ROBUSTNESS_WORKERS: Dict[str, Callable] = {\n",
        "    'filter_rows': _apply_row_filter,\n",
        "    'change_spec': _apply_spec_change,\n",
        "    'trim_quantiles': _apply_quantile_trim,\n",
        "    'filter_event_density': _apply_event_density_filter,\n",
        "    'resample_and_prepare': _apply_temporal_aggregation, # New worker\n",
        "}\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Configuration Definition Function\n",
        "# ==============================================================================\n",
        "\n",
        "def define_spec_and_measurement_checks(\n",
        "    base_spec: Dict[str, Any]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Defines the configuration for alternative specification and measurement checks.\n",
        "\n",
        "    This function programmatically generates the configuration dictionary that\n",
        "    will drive the robustness orchestrator. It uses a systematic mapping\n",
        "    function to ensure that all interaction terms are correctly updated when\n",
        "    a core variable is swapped, eliminating potential for manual error.\n",
        "\n",
        "    Args:\n",
        "        base_spec (Dict[str, Any]): The baseline model specification dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: The configuration dictionary for the\n",
        "                                   robustness orchestrator.\n",
        "    \"\"\"\n",
        "    # --- 1. Define the basic variable swaps ---\n",
        "    # Define the swap for Political Distance measures.\n",
        "    pd_swap = {'PD_GDELT_Filtered_IHS': 'PD_UNGA_IHS'}\n",
        "\n",
        "    # Define the swap for Governance control variables.\n",
        "    gov_swap = {\n",
        "        'Corruption_IHS': 'WGI_RuleOfLaw_IHS',\n",
        "        'Polity_IHS': 'WGI_VoiceAndAccountability_IHS'\n",
        "    }\n",
        "\n",
        "    # --- 2. Systematically generate the full variable maps ---\n",
        "    # Get the full list of regressors from the base specification.\n",
        "    all_regressors = base_spec['main_regressors'] + base_spec['interaction_regressors']\n",
        "\n",
        "    # Create the complete map for the PD swap.\n",
        "    pd_var_map = _create_systematic_var_map(all_regressors, pd_swap)\n",
        "    # Also map the main PD term identifier used in downstream tasks.\n",
        "    pd_var_map[base_spec['main_pd_term']] = base_spec['main_pd_term'].replace(\n",
        "        list(pd_swap.keys())[0], list(pd_swap.values())[0]\n",
        "    )\n",
        "\n",
        "    # Create the complete map for the governance swap.\n",
        "    gov_var_map = _create_systematic_var_map(all_regressors, gov_swap)\n",
        "\n",
        "    # --- 3. Define the Configuration Dictionary ---\n",
        "    robustness_config = {\n",
        "        \"Alternative PD Measure (UNGA)\": {\n",
        "            \"type\": \"change_spec\",\n",
        "            \"params\": {\"var_map\": pd_var_map}\n",
        "        },\n",
        "        \"Alternative Governance Controls (WGI)\": {\n",
        "            \"type\": \"change_spec\",\n",
        "            \"params\": {\"var_map\": gov_var_map}\n",
        "        },\n",
        "        # This check now requires a new worker, but the config is clean.\n",
        "        \"Quarterly Temporal Aggregation\": {\n",
        "            \"type\": \"resample_and_prepare\",\n",
        "            \"params\": {\n",
        "                \"freq\": \"Q\",\n",
        "                \"agg_rules\": {\"_USD\": \"sum\", \"Count\": \"sum\", \"_IHS\": \"mean\"},\n",
        "                \"gdelt_rule\": {\"GDELT_GoldsteinMean\": \"first\"}\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return robustness_config\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Master Orchestrator Function (Illustrative Usage)\n",
        "# ==============================================================================\n",
        "\n",
        "def run_spec_and_measurement_robustness_pipeline(\n",
        "    base_df: pd.DataFrame,\n",
        "    base_spec: Dict[str, Any],\n",
        "    n_jobs: int = -1\n",
        ") -> Dict[str, Fixest]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of all specification and measurement robustness checks.\n",
        "\n",
        "    Args:\n",
        "        base_df (pd.DataFrame): The baseline, fully prepared analytical DataFrame.\n",
        "        base_spec (Dict[str, Any]): The baseline model specification dictionary.\n",
        "        n_jobs (int): The number of CPU cores to use for parallel execution.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Fixest]: A dictionary of the fitted `Fixest` model objects\n",
        "                           for each robustness check.\n",
        "    \"\"\"\n",
        "    # Step 1: Define the full suite of specification checks programmatically.\n",
        "    print(\"Step 1: Defining specification and measurement robustness checks...\")\n",
        "    spec_config = define_spec_and_measurement_checks(base_spec)\n",
        "    print(f\"Step 1: Success. Defined {len(spec_config)} checks.\")\n",
        "\n",
        "    # Step 2: Execute all defined checks using the master orchestrator.\n",
        "    print(\"Step 2: Executing all specification checks in parallel...\")\n",
        "    # Note: `run_robustness_checks_pipeline` is the orchestrator from Task 16.\n",
        "    results = run_robustness_checks_pipeline(\n",
        "        base_df=base_df,\n",
        "        base_spec=base_spec,\n",
        "        robustness_config=spec_config,\n",
        "        n_jobs=n_jobs\n",
        "    )\n",
        "\n",
        "    print(\"\\nSUCCESS: Specification and measurement robustness pipeline complete.\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "8-LPhVS7G8dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Robustness Analysis Orchestrator Function\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16-18: Consolidated Robustness Analysis Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def run_all_robustness_analyses(\n",
        "    base_df: pd.DataFrame,\n",
        "    base_spec: Dict[str, Any],\n",
        "    constants: 'ProjectConstants',\n",
        "    n_jobs: int = -1\n",
        ") -> Dict[str, Fixest]:\n",
        "    \"\"\"\n",
        "    Executes the complete suite of robustness analyses for the project.\n",
        "\n",
        "    This high-level orchestrator serves as the single entry point for the entire\n",
        "    robustness analysis phase (Phase 6). It systematically performs the following steps:\n",
        "    1. Calls specialized functions to generate the configurations for both\n",
        "       sample restriction checks (Task 17) and specification/measurement\n",
        "       checks (Task 18).\n",
        "    2. Merges these configurations into a single, comprehensive suite of tests,\n",
        "       validating for non-overlapping check names.\n",
        "    3. Passes this master configuration to the powerful, parallelized robustness\n",
        "       execution engine (`run_robustness_checks_pipeline` from Task 16).\n",
        "\n",
        "    This design is highly modular and extensible, separating the definition of\n",
        "    robustness checks from their execution.\n",
        "\n",
        "    Args:\n",
        "        base_df (pd.DataFrame): The baseline, fully prepared analytical DataFrame.\n",
        "        base_spec (Dict[str, Any]): The baseline model specification dictionary.\n",
        "        constants (ProjectConstants): The dataclass of project constants, needed\n",
        "                                      for defining some checks.\n",
        "        n_jobs (int): The number of CPU cores to use for parallel execution.\n",
        "                      -1 means use all available cores.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Fixest]: A dictionary where keys are the descriptive names of\n",
        "                           each robustness check and values are the fitted `Fixest`\n",
        "                           model objects (or None if a check failed).\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    # Basic type checking for the primary inputs.\n",
        "    if not isinstance(base_df, pd.DataFrame):\n",
        "        raise TypeError(\"base_df must be a pandas DataFrame.\")\n",
        "    if not isinstance(base_spec, dict):\n",
        "        raise TypeError(\"base_spec must be a dictionary.\")\n",
        "\n",
        "    try:\n",
        "        # --- 2. Generate Configurations for Each Type of Check ---\n",
        "        print(\"Step 1: Defining all robustness check configurations...\")\n",
        "\n",
        "        # Generate the configuration for sample restriction checks from Task 17.\n",
        "        sample_restriction_config = define_sample_restriction_checks(\n",
        "            pd_var_for_trimming=base_spec['main_pd_term'],\n",
        "            robustness_min_obs=constants.ROBUSTNESS_MIN_OBS\n",
        "        )\n",
        "\n",
        "        # Generate the configuration for specification & measurement checks from Task 18.\n",
        "        spec_measurement_config = define_spec_and_measurement_checks(base_spec)\n",
        "\n",
        "        print(\n",
        "            f\"Step 1: Success. Defined {len(sample_restriction_config)} sample restriction checks \"\n",
        "            f\"and {len(spec_measurement_config)} specification checks.\"\n",
        "        )\n",
        "\n",
        "        # --- 3. Merge Configurations and Validate ---\n",
        "        # Check for any overlapping keys, which would indicate a naming collision.\n",
        "        common_keys = sample_restriction_config.keys() & spec_measurement_config.keys()\n",
        "        if common_keys:\n",
        "            raise ValueError(f\"Duplicate robustness check names found: {common_keys}\")\n",
        "\n",
        "        # Merge the two dictionaries into a single master configuration.\n",
        "        # The `|` operator is a clean way to merge dicts in Python 3.9+.\n",
        "        master_robustness_config = sample_restriction_config | spec_measurement_config\n",
        "\n",
        "        print(f\"Total robustness checks to execute: {len(master_robustness_config)}\")\n",
        "\n",
        "        # --- 4. Execute All Checks via the Master Orchestrator ---\n",
        "        # Call the powerful, parallelized pipeline from Task 16 to run everything.\n",
        "        all_results = run_robustness_checks_pipeline(\n",
        "            base_df=base_df,\n",
        "            base_spec=base_spec,\n",
        "            robustness_config=master_robustness_config,\n",
        "            n_jobs=n_jobs\n",
        "        )\n",
        "\n",
        "        print(\"\\nSUCCESS: All robustness analyses have been completed.\")\n",
        "        return all_results\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any errors during the process for a clean failure.\n",
        "        print(f\"\\nERROR: The main robustness analysis pipeline failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "fVc9Sc3DmcbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19: Comprehensive Results Table Construction\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19, Step 1 & 2: Main and Margin-Specific Results Tables\n",
        "# ==============================================================================\n",
        "\n",
        "class StoredLogitResults:\n",
        "    \"\"\"\n",
        "    An adapter class to make custom Logit results compatible with modelsummary.\n",
        "\n",
        "    The `modelsummary` library is designed to parse standard statistical model\n",
        "    objects from libraries like `statsmodels` or `pyfixest`. This class serves\n",
        "    as a lightweight wrapper around the custom-calculated Logit results (which\n",
        "    are stored in a dictionary). It exposes the key estimation outputs (coefficients,\n",
        "    standard errors, nobs) as attributes and provides a `pvalues` method,\n",
        "    mimicking the interface that `modelsummary` expects. This allows for the\n",
        "    seamless integration of our bespoke, computationally intensive Logit results\n",
        "    into the same professional-grade tables as the more standard PPML results.\n",
        "\n",
        "    Attributes:\n",
        "        params (pd.Series): A Series of the bias-corrected coefficient estimates.\n",
        "        bse (pd.Series): A Series of the bootstrapped standard errors.\n",
        "        nobs (int): The number of observations used in the estimation.\n",
        "        model_name (str): An optional attribute to set the column name in the table.\n",
        "    \"\"\"\n",
        "    def __init__(self, logit_results: Dict[str, pd.Series], nobs: int):\n",
        "        \"\"\"\n",
        "        Initializes the StoredLogitResults adapter.\n",
        "\n",
        "        Args:\n",
        "            logit_results (Dict[str, pd.Series]): A dictionary containing the\n",
        "                final Logit estimation results, expecting keys 'coefficients'\n",
        "                and 'standard_errors'.\n",
        "            nobs (int): The number of observations used in the Logit estimation.\n",
        "\n",
        "        Raises:\n",
        "            KeyError: If the `logit_results` dictionary is missing required keys.\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        if 'coefficients' not in logit_results or 'standard_errors' not in logit_results:\n",
        "            raise KeyError(\"logit_results dict must contain 'coefficients' and 'standard_errors'.\")\n",
        "\n",
        "        # --- Attribute Assignment ---\n",
        "        # Assign the coefficient Series to the 'params' attribute, which is a\n",
        "        # standard name that modelsummary looks for.\n",
        "        self.params: pd.Series = logit_results['coefficients']\n",
        "\n",
        "        # Assign the standard error Series to the 'bse' (beta standard error)\n",
        "        # attribute, another standard name.\n",
        "        self.bse: pd.Series = logit_results['standard_errors']\n",
        "\n",
        "        # Assign the number of observations.\n",
        "        self.nobs: int = nobs\n",
        "\n",
        "        # Initialize a placeholder for the model name.\n",
        "        self.model_name: str = \"Model\"\n",
        "\n",
        "    def pvalues(self) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Computes and returns the p-values for the coefficient estimates.\n",
        "\n",
        "        This method is explicitly required by `modelsummary` to generate\n",
        "        significance stars. It calculates two-tailed p-values based on a\n",
        "        standard normal (Z) distribution.\n",
        "\n",
        "        Returns:\n",
        "            pd.Series: A Series of p-values, indexed by the coefficient names.\n",
        "        \"\"\"\n",
        "        # Calculate Z-scores by dividing the coefficients by their standard errors.\n",
        "        z_scores = self.params / self.bse\n",
        "\n",
        "        # Calculate the two-tailed p-value from the standard normal CDF.\n",
        "        # p = 2 * (1 - CDF(|Z|))\n",
        "        p_vals = 2 * (1 - norm.cdf(np.abs(z_scores)))\n",
        "\n",
        "        return p_vals\n",
        "\n",
        "\n",
        "def create_main_results_table(\n",
        "    model_list: List[Any],\n",
        "    model_names: List[str],\n",
        "    title: str,\n",
        "    coef_rename_map: Dict[str, str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates a publication-quality regression table using the modelsummary library.\n",
        "\n",
        "    This function acts as a high-level wrapper around `modelsummary`, providing a\n",
        "    standardized way to generate the main results tables for the project. It\n",
        "    takes a list of fitted model objects and formats them into a clean, readable\n",
        "    pandas DataFrame that resembles tables found in academic journals.\n",
        "\n",
        "    Args:\n",
        "        model_list (List[Any]): A list of fitted model objects. These can be\n",
        "                                `pyfixest` objects or instances of the\n",
        "                                `StoredLogitResults` adapter class.\n",
        "        model_names (List[str]): A list of strings to be used as column headers\n",
        "                                 for each model in the table.\n",
        "        title (str): The title to be displayed above the regression table.\n",
        "        coef_rename_map (Dict[str, str]): A dictionary to rename raw coefficient\n",
        "                                          names (keys) to human-readable labels\n",
        "                                          (values) for presentation.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A pandas DataFrame containing the formatted regression table.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the number of models does not match the number of model names.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    # Ensure that each model provided will have a corresponding name in the table.\n",
        "    if len(model_list) != len(model_names):\n",
        "        raise ValueError(\"Length of model_list must match length of model_names.\")\n",
        "\n",
        "    # --- 2. Define Table Formatting ---\n",
        "    # Define a list of goodness-of-fit statistics to display at the bottom of the table.\n",
        "    # `modelsummary` allows specifying the raw attribute name, a clean display name,\n",
        "    # and a number format string. It will intelligently skip metrics not\n",
        "    # found on a given model object.\n",
        "    gof_metrics = [\n",
        "        {\"raw\": \"nobs\", \"clean\": \"Observations\", \"fmt\": \"{:,}\"},\n",
        "        {\"raw\": \"rsquared\", \"clean\": \"R2\", \"fmt\": \"{:.3f}\"},\n",
        "        # The custom Logit adapter does not have an R2, but we can add a custom\n",
        "        # attribute to it if we were to calculate a pseudo R2.\n",
        "        # For now, modelsummary will just skip it for the Logit model.\n",
        "    ]\n",
        "\n",
        "    # --- 3. Prepare Models ---\n",
        "    # Set the custom model names for the columns. `modelsummary` looks for a\n",
        "    # `model_name` attribute on the passed objects.\n",
        "    for model, name in zip(model_list, model_names):\n",
        "        # This try-except block makes the function robust to model objects\n",
        "        # that might not have a `model_name` attribute or are immutable.\n",
        "        try:\n",
        "            model.model_name = name\n",
        "        except AttributeError:\n",
        "            warnings.warn(f\"Could not set model_name for model of type {type(model)}.\")\n",
        "\n",
        "    # --- 4. Generate Table with modelsummary ---\n",
        "    # Call the main modelsummary function with the specified configuration.\n",
        "    results_table = modelsummary.modelsummary(\n",
        "        models=model_list,\n",
        "        output='df',  # Specify DataFrame as the output format.\n",
        "        title=title,\n",
        "        stars=True,  # Automatically add significance stars (e.g., *, **, ***).\n",
        "        coef_map=coef_rename_map,\n",
        "        gof_map=gof_metrics,\n",
        "        fmt=\"{:.3f}\"  # Format all numeric estimates to 3 decimal places.\n",
        "    )\n",
        "\n",
        "    # Return the resulting DataFrame.\n",
        "    return results_table\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19, Step 3: Robustness Results Summary Table\n",
        "# ==============================================================================\n",
        "\n",
        "def _format_coef_and_stars(coef: float, p_value: float) -> str:\n",
        "    \"\"\"\n",
        "    A helper function to format a coefficient and its significance stars into a string.\n",
        "\n",
        "    This utility is used for creating custom summary tables (like the robustness\n",
        "    table) where results need to be presented concisely in a single cell.\n",
        "\n",
        "    Args:\n",
        "        coef (float): The coefficient estimate.\n",
        "        p_value (float): The p-value associated with the coefficient.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string, e.g., \"0.123***\".\n",
        "    \"\"\"\n",
        "    # --- Determine Significance Level ---\n",
        "    # Assign the appropriate star notation based on conventional p-value thresholds.\n",
        "    if p_value < 0.01:\n",
        "        stars = '***'\n",
        "    elif p_value < 0.05:\n",
        "        stars = '**'\n",
        "    elif p_value < 0.1:\n",
        "        stars = '*'\n",
        "    else:\n",
        "        stars = ''\n",
        "\n",
        "    # --- Format and Return String ---\n",
        "    # Combine the formatted coefficient (to 3 decimal places) and the stars.\n",
        "    return f\"{coef:.3f}{stars}\"\n",
        "\n",
        "def create_robustness_summary_table(\n",
        "    robustness_results: Dict[str, Fixest],\n",
        "    key_variables: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a summary table comparing key coefficients across robustness checks.\n",
        "\n",
        "    This function distills the results from the entire suite of robustness\n",
        "    checks into a concise table, allowing for a quick assessment of how stable\n",
        "    the key findings are across different model specifications and samples.\n",
        "\n",
        "    Args:\n",
        "        robustness_results (Dict[str, Fixest]): The dictionary of results from\n",
        "            the robustness orchestrator.\n",
        "        key_variables (List[str]): A list of the key coefficient names to display.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame where rows are the key variables and columns\n",
        "                      are the different robustness checks.\n",
        "    \"\"\"\n",
        "    # --- 1. Initialize Storage ---\n",
        "    summary_data = []\n",
        "\n",
        "    # --- 2. Iterate Through Robustness Check Results ---\n",
        "    for check_name, model_fit in robustness_results.items():\n",
        "        # Initialize a dictionary for this column of the table.\n",
        "        result_col = {'Check': check_name}\n",
        "\n",
        "        # If the model failed to run, fill with 'NA'.\n",
        "        if model_fit is None:\n",
        "            for var in key_variables:\n",
        "                result_col[var] = \"NA\"\n",
        "        else:\n",
        "            # Extract the full summary table for this model.\n",
        "            summary = model_fit.summary()\n",
        "            # For each key variable, extract and format the result.\n",
        "            for var in key_variables:\n",
        "                if var in summary.index:\n",
        "                    coef = summary.loc[var, 'Estimate']\n",
        "                    p_val = summary.loc[var, 'Pr(>|t|)']\n",
        "                    result_col[var] = _format_coef_and_stars(coef, p_val)\n",
        "                else:\n",
        "                    result_col[var] = \"NA\" # Variable not in this model\n",
        "\n",
        "        summary_data.append(result_col)\n",
        "\n",
        "    # --- 3. Format and Return Output ---\n",
        "    # Convert the list of dictionaries to a DataFrame.\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    # Set the check name as the index and transpose for the desired format.\n",
        "    summary_df = summary_df.set_index('Check').T\n",
        "\n",
        "    return summary_df\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def run_results_presentation_pipeline(\n",
        "    ppml_models: Dict[str, Fixest],\n",
        "    logit_models: Dict[str, Dict[str, pd.Series]],\n",
        "    logit_nobs: Dict[str, int],\n",
        "    robustness_results: Dict[str, Fixest],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the creation of all final results tables.\n",
        "\n",
        "    Args:\n",
        "        ppml_models (Dict[str, Fixest]): A dictionary of fitted PPML models.\n",
        "        logit_models (Dict[str, Dict[str, pd.Series]]): A dictionary of Logit results.\n",
        "        logit_nobs (Dict[str, int]): A dictionary of observation counts for Logit models.\n",
        "        robustness_results (Dict[str, Fixest]): The dictionary of all robustness run results.\n",
        "        config (Dict[str, Any]): A configuration dictionary containing specifications\n",
        "                                 for table titles, variable renaming, etc.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary of the generated results tables.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Prepare Models for Main Table ---\n",
        "        print(\"Step 1: Preparing models for main results table...\")\n",
        "        # Combine PPML and Logit models into a single list.\n",
        "        all_models = list(ppml_models.values())\n",
        "        model_names = list(ppml_models.keys())\n",
        "\n",
        "        # Adapt the custom Logit results for modelsummary.\n",
        "        for name, results in logit_models.items():\n",
        "            all_models.append(StoredLogitResults(results, logit_nobs[name]))\n",
        "            model_names.append(name)\n",
        "        print(\"Step 1: Success.\")\n",
        "\n",
        "        # --- 2. Create Main Results Table ---\n",
        "        print(\"Step 2: Creating main results table...\")\n",
        "        main_table = create_main_results_table(\n",
        "            model_list=all_models,\n",
        "            model_names=model_names,\n",
        "            title=config.get('main_table_title', \"Main Regression Results\"),\n",
        "            coef_rename_map=config.get('coef_rename_map', {})\n",
        "        )\n",
        "        print(\"Step 2: Success.\")\n",
        "\n",
        "        # --- 3. Create Robustness Summary Table ---\n",
        "        print(\"Step 3: Creating robustness summary table...\")\n",
        "        robustness_table = create_robustness_summary_table(\n",
        "            robustness_results,\n",
        "            key_variables=config.get('key_robustness_vars', [])\n",
        "        )\n",
        "        print(\"Step 3: Success.\")\n",
        "\n",
        "        # --- 4. Compile and Return ---\n",
        "        tables = {\n",
        "            'main_results': main_table,\n",
        "            'robustness_summary': robustness_table\n",
        "        }\n",
        "\n",
        "        print(\"\\nSUCCESS: All results tables have been generated.\")\n",
        "        return tables\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Results presentation pipeline failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "DDfpWkFkIAS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20: Effect Size Visualization and Temporal Analysis\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20, Step 1: Margin-Specific Effect Size Charts\n",
        "# ==============================================================================\n",
        "\n",
        "def plot_marginal_effects(\n",
        "    marginal_effects_df: pd.DataFrame,\n",
        "    title: str,\n",
        "    x_label: str\n",
        ") -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Creates a horizontal bar chart to visualize marginal effects and their CIs.\n",
        "\n",
        "    This function takes a DataFrame of calculated marginal effects and produces a\n",
        "    professional-quality \"coefficient plot\" that clearly shows the magnitude and\n",
        "    statistical significance of the effects for different scenarios.\n",
        "\n",
        "    Args:\n",
        "        marginal_effects_df (pd.DataFrame): A DataFrame indexed by scenario name,\n",
        "            with columns 'EffectPct' (or 'AME_pp') and 'StdError'.\n",
        "        title (str): The title for the plot.\n",
        "        x_label (str): The label for the x-axis (e.g., '% Change in Trade').\n",
        "\n",
        "    Returns:\n",
        "        matplotlib.figure.Figure: The generated figure object.\n",
        "    \"\"\"\n",
        "    # --- 1. Data Preparation ---\n",
        "    # Make a copy to avoid modifying the original DataFrame.\n",
        "    plot_df = marginal_effects_df.copy()\n",
        "    # The effect column might have different names, so standardize it.\n",
        "    effect_col = [c for c in plot_df.columns if 'Effect' in c or 'AME' in c][0]\n",
        "    plot_df.rename(columns={effect_col: 'Effect'}, inplace=True)\n",
        "\n",
        "    # Calculate the confidence interval bounds.\n",
        "    plot_df['ci_half_width'] = 1.96 * plot_df['StdError']\n",
        "\n",
        "    # --- 2. Plotting ---\n",
        "    # Set a professional plot style.\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Create the horizontal bar plot.\n",
        "    bars = ax.barh(\n",
        "        y=plot_df.index,\n",
        "        width=plot_df['Effect'],\n",
        "        color=sns.color_palette(\"vlag\", n_colors=len(plot_df))[::-1],\n",
        "        alpha=0.8\n",
        "    )\n",
        "\n",
        "    # Add the error bars representing the 95% confidence interval.\n",
        "    ax.errorbar(\n",
        "        x=plot_df['Effect'],\n",
        "        y=plot_df.index,\n",
        "        xerr=plot_df['ci_half_width'],\n",
        "        fmt='none',\n",
        "        color='black',\n",
        "        capsize=5\n",
        "    )\n",
        "\n",
        "    # --- 3. Aesthetics and Labels ---\n",
        "    # Add a vertical line at zero for reference.\n",
        "    ax.axvline(0, color='black', linestyle='--', linewidth=1)\n",
        "    # Set the title and labels.\n",
        "    ax.set_title(title, fontsize=16, pad=20)\n",
        "    ax.set_xlabel(x_label, fontsize=12)\n",
        "    ax.set_ylabel(\"\")\n",
        "    # Invert y-axis to have the first item on top.\n",
        "    ax.invert_yaxis()\n",
        "    # Add data labels to the bars.\n",
        "    ax.bar_label(bars, fmt='%.2f', padding=5)\n",
        "\n",
        "    # Ensure a tight layout.\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20, Step 2: Temporal Heterogeneity Visualization\n",
        "# ==============================================================================\n",
        "\n",
        "def plot_temporal_heterogeneity(\n",
        "    period_effects_df: pd.DataFrame,\n",
        "    title: str\n",
        ") -> sns.FacetGrid:\n",
        "    \"\"\"\n",
        "    Creates a point plot to visualize the evolution of effects over time.\n",
        "\n",
        "    This function uses seaborn's `catplot` to create a faceted point plot,\n",
        "    which is ideal for showing how an estimated effect (and its confidence\n",
        "    interval) changes across different discrete time periods.\n",
        "\n",
        "    Args:\n",
        "        period_effects_df (pd.DataFrame): The DataFrame of period-specific\n",
        "            effects from Task 15, with a MultiIndex of (Period, Scenario).\n",
        "        title (str): The main title for the entire plot grid.\n",
        "\n",
        "    Returns:\n",
        "        seaborn.FacetGrid: The generated FacetGrid object, which can be further\n",
        "                           customized or saved.\n",
        "    \"\"\"\n",
        "    # --- 1. Data Preparation ---\n",
        "    # Reset the index to make 'Period' and 'Scenario' regular columns for plotting.\n",
        "    plot_df = period_effects_df.reset_index()\n",
        "\n",
        "    # --- 2. Plotting with Seaborn FacetGrid ---\n",
        "    # `catplot` is a high-level interface for drawing categorical plots onto a\n",
        "    # FacetGrid. `kind='point'` creates the point plot with CI error bars.\n",
        "    # `col='Scenario'` creates separate subplots for each scenario.\n",
        "    g = sns.catplot(\n",
        "        data=plot_df,\n",
        "        x='Period',\n",
        "        y='EffectPct',\n",
        "        kind='point',\n",
        "        col='Scenario',\n",
        "        capsize=0.2,\n",
        "        height=5,\n",
        "        aspect=1.2,\n",
        "        sharey=True,\n",
        "        palette='viridis'\n",
        "    )\n",
        "\n",
        "    # --- 3. Aesthetics and Labels ---\n",
        "    # Set the main title for the figure.\n",
        "    g.fig.suptitle(title, y=1.03, fontsize=16)\n",
        "    # Set axis labels.\n",
        "    g.set_axis_labels(\"Time Period\", \"Marginal Effect (%)\")\n",
        "    # Add a horizontal line at zero on each subplot for reference.\n",
        "    g.map(plt.axhline, y=0, color='black', linestyle='--', linewidth=1)\n",
        "\n",
        "    return g\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20, Step 3: Robustness Test Visualization\n",
        "# ==============================================================================\n",
        "\n",
        "def plot_robustness_check_stability(\n",
        "    baseline_model: Fixest,\n",
        "    robustness_results: Dict[str, Fixest],\n",
        "    variable_to_plot: str,\n",
        "    title: str\n",
        ") -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Creates a coefficient stability plot to visualize robustness check results.\n",
        "\n",
        "    This plot shows the point estimate and 95% confidence interval for a key\n",
        "    coefficient from the baseline model and compares it to the estimates from\n",
        "    a series of robustness check models.\n",
        "\n",
        "    Args:\n",
        "        baseline_model (Fixest): The fitted baseline model object.\n",
        "        robustness_results (Dict[str, Fixest]): The dictionary of results from\n",
        "            the robustness orchestrator.\n",
        "        variable_to_plot (str): The name of the key coefficient to visualize.\n",
        "        title (str): The title for the plot.\n",
        "\n",
        "    Returns:\n",
        "        matplotlib.figure.Figure: The generated figure object.\n",
        "    \"\"\"\n",
        "    # --- 1. Extract Data for Plotting ---\n",
        "    plot_data = []\n",
        "    # First, get the baseline result.\n",
        "    summary_base = baseline_model.summary()\n",
        "    base_estimate = summary_base.loc[variable_to_plot, 'Estimate']\n",
        "    base_se = summary_base.loc[variable_to_plot, 'Std. Error']\n",
        "\n",
        "    # Iterate through the robustness check results.\n",
        "    for check_name, model_fit in robustness_results.items():\n",
        "        if model_fit is not None and variable_to_plot in model_fit.coef().index:\n",
        "            summary = model_fit.summary()\n",
        "            estimate = summary.loc[variable_to_plot, 'Estimate']\n",
        "            se = summary.loc[variable_to_plot, 'Std. Error']\n",
        "            plot_data.append({\n",
        "                'Check': check_name,\n",
        "                'Estimate': estimate,\n",
        "                'LowerCI': estimate - 1.96 * se,\n",
        "                'UpperCI': estimate + 1.96 * se\n",
        "            })\n",
        "\n",
        "    plot_df = pd.DataFrame(plot_data)\n",
        "\n",
        "    # --- 2. Plotting ---\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "    fig, ax = plt.subplots(figsize=(10, 0.5 * len(plot_df) + 2))\n",
        "\n",
        "    # Plot the point estimates for each robustness check.\n",
        "    ax.scatter(y=plot_df['Check'], x=plot_df['Estimate'], color='black', zorder=10)\n",
        "    # Plot the confidence interval lines for each check.\n",
        "    for i, row in plot_df.iterrows():\n",
        "        ax.plot([row['LowerCI'], row['UpperCI']], [i, i], color='black', linewidth=1)\n",
        "\n",
        "    # --- 3. Add Baseline Reference ---\n",
        "    # Add a vertical line for the baseline model's point estimate.\n",
        "    ax.axvline(base_estimate, color='red', linestyle='-', linewidth=2, label='Baseline Estimate')\n",
        "    # Add a shaded region for the baseline model's 95% confidence interval.\n",
        "    base_lower_ci = base_estimate - 1.96 * base_se\n",
        "    base_upper_ci = base_estimate + 1.96 * base_se\n",
        "    ax.axvspan(base_lower_ci, base_upper_ci, color='red', alpha=0.1, label='Baseline 95% CI')\n",
        "\n",
        "    # --- 4. Aesthetics and Labels ---\n",
        "    ax.set_title(title, fontsize=16, pad=20)\n",
        "    ax.set_xlabel(f\"Coefficient Estimate for '{variable_to_plot}'\", fontsize=12)\n",
        "    ax.set_ylabel(\"\")\n",
        "    ax.invert_yaxis()\n",
        "    ax.legend()\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "cJGTJwqKMab-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21: Research Synthesis and Policy Implications\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Helper Functions for Synthesizing Results\n",
        "# ==============================================================================\n",
        "\n",
        "def _format_finding(estimate: float, se: float, unit: str) -> str:\n",
        "    \"\"\"\n",
        "    Formats a key quantitative finding into a standardized, human-readable sentence.\n",
        "\n",
        "    This helper function takes a point estimate, its standard error, and a unit\n",
        "    of measurement, then generates a complete sentence that reports the finding\n",
        "    along with its statistical significance. It is a crucial component for the\n",
        "    programmatic generation of a research summary report.\n",
        "\n",
        "    The process involves:\n",
        "    1. Calculating the Z-score from the estimate and standard error.\n",
        "    2. Computing the two-tailed p-value based on the standard normal distribution.\n",
        "    3. Determining the level of statistical significance based on conventional\n",
        "       thresholds (10%, 5%, 1%).\n",
        "    4. Assembling a formatted string that includes all this information.\n",
        "\n",
        "    Args:\n",
        "        estimate (float): The point estimate of the effect (e.g., a coefficient\n",
        "                          or marginal effect).\n",
        "        se (float): The standard error of the estimate.\n",
        "        unit (str): The unit of the estimate, to be included in the output\n",
        "                    string (e.g., \"%\", \"pp\", \"points\").\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string summarizing the finding, for example:\n",
        "             \"The estimated effect is -5.20 % (SE = 1.34, p = 0.000), which is\n",
        "             statistically significant at the 1% level.\"\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the standard error is non-positive, which would make\n",
        "                    the Z-score calculation invalid.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    # Ensure the standard error is a positive number for a valid calculation.\n",
        "    if not isinstance(se, (int, float)) or se <= 0:\n",
        "        raise ValueError(\"Standard error (se) must be a positive number.\")\n",
        "\n",
        "    # --- 2. Calculate Test Statistics ---\n",
        "    # Calculate the Z-score, which measures how many standard errors the\n",
        "    # estimate is away from zero.\n",
        "    z_score = estimate / se\n",
        "\n",
        "    # Calculate the two-tailed p-value from the Z-score using the cumulative\n",
        "    # distribution function (CDF) of the standard normal distribution.\n",
        "    # p = 2 * P(Z > |z_score|) = 2 * (1 - CDF(|z_score|))\n",
        "    p_value = 2 * (1 - norm.cdf(np.abs(z_score)))\n",
        "\n",
        "    # --- 3. Determine Significance Level ---\n",
        "    # Compare the p-value against conventional alpha levels to determine the\n",
        "    # appropriate significance statement.\n",
        "    if p_value < 0.01:\n",
        "        significance = \"statistically significant at the 1% level\"\n",
        "    elif p_value < 0.05:\n",
        "        significance = \"statistically significant at the 5% level\"\n",
        "    elif p_value < 0.1:\n",
        "        significance = \"statistically significant at the 10% level\"\n",
        "    else:\n",
        "        significance = \"not statistically significant\"\n",
        "\n",
        "    # --- 4. Format and Return the Final String ---\n",
        "    # Use an f-string to assemble the final, comprehensive summary sentence.\n",
        "    # Estimates are formatted to two decimal places, and the p-value to three.\n",
        "    return (\n",
        "        f\"The estimated effect is {estimate:.2f} {unit} \"\n",
        "        f\"(SE = {se:.2f}, p = {p_value:.3f}), which is {significance}.\"\n",
        "    )\n",
        "\n",
        "def summarize_baseline_findings(\n",
        "    ppml_marginal_effects: pd.DataFrame,\n",
        "    wto_attenuation: Dict[str, float],\n",
        "    baseline_scenario: str,\n",
        "    wto_scenario: str\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Summarizes the main findings from the baseline model.\n",
        "\n",
        "    Args:\n",
        "        ppml_marginal_effects (pd.DataFrame): DataFrame of marginal effects.\n",
        "        wto_attenuation (Dict[str, float]): Dictionary with attenuation effect results.\n",
        "        baseline_scenario (str): Name of the baseline (non-WTO) scenario.\n",
        "        wto_scenario (str): Name of the WTO member scenario.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: A dictionary of summary statements.\n",
        "    \"\"\"\n",
        "    # Extract baseline penalty on trade.\n",
        "    base_effect = ppml_marginal_effects.loc[baseline_scenario, 'EffectPct']\n",
        "    base_se = ppml_marginal_effects.loc[baseline_scenario, 'StdError']\n",
        "\n",
        "    # Extract WTO attenuation effect.\n",
        "    attenuation_effect = wto_attenuation['Attenuation_Effect_Pct']\n",
        "    attenuation_se = wto_attenuation['Attenuation_StdError']\n",
        "\n",
        "    # Generate summary statements.\n",
        "    summary = {\n",
        "        \"Political Distance Penalty\": _format_finding(base_effect, base_se, \"%\"),\n",
        "        \"WTO Attenuation Effect\": _format_finding(attenuation_effect, attenuation_se, \"pp\"),\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "def assess_robustness(\n",
        "    baseline_model: Fixest,\n",
        "    robustness_results: Dict[str, Fixest],\n",
        "    key_coefficient: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Assesses the robustness of a key coefficient across multiple model specifications.\n",
        "\n",
        "    Args:\n",
        "        baseline_model (Fixest): The fitted baseline model object.\n",
        "        robustness_results (Dict[str, Fixest]): Dictionary of robustness model results.\n",
        "        key_coefficient (str): The name of the coefficient to assess.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary summarizing the robustness assessment.\n",
        "    \"\"\"\n",
        "    # Get the sign and significance of the baseline coefficient.\n",
        "    base_summary = baseline_model.summary()\n",
        "    base_sign = np.sign(base_summary.loc[key_coefficient, 'Estimate'])\n",
        "    base_is_sig = base_summary.loc[key_coefficient, 'Pr(>|t|)'] < 0.05\n",
        "\n",
        "    # Initialize counters.\n",
        "    successful_runs = 0\n",
        "    sign_consistent = 0\n",
        "    sig_consistent = 0\n",
        "\n",
        "    # Iterate through robustness checks.\n",
        "    for model in robustness_results.values():\n",
        "        if model is not None and key_coefficient in model.coef().index:\n",
        "            successful_runs += 1\n",
        "            summary = model.summary()\n",
        "            current_sign = np.sign(summary.loc[key_coefficient, 'Estimate'])\n",
        "            current_is_sig = summary.loc[key_coefficient, 'Pr(>|t|)'] < 0.05\n",
        "\n",
        "            if current_sign == base_sign:\n",
        "                sign_consistent += 1\n",
        "                if current_is_sig == base_is_sig:\n",
        "                    sig_consistent += 1\n",
        "\n",
        "    # Classify robustness based on consistency.\n",
        "    consistency_frac = sig_consistent / successful_runs if successful_runs > 0 else 0\n",
        "    if consistency_frac > 0.9: classification = \"Highly Robust\"\n",
        "    elif consistency_frac > 0.7: classification = \"Moderately Robust\"\n",
        "    else: classification = \"Sensitive\"\n",
        "\n",
        "    return {\n",
        "        \"Key Coefficient\": key_coefficient,\n",
        "        \"Total Checks\": len(robustness_results),\n",
        "        \"Successful Runs\": successful_runs,\n",
        "        \"Sign Consistency\": f\"{sign_consistent}/{successful_runs} ({sign_consistent/successful_runs:.0%})\",\n",
        "        \"Significance Consistency\": f\"{sig_consistent}/{successful_runs} ({consistency_frac:.0%})\",\n",
        "        \"Overall Assessment\": classification\n",
        "    }\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21, New Helper Function: Assess Economic Significance\n",
        "# ==============================================================================\n",
        "\n",
        "def assess_economic_significance(\n",
        "    effect_estimate: float,\n",
        "    thresholds: Dict[str, float]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Classifies the economic significance of an estimated effect based on thresholds.\n",
        "\n",
        "    This function translates a statistical point estimate into a qualitative\n",
        "    assessment of its practical importance, moving beyond p-values to consider\n",
        "    the magnitude of the effect.\n",
        "\n",
        "    Args:\n",
        "        effect_estimate (float): The point estimate of the effect (e.g., a\n",
        "                                 percentage point change).\n",
        "        thresholds (Dict[str, float]): A dictionary defining the boundaries for\n",
        "            classification. Keys are labels (e.g., \"Substantial\") and values\n",
        "            are the minimum absolute effect size for that category. The\n",
        "            dictionary should be ordered from most to least substantial.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the original effect size and a\n",
        "                        string classification of its economic significance.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    if not isinstance(effect_estimate, (int, float)):\n",
        "        raise TypeError(\"effect_estimate must be a numeric value.\")\n",
        "    if not isinstance(thresholds, dict) or not thresholds:\n",
        "        raise ValueError(\"thresholds must be a non-empty dictionary.\")\n",
        "\n",
        "    # --- 2. Classification Logic ---\n",
        "    # Take the absolute value of the effect for magnitude comparison.\n",
        "    abs_effect = abs(effect_estimate)\n",
        "\n",
        "    # Default classification is \"Marginal\".\n",
        "    classification = \"Marginal\"\n",
        "\n",
        "    # Iterate through the thresholds (assumed to be ordered high to low)\n",
        "    # to find the appropriate classification.\n",
        "    for label, threshold_value in thresholds.items():\n",
        "        if abs_effect >= threshold_value:\n",
        "            classification = label\n",
        "            break # Stop at the first (highest) category met.\n",
        "\n",
        "    # --- 3. Format and Return Output ---\n",
        "    return {\n",
        "        \"Effect_Size\": f\"{effect_estimate:.2f}\",\n",
        "        \"Classification\": classification\n",
        "    }\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Master Orchestrator Function\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_synthesis_report(\n",
        "    baseline_ppml_results: Fixest,\n",
        "    ppml_marginal_effects: pd.DataFrame,\n",
        "    wto_attenuation_results: Dict[str, float],\n",
        "    robustness_results: Dict[str, Fixest],\n",
        "    temporal_break_test_results: pd.Series,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generates a complete, structured synthesis report of all key quantitative findings.\n",
        "\n",
        "    This function orchestrates the entire synthesis phase, calling helper\n",
        "    functions to programmatically extract, assess, and summarize the main\n",
        "    results from the entire analytical pipeline. The output is a nested\n",
        "    dictionary that serves as a complete, reproducible summary of the project's\n",
        "    conclusions, now including an assessment of economic significance.\n",
        "\n",
        "    Args:\n",
        "        baseline_ppml_results (Fixest): The main fitted PPML model object.\n",
        "        ppml_marginal_effects (pd.DataFrame): DataFrame of marginal effects.\n",
        "        wto_attenuation_results (Dict[str, float]): Results of the attenuation analysis.\n",
        "        robustness_results (Dict[str, Fixest]): Dictionary of all robustness models.\n",
        "        temporal_break_test_results (pd.Series): Results of the structural break test.\n",
        "        config (Dict[str, Any]): A configuration dictionary specifying key variable\n",
        "                                 names, scenario names, and thresholds for the report.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing the full synthesis report.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- Step 1: Summarize Baseline Findings ---\n",
        "        print(\"Step 1: Summarizing baseline findings...\")\n",
        "        baseline_summary = summarize_baseline_findings(\n",
        "            ppml_marginal_effects,\n",
        "            wto_attenuation_results,\n",
        "            config['baseline_scenario'],\n",
        "            config['wto_scenario']\n",
        "        )\n",
        "        print(\"Step 1: Success.\")\n",
        "\n",
        "        # --- Step 2: Assess Robustness of the Key Finding ---\n",
        "        print(\"Step 2: Assessing robustness of the key finding...\")\n",
        "        robustness_summary = assess_robustness(\n",
        "            baseline_ppml_results,\n",
        "            robustness_results,\n",
        "            config['key_coefficient_for_robustness']\n",
        "        )\n",
        "        print(\"Step 2: Success.\")\n",
        "\n",
        "        # --- Step 3: Summarize Temporal Evolution ---\n",
        "        print(\"Step 3: Summarizing temporal evolution...\")\n",
        "        temporal_summary = {\n",
        "            \"Finding\": f\"Test for structural break in WTO effect post-{config['break_year']}\",\n",
        "            \"Summary\": _format_finding(\n",
        "                temporal_break_test_results['Estimate'],\n",
        "                temporal_break_test_results['Std. Error'],\n",
        "                \"points\"\n",
        "            )\n",
        "        }\n",
        "        print(\"Step 3: Success.\")\n",
        "\n",
        "        # --- Step 4: Assess Economic Significance (New Step) ---\n",
        "        print(\"Step 4: Assessing economic significance...\")\n",
        "        # Define the thresholds for significance.\n",
        "        eco_sig_thresholds = {\"Substantial\": 5.0, \"Meaningful\": 1.0}\n",
        "        # Extract the key effect to assess.\n",
        "        key_effect = wto_attenuation_results['Attenuation_Effect_Pct']\n",
        "        # Call the new helper function.\n",
        "        economic_significance_summary = assess_economic_significance(\n",
        "            key_effect,\n",
        "            eco_sig_thresholds\n",
        "        )\n",
        "        print(\"Step 4: Success.\")\n",
        "\n",
        "        # --- Step 5: Compile Final Report ---\n",
        "        final_report = {\n",
        "            \"1_Baseline_Findings\": baseline_summary,\n",
        "            \"2_Robustness_Assessment\": robustness_summary,\n",
        "            \"3_Temporal_Heterogeneity\": temporal_summary,\n",
        "            \"4_Economic_Significance\": economic_significance_summary\n",
        "        }\n",
        "\n",
        "        print(\"\\nSUCCESS: Complete synthesis report generated.\")\n",
        "        return final_report\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Synthesis report generation failed.\")\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "LfLmkZDsNq8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator Function\n",
        "\n",
        "# ==============================================================================\n",
        "# Standalone Helper Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def synthesize_country_level_data(master_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Synthesizes a country-year panel from a dyadic master panel.\n",
        "\n",
        "    This function is a crucial pre-processing step that extracts country-specific\n",
        "    time-series data (like GDP and Total Exports) from a dyadic panel where\n",
        "    such data is repeated for both the exporter and importer. It isolates the\n",
        "    exporter-specific and importer-specific columns, stacks them, standardizes\n",
        "    their names, and removes duplicate country-year observations to create a\n",
        "    clean, canonical country-year dataset. This dataset is essential for\n",
        "    calculating domestic trade flows as required by structural gravity theory.\n",
        "\n",
        "    Args:\n",
        "        master_df (pd.DataFrame): The dyadic master DataFrame. It must have a\n",
        "            DatetimeIndex and columns for ExporterISO, ImporterISO, and\n",
        "            country-specific variables suffixed with '_Exporter' and '_Importer'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A clean, de-duplicated country-year panel with columns\n",
        "                      ['ISO', 'Year', 'GDP_USD', 'TotalExports_USD', ...].\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input is not a pandas DataFrame.\n",
        "        ValueError: If essential columns are missing.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    # Verify the input is a pandas DataFrame with a DatetimeIndex.\n",
        "    if not isinstance(master_df, pd.DataFrame) or not isinstance(master_df.index, pd.DatetimeIndex):\n",
        "        raise TypeError(\"Input must be a pandas DataFrame with a DatetimeIndex.\")\n",
        "\n",
        "    # --- 2. Data Extraction and Reshaping ---\n",
        "    # Create a temporary copy to work with.\n",
        "    df = master_df.copy()\n",
        "    # Extract the year from the index, as it's a key for the country-year panel.\n",
        "    df['Year'] = df.index.year\n",
        "\n",
        "    # Define the mapping for exporter columns to generic names.\n",
        "    exp_cols = {c: c.replace('_Exporter', '') for c in df.columns if '_Exporter' in c}\n",
        "    # Define the mapping for importer columns to generic names.\n",
        "    imp_cols = {c: c.replace('_Importer', '') for c in df.columns if '_Importer' in c}\n",
        "\n",
        "    # Create the exporter-specific DataFrame.\n",
        "    exp_df = df[['ExporterISO', 'Year'] + list(exp_cols.keys())].rename(\n",
        "        columns={'ExporterISO': 'ISO', **exp_cols}\n",
        "    )\n",
        "    # Create the importer-specific DataFrame.\n",
        "    imp_df = df[['ImporterISO', 'Year'] + list(imp_cols.keys())].rename(\n",
        "        columns={'ImporterISO': 'ISO', **imp_cols}\n",
        "    )\n",
        "\n",
        "    # --- 3. Concatenation and Deduplication ---\n",
        "    # Stack the exporter and importer dataframes.\n",
        "    country_df = pd.concat([exp_df, imp_df], ignore_index=True)\n",
        "\n",
        "    # Drop duplicate country-year observations to create the final panel.\n",
        "    country_df.drop_duplicates(subset=['ISO', 'Year'], inplace=True)\n",
        "\n",
        "    # Drop any rows that might have missing data after the reshape.\n",
        "    return country_df.dropna().reset_index(drop=True)\n",
        "\n",
        "\n",
        "def synthesize_unga_data(master_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Synthesizes a UNGA ideal point country-year panel from a dyadic master panel.\n",
        "\n",
        "    This function assumes that UNGA ideal point estimates have already been\n",
        "    merged onto the master dyadic DataFrame (as 'IdealPoint_Exporter' and\n",
        "    'IdealPoint_Importer'). It extracts these values, stacks them, and creates\n",
        "    the clean, canonical country-year UNGA dataset required by the variable\n",
        "    construction pipeline.\n",
        "\n",
        "    Args:\n",
        "        master_df (pd.DataFrame): The dyadic master DataFrame, which must contain\n",
        "            the 'IdealPoint_Exporter' and 'IdealPoint_Importer' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A clean, de-duplicated country-year panel with columns\n",
        "                      ['ISO', 'Year', 'IdealPoint'].\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the required IdealPoint columns are not in the DataFrame.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    required_cols = ['IdealPoint_Exporter', 'IdealPoint_Importer', 'ExporterISO', 'ImporterISO']\n",
        "    if not all(col in master_df.columns for col in required_cols):\n",
        "        raise ValueError(f\"master_df must contain required columns: {required_cols}\")\n",
        "\n",
        "    # --- 2. Data Extraction and Reshaping ---\n",
        "    # Create a temporary copy and extract the year.\n",
        "    df = master_df.copy()\n",
        "    df['Year'] = df.index.year\n",
        "\n",
        "    # Extract non-null exporter ideal points.\n",
        "    exp_unga = df.loc[df['IdealPoint_Exporter'].notna(), ['ExporterISO', 'Year', 'IdealPoint_Exporter']].rename(\n",
        "        columns={'ExporterISO': 'ISO', 'IdealPoint_Exporter': 'IdealPoint'}\n",
        "    )\n",
        "    # Extract non-null importer ideal points.\n",
        "    imp_unga = df.loc[df['IdealPoint_Importer'].notna(), ['ImporterISO', 'Year', 'IdealPoint_Importer']].rename(\n",
        "        columns={'ImporterISO': 'ISO', 'IdealPoint_Importer': 'IdealPoint'}\n",
        "    )\n",
        "\n",
        "    # --- 3. Concatenation and Deduplication ---\n",
        "    # Stack the two datasets.\n",
        "    unga_df = pd.concat([exp_unga, imp_unga], ignore_index=True)\n",
        "\n",
        "    # Drop duplicate country-year observations.\n",
        "    return unga_df.drop_duplicates(subset=['ISO', 'Year']).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def define_baseline_model_spec() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Defines and returns the baseline model specification as a dictionary.\n",
        "\n",
        "    This function centralizes the main econometric model's specification.\n",
        "    Encapsulating the specification in a function ensures consistency across\n",
        "    different parts of the pipeline (e.g., main estimation, robustness checks)\n",
        "    and makes it easy to modify the baseline model in one place.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing all components of the model\n",
        "                        specification, including dependent variable, regressors,\n",
        "                        fixed effects, and cluster variable.\n",
        "    \"\"\"\n",
        "    # This dictionary structure is designed to be parsed by the various\n",
        "    # estimation and analysis orchestrators.\n",
        "    specification = {\n",
        "        'dependent_var': 'BilateralTradeValue_USD',\n",
        "        'main_regressors': ['RTA', 'GATTWTO_One', 'GATTWTO_Both'],\n",
        "        'interaction_regressors': [\n",
        "            'PD_GDELT_Filtered_IHS',\n",
        "            'PD_GDELT_Filtered_IHS:RTA',\n",
        "            'PD_GDELT_Filtered_IHS:GATTWTO_One',\n",
        "            'PD_GDELT_Filtered_IHS:GATTWTO_Both'\n",
        "        ],\n",
        "        'fixed_effects': ['FE_ExporterYear', 'FE_ImporterYear', 'DyadicPair'],\n",
        "        'cluster_col': 'DyadicPair',\n",
        "        'main_pd_term': 'is_international:PD_GDELT_Filtered_IHS',\n",
        "        'wto_interaction_term': 'is_international:PD_GDELT_Filtered_IHS:GATTWTO_Both',\n",
        "        'rta_interaction_term': 'is_international:PD_GDELT_Filtered_IHS:RTA',\n",
        "        'governance_interaction_terms': [] # Baseline model excludes these.\n",
        "    }\n",
        "    return specification\n",
        "\n",
        "\n",
        "def generate_and_save_all_figures(output_dir: Path, **kwargs: Any) -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the generation and saving of all primary project figures.\n",
        "\n",
        "    This function takes a keyword dictionary of all necessary data and results\n",
        "    objects, calls the respective plotting functions for each figure, and saves\n",
        "    the output to high-resolution files in the specified directory.\n",
        "\n",
        "    Args:\n",
        "        output_dir (Path): A pathlib.Path object pointing to the output directory.\n",
        "        **kwargs (Any): A keyword dictionary of results objects. Expected keys include:\n",
        "            'ppml_marginal_effects' (pd.DataFrame),\n",
        "            'temporal_effects' (pd.DataFrame),\n",
        "            'ppml_results' (Fixest),\n",
        "            'robustness_results' (Dict[str, Fixest]),\n",
        "            'base_spec' (Dict[str, Any]).\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    # Check that the output directory exists.\n",
        "    if not output_dir.is_dir():\n",
        "        raise FileNotFoundError(f\"Output directory does not exist: {output_dir}\")\n",
        "\n",
        "    print(\"--- Generating and saving all figures ---\")\n",
        "\n",
        "    # --- 2. Plot 1: PPML Marginal Effects ---\n",
        "    # Check if the required data for this plot is available.\n",
        "    if 'ppml_marginal_effects' in kwargs:\n",
        "        # Generate the plot using the dedicated plotting function.\n",
        "        fig1 = plot_marginal_effects(\n",
        "            kwargs['ppml_marginal_effects'],\n",
        "            title=\"PPML: Effect of a 1-SD Increase in Political Distance\",\n",
        "            x_label=\"% Change in Trade Value\"\n",
        "        )\n",
        "        # Define the output path and save the figure.\n",
        "        fig1_path = output_dir / \"figure_1_ppml_marginal_effects.png\"\n",
        "        fig1.savefig(fig1_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Saved Figure 1 to: {fig1_path}\")\n",
        "\n",
        "    # --- 3. Plot 2: Temporal Heterogeneity ---\n",
        "    # Check for the required data.\n",
        "    if 'temporal_effects' in kwargs:\n",
        "        # Generate the plot.\n",
        "        fig2 = plot_temporal_heterogeneity(\n",
        "            kwargs['temporal_effects'],\n",
        "            title=\"Evolution of WTO Attenuation Effect Over Time\"\n",
        "        )\n",
        "        # Save the figure.\n",
        "        fig2_path = output_dir / \"figure_2_temporal_heterogeneity.png\"\n",
        "        fig2.savefig(fig2_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Saved Figure 2 to: {fig2_path}\")\n",
        "\n",
        "    # --- 4. Plot 3: Robustness Stability Plot ---\n",
        "    # This plot requires multiple inputs.\n",
        "    required_robustness_keys = ['ppml_results', 'robustness_results', 'base_spec']\n",
        "    if all(key in kwargs for key in required_robustness_keys) and kwargs['robustness_results']:\n",
        "        # Generate the plot.\n",
        "        fig3 = plot_robustness_check_stability(\n",
        "            kwargs['ppml_results'],\n",
        "            kwargs['robustness_results'],\n",
        "            variable_to_plot=kwargs['base_spec']['wto_interaction_term'],\n",
        "            title=\"Coefficient Stability of WTO Interaction Term\"\n",
        "        )\n",
        "        # Save the figure.\n",
        "        fig3_path = output_dir / \"figure_3_robustness_stability.png\"\n",
        "        fig3.savefig(fig3_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Saved Figure 3 to: {fig3_path}\")\n",
        "\n",
        "    # --- 5. Cleanup ---\n",
        "    # Close all figure objects to free up memory.\n",
        "    plt.close('all')\n",
        "    print(\"--- All figures generated and saved successfully ---\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Define Full Input Schema\n",
        "# ==============================================================================\n",
        "\n",
        "def define_full_schema() -> Dict[str, Type]:\n",
        "    \"\"\"\n",
        "    Defines the complete and accurate schema for the master input DataFrame.\n",
        "\n",
        "    This function serves as the single source of truth for the expected structure\n",
        "    of the input data. It includes all 26 columns required for the full end-to-end\n",
        "    pipeline, incorporating critical corrections and additions identified during\n",
        "    the detailed analysis of the research methodology. Specifically:\n",
        "    - It adds 'IdealPoint_Exporter' and 'IdealPoint_Importer' for the UNGA analysis.\n",
        "    - It correctly specifies 'TradeProd_SectorCount' for the margin analysis,\n",
        "      replacing the less accurate 'ExtensiveMargin_ProductCount'.\n",
        "\n",
        "    The schema is used by the `run_dataframe_validation_and_assessment` function\n",
        "    to rigorously validate the raw input data before any processing begins.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Type]: A dictionary where keys are the required column names\n",
        "                         and values are their precise expected numpy/object dtypes.\n",
        "    \"\"\"\n",
        "    # Define the schema as a dictionary for clarity and easy validation.\n",
        "    # Using specific numpy dtypes (e.g., np.float64) is a best practice for\n",
        "    # ensuring data precision and memory consistency.\n",
        "\n",
        "    full_schema = {\n",
        "        # --- Core Bilateral Trade Variables ---\n",
        "        \"ExporterISO\": object,\n",
        "        \"ImporterISO\": object,\n",
        "        \"DyadicPair\": object,\n",
        "        \"BilateralTradeValue_USD\": np.float64,\n",
        "\n",
        "        # --- Extensive Margin Variables (Corrected) ---\n",
        "        # NOTE: 'ExtensiveMargin_ProductCount' is replaced with the correct\n",
        "        # variable for the paper's margin analysis.\n",
        "        \"TradeProd_SectorCount\": np.int64,\n",
        "        \"BACI_TotalValue_USD\": np.float64,\n",
        "\n",
        "        # --- Political Distance and Institutional Variables ---\n",
        "        \"GDELT_GoldsteinMean\": np.float64,\n",
        "        \"GDELT_GoldsteinStd\": np.float64,\n",
        "        \"GDELT_EventCount\": np.int64,\n",
        "        \"GATTWTO_Both\": np.int64,\n",
        "        \"GATTWTO_One\": np.int64,\n",
        "        \"RTA\": np.int64,\n",
        "\n",
        "        # --- Country-Level Economic Controls ---\n",
        "        \"GDP_USD_Exporter\": np.float64,\n",
        "        \"GDP_USD_Importer\": np.float64,\n",
        "        \"TotalExports_USD_Exporter\": np.float64,\n",
        "        \"TotalExports_USD_Importer\": np.float64,\n",
        "        \"DomesticTrade_Exporter\": np.float64,\n",
        "        \"DomesticTrade_Importer\": np.float64,\n",
        "\n",
        "        # --- Political and Governance Variables ---\n",
        "        \"PolityScore_Exporter\": np.int64,\n",
        "        \"PolityScore_Importer\": np.int64,\n",
        "        \"CorruptionIndex_Exporter\": np.float64,\n",
        "        \"CorruptionIndex_Importer\": np.float64,\n",
        "\n",
        "        # --- Constructed Political Variables ---\n",
        "        \"DemocraticPair\": np.int64,\n",
        "        \"PoliticalDistance\": np.float64,\n",
        "\n",
        "        # --- CRITICAL ADDITIONS for UNGA Analysis ---\n",
        "        # These columns are required by the pipeline but were missing from the\n",
        "        # initial 24-column specification.\n",
        "        \"IdealPoint_Exporter\": np.float64,\n",
        "        \"IdealPoint_Importer\": np.float64,\n",
        "    }\n",
        "\n",
        "    return full_schema\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# The Top-Level Orchestrator\n",
        "# ==============================================================================\n",
        "\n",
        "def execute_full_research_pipeline(\n",
        "    master_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    output_dir: str,\n",
        "    run_intensive_logit: bool = True,\n",
        "    run_robustness_checks: bool = True\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end research pipeline for the study.\n",
        "\n",
        "    This master function is the single entry point for the entire project. It\n",
        "    takes the raw, monolithic dataset and the project configuration, and then\n",
        "    sequentially executes all 21 tasks, from data validation and preprocessing\n",
        "    to estimation, analysis, robustness checks, and final report generation.\n",
        "\n",
        "    The function is designed to be a \"one-button\" replication script. It has no\n",
        "    return value; its primary effect is to populate the specified output\n",
        "    directory with all reproducible research outputs, including data tables,\n",
        "    figures, and a final JSON synthesis report.\n",
        "\n",
        "    Args:\n",
        "        master_df (pd.DataFrame): The single, monolithic input DataFrame containing\n",
        "                                  all raw dyadic and country-level variables. It\n",
        "                                  is assumed to contain the necessary columns for\n",
        "                                  synthesizing all required sub-datasets.\n",
        "        config (Dict[str, Any]): The master configuration dictionary for the project.\n",
        "        output_dir (str): The path to the directory where all outputs will be saved.\n",
        "        run_intensive_logit (bool): Flag to run the computationally expensive\n",
        "                                    Logit estimation. Defaults to True.\n",
        "        run_robustness_checks (bool): Flag to run the computationally expensive\n",
        "                                      robustness checks. Defaults to True.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Re-raises any exception caught during the pipeline's\n",
        "                   execution after printing a detailed error message and\n",
        "                   traceback.\n",
        "    \"\"\"\n",
        "    # Wrap the entire pipeline in a try-except block for robust error handling.\n",
        "    try:\n",
        "        # --- Setup ---\n",
        "        # Define the output directory using pathlib for cross-platform compatibility.\n",
        "        output_path = Path(output_dir)\n",
        "        # Create the directory and any parent directories if they do not exist.\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # --- Phase 1: Configuration and Data Validation ---\n",
        "        print(\"--- PHASE 1: CONFIGURATION AND DATA VALIDATION ---\")\n",
        "        # Validate the configuration dictionary and create the immutable constants object.\n",
        "        constants = run_configuration_and_validation(config)\n",
        "        # Define the full, corrected schema for the input data.\n",
        "        full_schema = define_full_schema()\n",
        "        # Validate the input DataFrame against the rigorous schema.\n",
        "        run_dataframe_validation_and_assessment(master_df, full_schema)\n",
        "        # Run a comprehensive data quality audit and print the results.\n",
        "        _ = run_data_quality_assessment(master_df)\n",
        "\n",
        "        # --- Phase 2: Data Preprocessing and Transformation ---\n",
        "        print(\"\\n--- PHASE 2: DATA PREPROCESSING AND TRANSFORMATION ---\")\n",
        "        # Synthesize the required country-level and UNGA data from the master frame.\n",
        "        country_df = synthesize_country_level_data(master_df)\n",
        "        unga_df = synthesize_unga_data(master_df)\n",
        "        # Run the GDELT preprocessing pipeline (filtering, AR(1) variance estimation).\n",
        "        gdelt_series, q_vars = run_gdelt_preprocessing_pipeline(master_df, constants)\n",
        "        # Run the Particle Filter to get the smoothed political distance measure.\n",
        "        filtered_gdelt = run_particle_filter_pipeline(gdelt_series, q_vars, constants, base_seed=2025)\n",
        "        # Run the final variable construction pipeline (merge results, create margins, etc.).\n",
        "        analytical_df = run_variable_construction_pipeline(\n",
        "            main_df=master_df,\n",
        "            filtered_gdelt_df=filtered_gdelt,\n",
        "            unga_ideal_points_df=unga_df,\n",
        "            ihs_columns=['PD_GDELT_Filtered', 'PD_UNGA', 'PolityScore_Exporter', 'PolityScore_Importer']\n",
        "        )\n",
        "\n",
        "        # --- Phase 3: Model Preparation ---\n",
        "        print(\"\\n--- PHASE 3: MODEL PREPARATION ---\")\n",
        "        # Define the baseline model specification.\n",
        "        base_spec = define_baseline_model_spec()\n",
        "        # Create the interaction terms required by the specification.\n",
        "        interaction_terms_to_create = [(term.split(':')[1], term.split(':')[2], term) for term in base_spec['interaction_regressors'] if len(term.split(':')) == 3]\n",
        "        # Add fixed effects identifiers and interaction terms to the DataFrame.\n",
        "        prepared_df = run_panel_data_preparation(analytical_df, interaction_terms_to_create)\n",
        "        # Define the core variables for listwise deletion.\n",
        "        core_vars = list(base_spec['main_regressors']) + [base_spec['dependent_var']] + [base_spec['interaction_regressors'][0]]\n",
        "        # Create the specific analytical samples (PPML, Logit, Temporal).\n",
        "        samples = run_sample_preparation_pipeline(prepared_df, country_df, {'core_regression_vars': core_vars})\n",
        "        # Prepare the generators for bootstrap and jackknife procedures.\n",
        "        inference_prep = run_inference_preparation(\n",
        "            samples['ppml_sample'], 'DyadicPair', constants.N_BOOTSTRAP, 50, seed=2025\n",
        "        )\n",
        "\n",
        "        # --- Phase 4: Primary Estimation ---\n",
        "        print(\"\\n--- PHASE 4: PRIMARY ECONOMETRIC ESTIMATION ---\")\n",
        "        # Estimate the main PPML model.\n",
        "        ppml_results = run_ppml_estimation_pipeline(samples['ppml_sample'], base_spec)\n",
        "        # Conditionally run the computationally intensive Logit model.\n",
        "        logit_results = None\n",
        "        if run_intensive_logit:\n",
        "            logit_spec = base_spec.copy()\n",
        "            logit_spec['dependent_var'] = 'AnyTrade_Sector_Dummy'\n",
        "            logit_results = run_logit_estimation_pipeline(\n",
        "                samples['logit_sample'], logit_spec, constants, seed=2025\n",
        "            )\n",
        "\n",
        "        # --- Phase 5: Analysis and Interpretation ---\n",
        "        print(\"\\n--- PHASE 5: ANALYSIS AND INTERPRETATION ---\")\n",
        "        # Calculate the standard deviation of the key regressor for marginal effects.\n",
        "        std_devs = calculate_pd_standard_deviations(samples['ppml_sample'], ['PD_GDELT_Filtered_IHS'])\n",
        "        # Calculate the PPML marginal effects.\n",
        "        ppml_marginal_effects = compute_ppml_marginal_effects(\n",
        "            ppml_results, std_devs['PD_GDELT_Filtered_IHS'],\n",
        "            base_spec['main_pd_term'],\n",
        "            {'Baseline (Non-WTO)': [], 'WTO Members': [base_spec['wto_interaction_term']]}\n",
        "        )\n",
        "        # Quantify the key institutional effects and run hypothesis tests.\n",
        "        quant_report = run_institutional_quantification_pipeline(ppml_results, std_devs['PD_GDELT_Filtered_IHS'], base_spec)\n",
        "        # Analyze the temporal stability of the findings.\n",
        "        temp_report = run_temporal_analysis_pipeline(\n",
        "            samples['temporal_subsamples'], samples['ppml_sample'], base_spec,\n",
        "            {'Baseline (Non-WTO)': [], 'WTO Members': [base_spec['wto_interaction_term']]}, 2009\n",
        "        )\n",
        "\n",
        "        # --- Phase 6: Robustness Analysis ---\n",
        "        print(\"\\n--- PHASE 6: ROBUSTNESS ANALYSIS ---\")\n",
        "        # Conditionally run the full suite of robustness checks.\n",
        "        robustness_results = None\n",
        "        if run_robustness_checks:\n",
        "            robustness_results = run_all_robustness_analyses(\n",
        "                samples['ppml_sample'], base_spec, constants\n",
        "            )\n",
        "\n",
        "        # --- Phase 7: Results Presentation ---\n",
        "        print(\"\\n--- PHASE 7: RESULTS PRESENTATION AND SYNTHESIS ---\")\n",
        "        # Generate and save all result tables.\n",
        "        tables = run_results_presentation_pipeline(\n",
        "            {'Baseline PPML': ppml_results},\n",
        "            {'Baseline Logit': logit_results} if logit_results else {},\n",
        "            {'Baseline Logit': len(samples['logit_sample'])} if logit_results else {},\n",
        "            robustness_results if robustness_results else {},\n",
        "            {'key_robustness_vars': [base_spec['wto_interaction_term']]}\n",
        "        )\n",
        "        tables['main_results'].to_csv(output_path / \"table_1_main_results.csv\")\n",
        "        if 'robustness_summary' in tables and not tables['robustness_summary'].empty:\n",
        "            tables['robustness_summary'].to_csv(output_path / \"table_2_robustness_summary.csv\")\n",
        "\n",
        "        # Generate and save all figures.\n",
        "        generate_and_save_all_figures(\n",
        "            output_path,\n",
        "            ppml_marginal_effects=ppml_marginal_effects,\n",
        "            temporal_effects=temp_report['period_specific_effects'],\n",
        "            ppml_results=ppml_results,\n",
        "            robustness_results=robustness_results,\n",
        "            base_spec=base_spec\n",
        "        )\n",
        "\n",
        "        # Generate and save the final JSON synthesis report.\n",
        "        final_report = generate_synthesis_report(\n",
        "            ppml_results, ppml_marginal_effects, quant_report['wto_attenuation'],\n",
        "            robustness_results if robustness_results else {},\n",
        "            temp_report['structural_break_test'],\n",
        "            {'baseline_scenario': 'Baseline (Non-WTO)', 'wto_scenario': 'WTO Members',\n",
        "             'key_coefficient_for_robustness': base_spec['wto_interaction_term'],\n",
        "             'break_year': 2009}\n",
        "        )\n",
        "        with open(output_path / \"final_synthesis_report.json\", 'w') as f:\n",
        "            json.dump(final_report, f, indent=4)\n",
        "\n",
        "        # Print a final success message.\n",
        "        print(f\"\\n--- EXECUTION COMPLETE ---\")\n",
        "        print(f\"All outputs saved to: {output_path.resolve()}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # If any error occurs anywhere in the pipeline, catch it, print a\n",
        "        # detailed error message and traceback, and then re-raise it.\n",
        "        print(f\"\\n--- TOP-LEVEL PIPELINE FAILED ---\")\n",
        "        print(f\"An error occurred in one of the pipeline stages: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "NXrxgO-LxT8A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}